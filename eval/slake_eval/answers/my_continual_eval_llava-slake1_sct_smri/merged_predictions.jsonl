{"question_id": 0, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "WUs6VrSCaFnM7D2VhrguhH", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "EoLoaAGFQwdhd5vButmk4M", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "KZK7ymU4TeZ5SHytq2Rqk4", "gt_response": "Lung, Spinal Cord", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 3, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "aUU6BPxAmH5qvyReiFC9j7", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 4, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "gf3amrNXHVsQdiE2XmR8DX", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 5, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "JmysBeR3DzQc2MbcYWhWh3", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 6, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "Yes", "answer_id": "avokxKtZK5sZZJAkUU9GFh", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 7, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "hBNUFVFovGgatTXi5LrzmN", "gt_response": "Lung Cancer", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 8, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "dyxXJCw8ogZHq9ko6TUmDQ", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 9, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "mYpmTTyoBeRGTYDPPZ7YXi", "gt_response": "Left Lung, Right", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 10, "prompt": "Which is the biggest in this image,lung,liver or heart?\n<image>", "pred_response": "Lung", "answer_id": "6DK47rpfFz3cPJAPPCATgo", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 11, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "Y3BSeosQjRsPMd69i6f949", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 12, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "crLqE5TBotoKdccc9zWCjq", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 13, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "2pdUbMVukBNFAVsT7XFGiR", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 14, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "d2KkDNk9dSzS9wADSeczzj", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 15, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "eHeHHVBLp7x2HQiMADhfyP", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 16, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "B6829nqZuNypoLNDoyvuLs", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 17, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "Xc26KLvQqSCv3wxQBtQwZa", "gt_response": "Lung Cancer", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 18, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "GsQhhR95HbKxNzcYWBA8oP", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 19, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "jE82omcUvb7QcTHhZ4mMLh", "gt_response": "Left Lung, Right", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 20, "prompt": "Which is the biggest in this image,lung,liver or heart?\n<image>", "pred_response": "Lung", "answer_id": "DW7Y3qD5xcP2sGHeumPe6q", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 21, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "k7ir7rHjV7tQPyRes5wmJH", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 22, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "5iUnNTZJ54tmbhgGKGP886", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 23, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "HepeoUgtL34DBYmgB6X5vi", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 24, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "aTdGT7QBPii5LhYtqCLQEu", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 25, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "mmshApnj4jTyneGeWix8xW", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 26, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "2uS7LZAipZxzJQ5RZKB7zT", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 27, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "cwUY2DJCTRALxwYqEQGnQU", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 28, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "WxFooHHzexqBgDAGNABByK", "gt_response": "Lung Cancer", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 29, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "3erMbbPxeDc7HHdUqnRMkw", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 30, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "nyLXq6zJmJyTMdcibRYTfH", "gt_response": "Right Lung, Left", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 31, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "PXg649uthS7MeUELeRDRfp", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 32, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "bTWor6FDJ7frMwEzw9LB3f", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 33, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "E824mp5xYVZthQU5LqXbAy", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 34, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "None", "answer_id": "79KFBkXWNCjsfuc5diD7xm", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 35, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "G9jV5xNjbnMqasVsYpj5D4", "gt_response": "Center", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 36, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "No", "answer_id": "mcc6LVuvsifKtERgJfBGe8", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 37, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "XGGvHTwBBjGZftE6XvBhDp", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 38, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "huUtofzjW3xCcLwk4dY8zA", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 39, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "hPDFJFLYGnV9srPMs25ZSP", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 40, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T1", "answer_id": "TNPwjc5wPaCVToEUJ7XpbU", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 41, "prompt": "Is there any abnormality in the spleen?\n<image>", "pred_response": "No", "answer_id": "f6UGJriWg57gPer6n9ocaL", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 42, "prompt": "Does the liver look normal?\n<image>", "pred_response": "No", "answer_id": "FiWvfNMKeqgM5jVmWia7qi", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 43, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "A8GzKtLJLKZXaDdHqG4YAZ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 44, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "mGUiPvYicwP2fRZVA8Qugd", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 45, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "MPYK9mVvGuiPN4LZBKH9MZ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 46, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "BYjnmbYMhXh4Se43ryGCGi", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 47, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "By3uuH9qCyk4mmcoMEPHwu", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 48, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "No", "answer_id": "aUHDsYakUwBSHYb7ms7iQT", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 49, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "FjebrEBSD3V6UQNG3KxdNm", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 50, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "bZWg3366T3qVBNpTfjizHh", "gt_response": "Pneumonia", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 51, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "krtouZcUEDNGqwHM7vz7WD", "gt_response": "Left Lung, Lower Right", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 52, "prompt": "Does the lung look healthy?\n<image>", "pred_response": "Yes", "answer_id": "UrRGdF84Hccb8geRVtng4S", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 53, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "S7ebnkVKioyryYNapXSJMZ", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 54, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "DHvzJaBcoaueHfWUAS6M4W", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 55, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "Vk5YXXjWdP8C57bZPYzXq9", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 56, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "eAwi4U5VWbDBshzBJydx4j", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 57, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "iVX6aX6U7K8bQdRwHv44mA", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 58, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left Lung", "answer_id": "bded3sfsKJFtjEaLRetJuy", "gt_response": "Center", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 59, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "QaTJzrV8FWpaU58ry6YSka", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 60, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "BuctVbZyZ4hQSnbGaVfVvk", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 61, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "3XnagGeKrFiGNGxnFp5jr4", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 62, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "No", "answer_id": "CdqEvvPpF7NjmyS9SjhsMN", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 63, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "3UorjQTLXK4LCoeUASfNeS", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 64, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "iRpSiu5AyuCNu4HMxUBKW3", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 65, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "aPPNjZ64Vtn4kRpHsWvrs7", "gt_response": "Center", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 66, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "LrdwBu5q2shieputwecUuo", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 67, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "ALocPg7d9HsrLpwuH9jQsR", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 68, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "fai3LAvahZHmqBiuDczoiM", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 69, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "No", "answer_id": "QBT5XM3f7jsxuSz7kE9C48", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 70, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "P8HTuaWgY8WQYjBLtx9heE", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 71, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "g3sdQLmKRhzZNnvWwKyYMR", "gt_response": "Atelectasis, Mass", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 72, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "gRxXL3HtGjoxtGpnxoNvkS", "gt_response": "Right Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 73, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "28SLwfXFYMGNPpDmYhTX2r", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 74, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "BJgmr87pXoemkwwPzjoayg", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 75, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "c2yH387XmbjJW2gQpsM75p", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 76, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "PnAWL3uh9s55M8MTQ9zDYS", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 77, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "No", "answer_id": "hvVeT5PtpEcqxSYfq5WLMh", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 78, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "ELt4AP2AXK857bYUE5mK32", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 79, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "None", "answer_id": "8WdNEYBri8ihkcBoVJrW2h", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 80, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "k8xtwZ5QXuXbit6VXdWDU4", "gt_response": "Center", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 81, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "WenjvHzsZa45crNpfEKQWU", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 82, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "7g8STTHRauRpYHz6SVJDK9", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 83, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "To7RsfMMRg9Z3F3aMnQWfc", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 84, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "77qzoTeAvdbBjKCWa6DRNy", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 85, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "iVGuCDcLwKyUjs5nDsyuM3", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 86, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "None", "answer_id": "VhnSoQJxpvfayFezEr84eh", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 87, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "LtJnVswFzWbVjERPA8UJHB", "gt_response": "Center", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 88, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "fLHwbMMdfanHTnM2D4nkPQ", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 89, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "nEJRodkLybennZnmV8dygm", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 90, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "TpanXAP9aHCguD9uqMZUP3", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 91, "prompt": "Does this image look normal?\n<image>", "pred_response": "Yes", "answer_id": "URhTW7LTyR5ap3YSpfqZhp", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 92, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "Db4dmvCv39duZho7NFgDai", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 93, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "fx5HEaPehrRovueMGhMCUu", "gt_response": "Nodule", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 94, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "95Cd63HKG2Fb3msDmz8N65", "gt_response": "Left Lung, Right", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 95, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "2w9o3mUocQCbmvmhPqZtu7", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 96, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "aoyirnarttwaNoSz3WsnBU", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 97, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "BDgKNYvcrg4mu5jajWqexU", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 98, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "asQ4qWjjBP65xM3okiECfF", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 99, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "jYFn83SRKkkyqGycp6wUtx", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 100, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "BKfyvJaothrk8JCe7ZgeHg", "gt_response": "Nodule", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 101, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "hrUBYpJDxgTz3aj3ThxtGk", "gt_response": "Right Lung, Left", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 102, "prompt": "Does this image look normal?\n<image>", "pred_response": "Yes", "answer_id": "6hCSzT9LZAcQ6CJTZsjpHy", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 103, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "YrjBwpXUYLcWbb2HSAeas9", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 104, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "9NubK9ivXur7pdn9oCK5LM", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 105, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "3RY2sQqerGZKKbo3t3UH2c", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 106, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "4TVGsuFTvBMM2WqLduf3g8", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 107, "prompt": "Does this image look normal?\n<image>", "pred_response": "Yes", "answer_id": "T9vSge2fTmrm2VKkANEHwW", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 108, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "ZFthc8q7EsUMg5xPSHCWJi", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 109, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "eVbfewM9YbL78dcRedYKUr", "gt_response": "Atelectasis", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 110, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "Y2KvUfLHqMMPrzfKjLgayB", "gt_response": "Right Lung, Lower Left", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 111, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "ewpzVM8k9hxVPLKTe9GB9i", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 112, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "AxgdkSouSKaN8WpbmN84gc", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 113, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "Kviqg54QzJWFucnpevuvN6", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 114, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "XPymsEptTm8W4qxKmcswXp", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 115, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "NqaEJHpJeNozD2vqYY49Bs", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 116, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "oJ5F7MTCzZHBiDsBhb8von", "gt_response": "Pneumonia", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 117, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "AkEav8XnRr6FnFChTjMqyj", "gt_response": "Left Lung, Right", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 118, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "aetwhJHEhXb3sbvPuS5QjC", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 119, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "No", "answer_id": "HpsUmvpt7bHGb5TnhQ43Nd", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 120, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "4iwf3Z6LLFZzXSBu8kxBoq", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 121, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "GveXxbN8PJLjNhx5BU6oAw", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 122, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "8JdMHyUeMofGjher4mmUsP", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 123, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T1", "answer_id": "UqM2hP8msPMKFP3Rp8WXB7", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 124, "prompt": "How many kidneys are there in this image?\n<image>", "pred_response": "2", "answer_id": "Nm35ynhispVwFDAsj5T8Zg", "gt_response": "1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 125, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "RHybbpR8Yms3PDPXdpagTP", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 126, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "DbGyayMfc99qpadAiEpYRC", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 127, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "Yes", "answer_id": "YGhjmwMvd2Nyb8n52YdnKC", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 128, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "Qq46B3tpvaSXTswvFQuCXe", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 129, "prompt": "Does the liver look normal ?\n<image>", "pred_response": "Yes", "answer_id": "PTdXwbUfMkbVnpkyFzGgkz", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 130, "prompt": "Does the kidney look abnormal?\n<image>", "pred_response": "No", "answer_id": "UTBj9X5nLLspePaEPzo4gN", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 131, "prompt": "Do the spleen look abnormal?\n<image>", "pred_response": "No", "answer_id": "gg8Kd9dtWGhKMXrf33BrBf", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 132, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "Y8iSsGfYRMVQX4eVuw4goa", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 133, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "dsmLbnNeVTNxisHMEoLf8n", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 134, "prompt": "Does this image look abnormal?\n<image>", "pred_response": "No", "answer_id": "8BFoiuddAnEjeyV8ewQXoi", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 135, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "c85Rhvqrxtdcq8W8ubNUYV", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 136, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "None", "answer_id": "HRuqDqd5JncDQzMyUzVR43", "gt_response": "Pneumothorax", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 137, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "bqRHNormfZircDg3ViPPkN", "gt_response": "Right Lung, Left", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 138, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "jvFpVMeHjPRiHixcNb33Jo", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 139, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "No", "answer_id": "8PupKTZMh6bZnMpg55ZVH4", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 140, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "by7mHtS46A9oEDV65NQwfC", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 141, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "nL3HRuEwhbzUNDVN2m69dQ", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 142, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "PWeixmtFunczBuvubDsnR5", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 143, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "6pMVoqsoAJTn3qPj7BpnNZ", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 144, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "Kn4ojFUguQh9K6jTeRrkEt", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 145, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "Yes", "answer_id": "Bv4htkNTNEwrFtHkGkkaiZ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 146, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "HiH8KDP8t3C5z9DZZrzmPm", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 147, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "f9AvYs2CSGa9DzAijsA9GM", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 148, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "67rmUbXAquDuWmJamoKeh7", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 149, "prompt": "Where is the liver?\n<image>", "pred_response": "Upper Left", "answer_id": "PSBHmy6MoCSqLmQq68Pzgn", "gt_response": "Left", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 150, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "eFRfJHweUqSZnVLdxPVdSP", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 151, "prompt": "Is the liver healthy?\n<image>", "pred_response": "Yes", "answer_id": "cgEfbGBcxyBx7w79xFT5ri", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 152, "prompt": "Which is bigger in this image, kidney or liver?\n<image>", "pred_response": "Liver", "answer_id": "cHKtgihuMUjEeC6if92apc", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 153, "prompt": "What is the organ located in the lower right corner of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "mguYBs2RCiHdxCeCuUtqUg", "gt_response": "Left Kidney", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 154, "prompt": "What is the organ on the left side of this image?\n<image>", "pred_response": "Liver", "answer_id": "25gbEKUjTgqvKmQewYThCJ", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 155, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "Trg7jWnmy99mkQH2dvj7th", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 156, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "EgWwNaLQfhDexSvzdxFvhb", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 157, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "W77XgPFXdbZSKTXZnyiHw3", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 158, "prompt": "Does this image look normal?\n<image>", "pred_response": "Yes", "answer_id": "jScdjhF46aNYDYZQ4gmHQS", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 159, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "1", "answer_id": "KYXWW2evSnSn8gZnKnU4Gx", "gt_response": "4", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 160, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "ANz875EbecYztEXCqFHZ5s", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 161, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "dUfbFDc7B3WUVHPZNqv3hU", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 162, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "NzkDxdcDKruAh4EmupmYzA", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 163, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "ikfnCULRFT6FcjqUodRCT7", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 164, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "E9W94ki7Bhehqiv8k87bh8", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 165, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "Yes", "answer_id": "c9DAAcBTbTguaXh9NQAehb", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 166, "prompt": "Where is the liver?\n<image>", "pred_response": "Left", "answer_id": "RNEMY7FkkkyoMoFHLXyet9", "gt_response": "Top", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 167, "prompt": "Is the liver healthy?\n<image>", "pred_response": "Yes", "answer_id": "kSEVhLwV5Eb5QvZLm83dWo", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 168, "prompt": "Which is the biggest in this image, lung,heart or liver?\n<image>", "pred_response": "Lung", "answer_id": "maDnDdUqQxxporJmxs3DRJ", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 169, "prompt": "What is the rightmost organ in this image?\n<image>", "pred_response": "Liver", "answer_id": "Scw5evXezj7gSWGJzfWqV6", "gt_response": "Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 170, "prompt": "What is the organ on the left side of this image?\n<image>", "pred_response": "Liver", "answer_id": "4KHeYHoR5VozUXFQAVS5Fo", "gt_response": "Right Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 171, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "dpyXfW8fxANqikBhDR3cSq", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 172, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "SApCboXLvP3br4GJe7SeRb", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 173, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "AEmXbprNWmE5g3TW9Fh2ZG", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 174, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "A7mZc6XzFrdpv4gBCuNfhR", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 175, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "emKcgpPc3tj2SiSm5rTSxP", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 176, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "Yes", "answer_id": "DKifkVaNUotUQ3YwUSKyVk", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 177, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "P5dDHSocUo8atwWvxqWq7A", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 178, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "WNVECkDzFTYKJe53NYzCGF", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 179, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "Yes", "answer_id": "keUynZLoKsggGuf4Ptfos9", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 180, "prompt": "Where is the liver?\n<image>", "pred_response": "Left", "answer_id": "dznPMZZfUEHmt7CMkKS3LC", "gt_response": "Left", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 181, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "du9PhgjmnikUhepdN75K3Q", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 182, "prompt": "Which is the biggest in this image, spleen,lung, or liver?\n<image>", "pred_response": "Liver", "answer_id": "2rYPjuDSSezNN6trePcfni", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 183, "prompt": "What is the organ on the left side of this image?\n<image>", "pred_response": "Liver", "answer_id": "e3sVrWL3TgB8sTauz2GaD5", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 184, "prompt": "What is the organ on the lower right corner of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "QXiuCvzNLwWGPSoc7nYqkH", "gt_response": "Spleen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 185, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "gkbU4WhrT9L9DkVZNfv3SU", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 186, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "STtFYND4XC7Ac3CveNwMJ8", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 187, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "kTnoXN8seJMv5sr232ZpBQ", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 188, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "fJGG8fx6HoEzcPbsKGqAUM", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 189, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "JQUMiwr9toFwVkvkzaoKLQ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 190, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "Yes", "answer_id": "N6cQa2Qz5iqJmrPnT7vouT", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 191, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "BM9SFrQVtQhuizANQNRqNY", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 192, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Liver Cancer", "answer_id": "MrSHrFSazjt9ScUGsYLrmo", "gt_response": "Liver Cancer", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 193, "prompt": "Where is the liver?\n<image>", "pred_response": "Left", "answer_id": "VzPn9s4wViQYYroyPJ3rp6", "gt_response": "Left and top", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 194, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Liver", "answer_id": "hEY3D3x4tQV8GhdUFqvEiF", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 195, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "dtdiagviXBiW6fzrwHtfzM", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 196, "prompt": "Which is bigger in this image, kidney or spleen?\n<image>", "pred_response": "Kidney", "answer_id": "iDknsfFJFRbT6xrLevyJif", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 197, "prompt": "What is the organ on the left side of this image?\n<image>", "pred_response": "Liver", "answer_id": "TKwRsWjQBFWmYzDUN7zQo2", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 198, "prompt": "Does this image look normal?\n<image>", "pred_response": "No", "answer_id": "Mx4rk6RTErypXaWEcKWWiX", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 199, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "9ujFGe4qc9jCVRuuVWEZUM", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 200, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "gYMV8h8JeFMMUYKNxBb7jS", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 201, "prompt": "What is the rightmost organ in this image?\n<image>", "pred_response": "Liver", "answer_id": "QfJBi3F7vRNCtymAQcF9nP", "gt_response": "Spleen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 202, "prompt": "What is the organ on the top of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "3HFVXGadk4UcDfQ8SGYnhJ", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 203, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "D4yRz5EVBhqf7FbhJWohMG", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 204, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "hyVLLvFDfZQrCrq3igXyNn", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 205, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "o6NMoDnpT6K2eshKaN6Q3p", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 206, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "K8ezydczLDDrvaaR4HSCt4", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 207, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "bfnmrtTprJqLpEuWHcFpL2", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 208, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "MvopEUFf29QM5kXD8iYC5j", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 209, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "8qGtKpePYqVGF7BVDhGsum", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 210, "prompt": "Where is the liver?\n<image>", "pred_response": "Left", "answer_id": "fFeWSSzFvYkBwBhJDKwgMT", "gt_response": "Left and top", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 211, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Liver", "answer_id": "AHXiXKV5YNC9WsnDuUw5jo", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 212, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "eDgw7tArVRmY2HHaYY6SBT", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 213, "prompt": "Does this image look normal?\n<image>", "pred_response": "Yes", "answer_id": "GVoPzEUMiuXgBaF38EZrXZ", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 214, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "T7EYTdHQFYeZYjqy3iSnuf", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 215, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "3qMhtxCXC5hFoBkFqekjqo", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 216, "prompt": "Which is bigger in this image, liver or spleen?\n<image>", "pred_response": "Liver", "answer_id": "5QoNpP6mN6qLN5JE8HjZPy", "gt_response": "Spleen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 217, "prompt": "What is the organ on the lower right corner of this image?\n<image>", "pred_response": "Liver", "answer_id": "cSdNuEBa4W56F7p5PBtw8j", "gt_response": "Spleen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 218, "prompt": "What is the organ on the top of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "iusKYogFiX353L8yBd3Pcx", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 219, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "6ose7WZttyU37yXGL26VHX", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 220, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "hxtPRgXgzcGXfi2RQbgFrQ", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 221, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "NHAHwFCZuE7ojp9k7qgtHF", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 222, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T1", "answer_id": "BgwLvSPhDNXwK7TP2PhDZY", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 223, "prompt": "How many kidneys are there in this image?\n<image>", "pred_response": "2", "answer_id": "a5HeoWd3GQfbfRFG6utR9v", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 224, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "AsEgHaRnkK6MTaC2NcMvVv", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 225, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "XTarK8nmCH8BqGrmkvBoCS", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 226, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "HdjaLUhS9NqZXLvo2oNw6c", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 227, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "m7GD8LdZpvZESFVdPsfS5T", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 228, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "WdrUJ6Yb5Tqc52tehTWdcd", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 229, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T1", "answer_id": "eHwrcTYjxQQwbA9wF3y5RU", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 230, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "RMzGAWgPULfW4k3wDc6UKn", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 231, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "TzfNrrddKCTwt8vznCXpNr", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 232, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "WZgNwhagz4Gx6dQBAcXqAS", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 233, "prompt": "Where is the liver?\n<image>", "pred_response": "Upper Left", "answer_id": "Vna3HkKmPPdSVprwGqaY8E", "gt_response": "Not seen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 234, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "ND3tvuxR9S46MnWuFjzchu", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 235, "prompt": "What is the organ on the left side of this image?\n<image>", "pred_response": "Lung", "answer_id": "ftXTGScp3K6wk945v8ksAv", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 236, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "AYVSrPNtA5winLJi476oCX", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 237, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "FAfS6KztWstpspTeHDkoGw", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 238, "prompt": "Does this image look normal?\n<image>", "pred_response": "Yes", "answer_id": "JsLpoDhvE59J2TDHymUWVw", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 239, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "by9r9fQZz3oroGpYkwj7Sv", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 240, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "VoNhsqCVKUCjqsRVDFnePQ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 241, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "gzGS9ofHjZoRDdKq45bMV8", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 242, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "EryKyRCxKEiN2tuLt8q4vW", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 243, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "dW6JMQsxiGD8A5jFTFivtz", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 244, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "Yes", "answer_id": "htkbhVkiHExXCsaPQLLFKU", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 245, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Liver Cancer", "answer_id": "96GwdVVnyhc5ZhsjnkQVMc", "gt_response": "Liver Cancer", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 246, "prompt": "Where is the liver?\n<image>", "pred_response": "Upper Left", "answer_id": "GDX9JSCz4Zecf4SFMZVssd", "gt_response": "Upper Left", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 247, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Liver", "answer_id": "ikmD8v5Ud3p8Y49PBKnu6F", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 248, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "3zLdrth6gkCXLZa5LHgLWQ", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 249, "prompt": "Which is bigger in this image, liver or heart?\n<image>", "pred_response": "Liver", "answer_id": "afsNHhZTUcgAfe8QVSGEzZ", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 250, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "krUaMQdg3XaRVf4vKu47cK", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 251, "prompt": "What organ is the black part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "X4bkcsDwgybuaSGvX5fpas", "gt_response": "Right Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 252, "prompt": "What is the organ on the top of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "4F4PSVDSopK2txQ7jUuLXz", "gt_response": "Heart and Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 253, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "cHxxmExBtxebqrfdhiirtZ", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 254, "prompt": "What are the white points in the black part of the body?\n<image>", "pred_response": "Lung", "answer_id": "XWNpybNG2ANXFEGsSwgYd9", "gt_response": "Pulmonary bronchus", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 255, "prompt": "Which is the kidney?\n<image>", "pred_response": "Left", "answer_id": "FubBE5H4wshbzzMdMAMrSK", "gt_response": "Symmetrical to the bone marrow", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 256, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "Avc9NoRgEFbjcG7w4HW9Xb", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 257, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "Ry44N2zmRcF4DRQKubpKuQ", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 258, "prompt": "Does this image look normal?\n<image>", "pred_response": "Yes", "answer_id": "cpKeawSQvpvQSnrB52hSJw", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 259, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "iUxod2LPSXL9STpQBNigu6", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 260, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "BNLcgtfvfW7yVhBy7geJVi", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 261, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "Yes", "answer_id": "VhsweBEMVcaUYXtTeHTy2P", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 262, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "C859mr5eXmXiw5ncwyUWmH", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 263, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "CXwbQ9WGDMs8WYMdBcYsM9", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 264, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "GucSu9kxv5XHe3J3MrTVrB", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 265, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Liver Cancer", "answer_id": "MTwmLjGHJWFmhjCGrcvyBc", "gt_response": "Liver Cancer", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 266, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Liver", "answer_id": "dopSKPbNcBqEXCXPCPuXGy", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 267, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "P4kdGjYHkk7U3nNDwpF2BJ", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 268, "prompt": "Which is bigger in this image, kidney or spleen?\n<image>", "pred_response": "Kidney", "answer_id": "NNJVeN5GFwaEf5PiTfcRWp", "gt_response": "Spleen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 269, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "TebtFid8jv8P6PP9ptLsUj", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 270, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "MakaPRAaYS6CyKCokcNNze", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 271, "prompt": "What is the shape of the kidney in the picture?\n<image>", "pred_response": "Oval", "answer_id": "8HTzZkoTWE3ZRXHBEWQNRF", "gt_response": "Irregular", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 272, "prompt": "Which is the kidney?\n<image>", "pred_response": "Left", "answer_id": "gdXXL35rzmbRFoaMcvdStA", "gt_response": "Symmetrical to the bone marrow", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 273, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "YQs5gGxegKsU7sb95pTdVX", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 274, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "Sofv96HYr97ScdkBd5d3fV", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 275, "prompt": "Does this image look normal?\n<image>", "pred_response": "Yes", "answer_id": "8sikHwwGrSJAsEpkzzMvMX", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 276, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "JU7AzRpUydxz96UHqmRWex", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 277, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "b6MqGmTDxg9vKwEgzkXZLh", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 278, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "6z9qded5DSuRQkSRd44FST", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 279, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "TFBK7MHU2Wx4PqbrXXy9yL", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 280, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "H7tWr72fQy67E8a7JhYSWV", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 281, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "Yes", "answer_id": "iLsPKQXcwAgWPvrLfE7AM3", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 282, "prompt": "Where is the liver?\n<image>", "pred_response": "Left", "answer_id": "azNazchSmb82GsakN9wYw2", "gt_response": "Left", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 283, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "DB5WGVHXAZeFaL9RAkL4to", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 284, "prompt": "What organ is the black part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "Ec95RvcGmPJmrXGhWe7AGZ", "gt_response": "Right Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 285, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "fPAW2cTQaXagpeKaK3HTQW", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 286, "prompt": "What are the white spots in the black part of the lower part of body?\n<image>", "pred_response": "Liver", "answer_id": "ngmwP3XKo4gzf36dVXaaXg", "gt_response": "Pulmonary bronchus", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 287, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "NWpc8qPpYeBbnEAy8BWyxX", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 288, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "cYmeyvTZbfQQHSa8jjk43h", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 289, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "2T3XvPAxzRU3ZdKsu3FpZ8", "gt_response": "Small Bowel", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 290, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "4QtrqvqiGAq7w7rycKif4Q", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 291, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "Yes", "answer_id": "8wrBeBkZNz7suWCMZEhTSW", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 292, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "WaGXN8JqHFghkNjZd7XFGG", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 293, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "cNsdgVAK5XeqNMSRSr59M6", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 294, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "fUBnnC8PGupDdoJfbuu5zj", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 295, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "SR7oz2RYQrNf9x8fGUaEX9", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 296, "prompt": "What is the rightmost organ in this image?\n<image>", "pred_response": "Liver", "answer_id": "8iM23zcbac9caW23pqZTSx", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 297, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "bfQs9SVRo7DuCLMd5Uv3CH", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 298, "prompt": "What is the organ on the top of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "Lg6CoWxERB3d52cmTRoMxP", "gt_response": "Small Bowel", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 299, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "JVDc3gLuawantVb49vPHsa", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 300, "prompt": "Which is the kidney?\n<image>", "pred_response": "Left", "answer_id": "QwibqogRfakBfGAPYgzNDA", "gt_response": "Symmetrical to the bone marrow", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 301, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "ciVyJdZ8GEfvKd9vVqFQXW", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 302, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "FSzEpR3niDbrnvGzMaiuLJ", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 303, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "FNtGNXYtqwLxquXW7arM2p", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 304, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "D9UbKVDJZ8ArCT5CnXjKNy", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 305, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "SVLsfDb9Sfhoactb6yfA5n", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 306, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "TSjYGkYrHuKxxAFHJqPuG9", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 307, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "9Qd8JnoH4HmRMM79bvvxUs", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 308, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "74Dp9cLamwyjom82xMc76v", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 309, "prompt": "What is the rightmost organ in this image?\n<image>", "pred_response": "Liver", "answer_id": "eZVGz4uTaYpEf6CoQw7AhL", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 310, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "Zhxt9vwSbkwLbrndaeEUt9", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 311, "prompt": "What organ is the black part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "SErupWs4JoCF95aPgJFVV6", "gt_response": "Right Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 312, "prompt": "What is the organ on the top of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "3aN3CgF4weaWGxsZoWQh8d", "gt_response": "Stomach", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 313, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "D8xXFmBi5x9k3eYhVAPxfW", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 314, "prompt": "What organ is the black hole in the upper right corner?\n<image>", "pred_response": "Liver", "answer_id": "n5Ko9igcuDVHQDFZ5XSoTe", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 315, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "YY72YDioSgxuqenZxaKmQ6", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 316, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "gqaVisarPuVgcEaocheUUp", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 317, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "c6Vt3fMmLy4TFFS9VGxKwo", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 318, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "DsSYGUhvUoVVgSgkQqYuxi", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 319, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "Yes", "answer_id": "mHs6LjGCMj8kt3GXw8Zeib", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 320, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "EiDEfGXtmyVDQhLokAgX3r", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 321, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "3F76DChjnbv3tUWbbpawgr", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 322, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "9UF9sfdg7iADxberPyETMF", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 323, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "be9HXXB9T7wrVdBpPdZmTJ", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 324, "prompt": "Which is bigger in this image, liver or spleen?\n<image>", "pred_response": "Liver", "answer_id": "GbsSoKgzYTmmTvz7XdMidD", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 325, "prompt": "What is the rightmost organ in this image?\n<image>", "pred_response": "Liver", "answer_id": "kWCTFy6SvjfLShu5WDMAbf", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 326, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "Q9cJN9dpUamhnH4yEzcCHU", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 327, "prompt": "What organ is the black part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "aJF6BZErTCWNMbkdsi6tqE", "gt_response": "Right Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 328, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "mapt7rzRrQNhfkxUGtFDRS", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 329, "prompt": "What organ is the black hole in the upper right corner?\n<image>", "pred_response": "Liver", "answer_id": "GskGKEAkijqtsifMr8vJfo", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 330, "prompt": "What type of medical image is this?\n<image>", "pred_response": "CT", "answer_id": "TJQa2yPi9p2nArChvrcW4H", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 331, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "8dNrxGrbDy8kQ3hJvXgUWv", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 332, "prompt": "Which part do the organs in this image locate from the aspect of the whole body?\n<image>", "pred_response": "Left", "answer_id": "hoiXe5xyCp3xp7XyLMEj3y", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 333, "prompt": "Which side of lung is abnormal in this image,left or right?\n<image>", "pred_response": "Left", "answer_id": "8P7xpCmxNpqE2PKRma5m5C", "gt_response": "Right", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 334, "prompt": "What part of the lung is the pneumothorax located in?\n<image>", "pred_response": "Right", "answer_id": "d3VAoWSM698RMtESHn2quk", "gt_response": "Upper Right Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 335, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Lung", "answer_id": "8XP2SiaiptnAwnZETkW2UN", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 336, "prompt": "Which organs appear in pairs?\n<image>", "pred_response": "Lung", "answer_id": "YtraLYF4rRcmuYVGSxvNRb", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 337, "prompt": "Is/Are there abnormalities in the patient's right lung?\n<image>", "pred_response": "Yes", "answer_id": "PNjmkdkBgxjrSL58Dqzfgw", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 338, "prompt": "Is this image normal?\n<image>", "pred_response": "Yes", "answer_id": "GdeGuYNFwicDPp9HS84bkK", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 339, "prompt": "Can pneumothorax be observed on the lower left lung in this picture?\n<image>", "pred_response": "No", "answer_id": "NrVeyEWYV9QfCaPAcUqK27", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 340, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "gxoifHxRg53uN68St8qk2K", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 341, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "Q7A8pp5uTLzQtUeMEVsu5H", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 342, "prompt": "What is the effect of the main organ in this picture?\n<image>", "pred_response": "Lung", "answer_id": "VszbojgLW8DWq6MpWPnPYs", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 343, "prompt": "What kind of symptoms will the disease on the upper right of lung bring to the patient?\n<image>", "pred_response": "Chest pain", "answer_id": "4xzTnkMRzdtkA9xR7eBKvA", "gt_response": "Chest pain, dyspnea", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 344, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "cZBCuk6SULyoGDbYMS9zs7", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 345, "prompt": "What imaging modality was used?\n<image>", "pred_response": "CT", "answer_id": "CCDTJxZ7M8KXfXDeAGkb2g", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 346, "prompt": "Is this a transverse plane?\n<image>", "pred_response": "No", "answer_id": "KyveGLyQ5B2bDQ43xuTPbh", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 347, "prompt": "Which part of the human body is the organ located in the image?\n<image>", "pred_response": "Abdomen", "answer_id": "T4HpSLQ5TNutMvcKN7dBNL", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 348, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Lung", "answer_id": "XxrbXoUFQgRccarY5nVRFP", "gt_response": "Lower Right Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 349, "prompt": "Where is the atelectasis in this image?\n<image>", "pred_response": "Right Lung", "answer_id": "jDHuZWmoX29mGhjZzK787p", "gt_response": "Lower Right Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 350, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Lung", "answer_id": "2nkdzAVDq3Cwg9Fh3nn3TG", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 351, "prompt": "Is this a study of the chest?\n<image>", "pred_response": "Yes", "answer_id": "BEdzicokcGc2hsiw5vVQWP", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 352, "prompt": "What disease is/are shown on the right of lung?\n<image>", "pred_response": "Lung Cancer", "answer_id": "UXZVaM64CooZiVGKTEwxhX", "gt_response": "Atelectasis", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 353, "prompt": "Is/Are there abnormalities in the patient's right lung?\n<image>", "pred_response": "Yes", "answer_id": "cjtTA5WqvUww4pA4vsVmXA", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 354, "prompt": "Can atelectasis be observed on the upper right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "GppzgqDYBQN7JnmVBHBFgQ", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 355, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "9b6CWLMzd4KfVra3vojZGi", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 356, "prompt": "How many existing lungs in this image?\n<image>", "pred_response": "2", "answer_id": "5tQMudEMvfr2DoYvdNujtq", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 357, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "Lung", "answer_id": "8AwgS3hxPshEFxdR6oyTFY", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 358, "prompt": "What is the main cause of the disease on the lower right of the lung in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "2kAv4ANFoQtVe7nMnPsppv", "gt_response": "Bronchial obstruction", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 359, "prompt": "Which organs/organ in the picture belong to the circulatory system?\n<image>", "pred_response": "Heart", "answer_id": "HRDYmNHgxdcsrH4B99WwPQ", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 360, "prompt": "Is this image taken via X-Ray?\n<image>", "pred_response": "No", "answer_id": "a3Rb9kv3pppfZS5JRcAuv8", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 361, "prompt": "Is this a transverse plane?\n<image>", "pred_response": "No", "answer_id": "GdxFDd37Wnndgx3AWd8Hz3", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 362, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "nkC5eZM5ALHsqBNCmeTwiH", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 363, "prompt": "Which side of lung is abnormal in this image,left or right?\n<image>", "pred_response": "Left", "answer_id": "GZnyoekx4ADv4AiHMqy95p", "gt_response": "Left", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 364, "prompt": "Where is the mass in this image?\n<image>", "pred_response": "Lung", "answer_id": "egZHqgDsCmMvp2poWbY3cg", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 365, "prompt": "Is this a study of the chest?\n<image>", "pred_response": "Yes", "answer_id": "7c75mwsCNkchHdRnK9jYhX", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 366, "prompt": "Is this a study of the pelvic cavity?\n<image>", "pred_response": "No", "answer_id": "G6k3tRNTRFMrUtmFaqKj7x", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 367, "prompt": "What disease is/are shown on the left of lung?\n<image>", "pred_response": "Lung Cancer", "answer_id": "UYwhq8GqJJYdqkQcRq2W22", "gt_response": "Pulmonary Mass", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 368, "prompt": "Is the lung abnormal?\n<image>", "pred_response": "No", "answer_id": "en8TSkeeETAHinHWQSer6r", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 369, "prompt": "Can mass be observed on the lower left lung in this picture?\n<image>", "pred_response": "No", "answer_id": "asPPEgtLRUPWoavZGM9bY2", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 370, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "White", "answer_id": "msDwYsDVvsWS7Tx4Czcbtb", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 371, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "2", "answer_id": "HexXm9nZHjDycdfAWFRdQZ", "gt_response": "1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 372, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "4h49Dxv56uAZdgE39DfX3u", "gt_response": "A Little", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 373, "prompt": "How to prevent the occurrence of the disease on the lower left lobe of lung in this image?\n<image>", "pred_response": "Avoid smoking", "answer_id": "RvxbzuyuzGZG4dNZjLakaz", "gt_response": "Keep healthy", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 374, "prompt": "Which organs/organ in the picture belong to the circulatory system?\n<image>", "pred_response": "Heart", "answer_id": "N45CLzcqZgQfbrzfKPjBa8", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 375, "prompt": "What type of medical image is this?\n<image>", "pred_response": "CT", "answer_id": "MXzs3MEC7tLUZbyFfLkBaj", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 376, "prompt": "Is this a coronal section?\n<image>", "pred_response": "No", "answer_id": "Nhmf6PkKJSrtHWStcTFm9R", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 377, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "FLHn9mvzWzbU4ojfRXB9Cz", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 378, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Lung", "answer_id": "TzjrXXy6dSCUSDiVdSxGdc", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 379, "prompt": "Where is the atelectasis in this image?\n<image>", "pred_response": "Lung", "answer_id": "E5peahGo6xXereiY9jYtiE", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 380, "prompt": "Is this a study of the neck?\n<image>", "pred_response": "No", "answer_id": "URmrPdCunc26M2SDdqNKrr", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 381, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "MAmUx9ghn3BL5av3xS278x", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 382, "prompt": "Does the patient have any abnormalities?\n<image>", "pred_response": "No", "answer_id": "bx4mnSzVwV7quyn8PBXUeA", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 383, "prompt": "Is this image normal?\n<image>", "pred_response": "Yes", "answer_id": "RHRKmMNTWH42UcqZ5ypK5u", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 384, "prompt": "Can atelectasis be observed on the lower left lung in this picture?\n<image>", "pred_response": "No", "answer_id": "UdJTCZ76L5aPvJULRNHCZZ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 385, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "9i48TJs8VntDXKdgwNvVJG", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 386, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "K6rKEDNkk4sVsthPNHDuDz", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 387, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "8t84GeKr9nJ3SAx67kgeyx", "gt_response": "A Little", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 388, "prompt": "How to treat the disease located on the left of lung in this image?\n<image>", "pred_response": "Lung Cancer", "answer_id": "6rzEAib4QB9dNbmjBNgZj3", "gt_response": "Physical therapy, surgical treatment", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 389, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "es2BCzSeyoiaQRGeujBW8q", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 390, "prompt": "Is this image taken via X-Ray?\n<image>", "pred_response": "No", "answer_id": "3a5uYfoQPYQCR2R8PTF44E", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 391, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "Z4QpSAq6RH3gyWcuin4Xbe", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 392, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "aQBHAoRv56YaNEZQ4MunGR", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 393, "prompt": "Which side of lung is abnormal in this image,left or right?\n<image>", "pred_response": "Left", "answer_id": "CLd9sBxhpZdtP8yXgVu5h7", "gt_response": "Right", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 394, "prompt": "Where is the pneumonia in the lung?\n<image>", "pred_response": "Left", "answer_id": "XHPPrYYFMLsbcSSERQveGf", "gt_response": "Right Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 395, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Respiratory", "answer_id": "fRx5uf6QR8M4WXSh8tLD9g", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 396, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Lung", "answer_id": "LY877GXwkhUdfkFgQNQfFm", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 397, "prompt": "What disease is/are shown on the right of lung?\n<image>", "pred_response": "Lung Cancer", "answer_id": "3uXaD8xmX9twRhCkALSK7S", "gt_response": "Pneumonia", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 398, "prompt": "Does the lung look abnormal?\n<image>", "pred_response": "No", "answer_id": "MvHts4LhtEeEQtQznZRmiL", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 399, "prompt": "Can pneumonia be observed on the right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "NvZwXiH4NL8cHfWKpDJfjF", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 400, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "XmbVywykNyC65jsGE4N9hm", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 401, "prompt": "How many existing lungs in this image?\n<image>", "pred_response": "2", "answer_id": "QkuFB4DdPKQPMLR249VGTM", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 402, "prompt": "What can the organ with black color in this image be used for?\n<image>", "pred_response": "Breathe", "answer_id": "SpUHHM2STpVSzLJtkSP323", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 403, "prompt": "How to treat the disease located on the right of lung in this image?\n<image>", "pred_response": "Lung Cancer", "answer_id": "TDcLd6Vo2BUwjGXCQf8obU", "gt_response": "Medical therapy, supportive therapy", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 404, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Lung", "answer_id": "GsDYeECrQ8y5SgxCsu4GN4", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 405, "prompt": "What modality is shown in this image?\n<image>", "pred_response": "CT", "answer_id": "ivajm9bp9UfEZsWvQqWjtn", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 406, "prompt": "Which plane is the image scanned, transverse plane or coronal plane?\n<image>", "pred_response": "Transverse Plane", "answer_id": "j5FbRmFxJCkcBsWEmkAWEi", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 407, "prompt": "Do the organs in the image exist in the abdomen?\n<image>", "pred_response": "Yes", "answer_id": "7EbnGj3ipqAiCKN3XQQm35", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 408, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Lung", "answer_id": "Qc3HLZMMPPDUBustnZ2vt3", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 409, "prompt": "Where is the cardiomegaly in this image?\n<image>", "pred_response": "Left", "answer_id": "YdC2j87jsi4d6NwqDBGFRN", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 410, "prompt": "Which organs appear in pairs?\n<image>", "pred_response": "Lung, Liver", "answer_id": "5iybhxazT2prgAg8Siiemq", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 411, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Respiratory", "answer_id": "eNbB6T9zysjLWTGisZfUQT", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 412, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "None", "answer_id": "Q8wioRrrxY28xae7JGgKRp", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 413, "prompt": "Is this image normal?\n<image>", "pred_response": "Yes", "answer_id": "HVCWmUpP4uUcHTe6ndoQj4", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 414, "prompt": "Can cardiomegaly be observed on the heart in this picture?\n<image>", "pred_response": "Yes", "answer_id": "3NBYFY796Nq5iEudkrmmZW", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 415, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "HcZsobngzY5cJH3W8GHcmu", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 416, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "12", "answer_id": "U5LM6gdZDdUivYET8dEGVa", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 417, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "Lung", "answer_id": "9ktLME8yBG4z8f38oF88ke", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 418, "prompt": "What kind of symptoms will the disease on the heart in this image bring to the patient?\n<image>", "pred_response": "Chest pain", "answer_id": "ccYkKbnSBYTtVTcahpJKsj", "gt_response": "Chest tightness, arrhythmia", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 419, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Heart", "answer_id": "86hJdK9Vbv9y2RynxPU6NJ", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 420, "prompt": "What type of medical image is this?\n<image>", "pred_response": "CT", "answer_id": "X2jpLcxXXFGWar7Q9zSijr", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 421, "prompt": "Is this sagittal plane?\n<image>", "pred_response": "No", "answer_id": "gdismCiMyyS6ftzCBuG5F2", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 422, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "92Eecb9UJiHmUbbCKe98Jq", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 423, "prompt": "Which lobe is abnormal in this image?\n<image>", "pred_response": "Left", "answer_id": "F2WqLziW5fX7KSKTxerqgW", "gt_response": "Left", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 424, "prompt": "Where is the pulmonary infiltration in this image?\n<image>", "pred_response": "Left Lung", "answer_id": "KXdFNALcBFnpsSKW7HRr3W", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 425, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Lung", "answer_id": "LuuEGHJJLjEJ8TB24MhGy4", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 426, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Lung", "answer_id": "WLU89D7rxjfXvBoc9EvWaX", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 427, "prompt": "Does the lung look abnormal?\n<image>", "pred_response": "No", "answer_id": "QNJpJcwugGtwuvEjd6cPaE", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 428, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "VaK2VVcMJDU5CM8pZuCGu7", "gt_response": "Pulmonary Infiltration", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 429, "prompt": "Can pulmonary infiltration be observed on the upper right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "SGcKVt7BW8xLeJEYBw6NoT", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 430, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "HWBXF7By33zAyWpsDRBP6S", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 431, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "1", "answer_id": "iwHAndyhTqAyUh5Tnz6na2", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 432, "prompt": "What is the effect of the main organ in this picture?\n<image>", "pred_response": "Lung", "answer_id": "RQEGDjbmiAvGNiAunVbHgh", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 433, "prompt": "How to prevent the occurrence of the disease on the lower left lobe of lung in this image?\n<image>", "pred_response": "Avoid smoking", "answer_id": "XZYkCyyz9hyFJP5Q4k7dSe", "gt_response": "Live healthy, enhance physical fitness", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 434, "prompt": "Which organs/organ is part of the respiratory system in this image?\n<image>", "pred_response": "Lung", "answer_id": "79cEEFw656MmQQFp8HKtq2", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 435, "prompt": "What imaging modality was used?\n<image>", "pred_response": "CT", "answer_id": "fErwR5cG2DCfxRr5t66mK8", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 436, "prompt": "Which plane is the image scanned, transverse plane or coronal plane?\n<image>", "pred_response": "Transverse plane", "answer_id": "Jkz2WBvufEg5XYzMvj96hC", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 437, "prompt": "Which part do the organs in this image locate from the aspect of the whole body?\n<image>", "pred_response": "Left", "answer_id": "PpbpKw9wRDi2LR5yCuawuP", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 438, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Left Lung", "answer_id": "Xu8JiLprUxNnDZBtQ4XQJQ", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 439, "prompt": "Where is the pulmonary nodule located in this image?\n<image>", "pred_response": "Left Lung", "answer_id": "GG7vNhGJiGMe6nZ2rtvkbr", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 440, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Lung", "answer_id": "S3gSvMFrrbhtmfYCnW6VKz", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 441, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Respiratory", "answer_id": "2cqBrWazH3wJuiButhDMF3", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 442, "prompt": "Is the lung normal appearing?\n<image>", "pred_response": "Yes", "answer_id": "mbyziMRH7vEvLed6mcRDF4", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 443, "prompt": "Is this image normal?\n<image>", "pred_response": "Yes", "answer_id": "TSu59VKRiWGVP5X2LyUW3Q", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 444, "prompt": "Can nodule be observed on the lower left lung in this picture?\n<image>", "pred_response": "Yes", "answer_id": "XoSryiJDZ5ayqhMGU8bi5b", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 445, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "ZLcYBbfKAiFSYJG3gYXyU4", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 446, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "AHXUh3YcWUnkHQZRE7EnE3", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 447, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "Lung", "answer_id": "dFhK6YuhQEMGaLaR5SGJcG", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 448, "prompt": "How to treat the disease located on the lower left of lung in this image?\n<image>", "pred_response": "Lung Cancer", "answer_id": "ZCpgA3ZTWFBdsddhkSNeiW", "gt_response": "Medical treatment, surgical treatment", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 449, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Heart", "answer_id": "Vp5Ki6t7sC5hofXNruCXvd", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 450, "prompt": "What imaging modality was used?\n<image>", "pred_response": "CT", "answer_id": "6Ra2yMSRNASDfPavQZcBvv", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 451, "prompt": "Which plane is the image scanned, transverse plane or coronal plane?\n<image>", "pred_response": "Transverse plane", "answer_id": "cBTmHnWFsNhAZ9GQapB3Na", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 452, "prompt": "Which part of the human body is the organ located in the image?\n<image>", "pred_response": "Abdomen", "answer_id": "R7m8Wt2RodemMgr5M68Qwm", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 453, "prompt": "Where is/are the abnormality located in this picture?\n<image>", "pred_response": "Lung", "answer_id": "nUx6CZ3bStKXNce9EK9KUP", "gt_response": "Upper Right Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 454, "prompt": "Where is the nodule in this image?\n<image>", "pred_response": "Right Lung", "answer_id": "hQysWQYNg6W64wbFLhzhVi", "gt_response": "Upper Right Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 455, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Lung", "answer_id": "T8PEmBSRcEMjtiZsMFNC9d", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 456, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Lung", "answer_id": "99WHywbEKG3Y7w8tNtKFGH", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 457, "prompt": "Does the patient have any abnormalities?\n<image>", "pred_response": "Yes", "answer_id": "kBZjADiyGmeSTkRrRDSuc5", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 458, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "3RqBdbVYHqwMcWGHLDG7Cy", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 459, "prompt": "Can nodule be observed on the upper right lung in this picture?\n<image>", "pred_response": "Yes", "answer_id": "AmyoHwwhFJ55SM5eoN6oLf", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 460, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "gZdFuE435e5fpKzzsK2UMn", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 461, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "fFmNe5BdbVRiPTz88H83uD", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 462, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "G2bjQBjCKguEyQQNoM7DGt", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 463, "prompt": "What kind of symptoms will the disease on the upper right lung bring to the patient?\n<image>", "pred_response": "Chest pain", "answer_id": "TAs9wGCGjtnoLAsUJdnZDQ", "gt_response": "Chest tightness, fatigue", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 464, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "L5Uxevq3mfWfEZpncFU4Wj", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 465, "prompt": "Is this an MRI scan?\n<image>", "pred_response": "No", "answer_id": "U6B35aSqqbuzBUY3NseUU3", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 466, "prompt": "Is this a coronal plane?\n<image>", "pred_response": "Yes", "answer_id": "SFdcUbHKQFFvzzjTVni2Va", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 467, "prompt": "Do the organs in the image exist in the chest?\n<image>", "pred_response": "Yes", "answer_id": "YfjMvxUMZwremsU9nZSKqW", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 468, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Lung", "answer_id": "UyqoncfPeMFZhZ5fQs9UqP", "gt_response": "Upper Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 469, "prompt": "What part of the lung is the pneumothorax located in?\n<image>", "pred_response": "Right", "answer_id": "DioQAexESnzKGHFrT9rQAA", "gt_response": "Upper Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 470, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Lung", "answer_id": "8TADTr6XkDpv5ta6JzFnu8", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 471, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Respiratory", "answer_id": "DXxeRoqdxPkTv6rHk7nxZU", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 472, "prompt": "Is the lung normal appearing?\n<image>", "pred_response": "Yes", "answer_id": "eaTHt98f5RVqjoWWPJyhaP", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 473, "prompt": "Is/Are there abnormalities in the patient's left lung?\n<image>", "pred_response": "No", "answer_id": "aY7SGgPpmy2dg9CBP4J6Dm", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 474, "prompt": "Can pneumothorax be observed on the lower right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "KU4PLtLphQP9BmVryKNohZ", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 475, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "Gray", "answer_id": "H4Q3A2vLFiahuqSxEeTNr3", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 476, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "1", "answer_id": "83yQL7af9hEdySxjGqJAHZ", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 477, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "ZHbRVC64vuyaygECmKoBKj", "gt_response": "A Little", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 478, "prompt": "How to treat the disease located on the upper left of lung in this image?\n<image>", "pred_response": "Lung Cancer", "answer_id": "XznsmcWDXbqc79kpFvqMiz", "gt_response": "Medical therapy, supportive therapy", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 479, "prompt": "Which organs/organ in the picture belong to the circulatory system?\n<image>", "pred_response": "Heart", "answer_id": "FhhF4DiMwC27pSGM8Dr3NB", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 480, "prompt": "Is this a X-Ray?\n<image>", "pred_response": "No", "answer_id": "bwVx6JFGKUafmb7ECmPh42", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 481, "prompt": "What scanning plane does this image belong to?\n<image>", "pred_response": "Transverse Plane", "answer_id": "HsjWzztGozEpbR6hGERiWa", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 482, "prompt": "Do the organs in the image exist in the abdomen?\n<image>", "pred_response": "Yes", "answer_id": "EknPLkdL2JPpPzGgXZuXd7", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 483, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Right Lung", "answer_id": "M8tx3a2be3nfZodS9WRU6x", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 484, "prompt": "Where is the pneumothorax in this image?\n<image>", "pred_response": "Left Lung", "answer_id": "DxnaG9ouzv5zEjXxbzgcpv", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 485, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Respiratory", "answer_id": "2nb2VPMD6WdMGNDiiJUJqM", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 486, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Abdomen", "answer_id": "5h2L2NvMnW2NN9ioZZiEhh", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 487, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "No", "answer_id": "YLU3nrEEBSZMdd4bG2VhPm", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 488, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "BQEsUJM9Wfz8jNpGqib7Yz", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 489, "prompt": "Can pneumothorax be observed on the lower left lung in this picture?\n<image>", "pred_response": "No", "answer_id": "YR28j4gj6fK6ycDWZsmChb", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 490, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "White", "answer_id": "ZrbaNrwJm4k7Kg3Rza32sS", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 491, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "1", "answer_id": "LBMCAVyCbgUhExR2F8SuR4", "gt_response": "1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 492, "prompt": "What is the effect of the center organ in this picture?\n<image>", "pred_response": "Lung", "answer_id": "VfcCmQ2d5ptFxnWHNhECqX", "gt_response": "Promote blood flow", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 493, "prompt": "What is the main cause of the disease on the lower left of the lung in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "PHD8ko2EamaPL9xkjJaPs3", "gt_response": "Chest injury, lung disease, bullae", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 494, "prompt": "Which organs/organ in the picture belong to the circulatory system?\n<image>", "pred_response": "Heart", "answer_id": "7RuhKERZibMZukJP6dMaKE", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 495, "prompt": "Is this image taken via X-Ray?\n<image>", "pred_response": "No", "answer_id": "mcBCyJUweiPs9xs6x79dQi", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 496, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse Plane", "answer_id": "maFykdhu8tUzWs2WUSRiM6", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 497, "prompt": "Which part of the human body is the organ located in the image?\n<image>", "pred_response": "Chest", "answer_id": "2vhq5VksYUteFVqUfeB9Yx", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 498, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Lung", "answer_id": "kVZc4HRZyMXoxugRSY2UtK", "gt_response": "Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 499, "prompt": "Where is/are the nodule located in this image?\n<image>", "pred_response": "Right Lung", "answer_id": "dWz9AjjaGLfFzE9DLfrZAw", "gt_response": "Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 500, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "czePKgLre5fw4NnqaiSW6t", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 501, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Lung", "answer_id": "UaWU5wT5htorvKaMK45FFd", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 502, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "GtAGeWCxSWBuJ2maJ4RT6X", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 503, "prompt": "Is the lung abnormal?\n<image>", "pred_response": "No", "answer_id": "5NnXmR8XQKKsBQeH95q9ZU", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 504, "prompt": "Can nodule be observed on the lower right lung in this picture?\n<image>", "pred_response": "Yes", "answer_id": "fTEfCNd9BnZFTggsRSJkZS", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 505, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "White", "answer_id": "f9vVWehkYKtyATLNspEgeD", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 506, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "1", "answer_id": "P9Sy3a2XtmtTRx5KPgiLAu", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 507, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "8p2byj247PfCkieSzLLoTW", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 508, "prompt": "How to prevent the occurrence of the disease on the left lobe of lung in this image?\n<image>", "pred_response": "Avoid smoking", "answer_id": "Dd784v2Z2ZdXgRKRjYVTma", "gt_response": "Quit smoking, enhance physical fitness", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 509, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "6FepsEwqjcy2Y8WWymjv57", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 510, "prompt": "Is this image taken via X-Ray?\n<image>", "pred_response": "No", "answer_id": "baMD7dcNKyvTvSp8Y9wZj2", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 511, "prompt": "What scanning plane does this image belong to?\n<image>", "pred_response": "Transverse Plane", "answer_id": "CGFSVZBcdr5yr3nqobnMn7", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 512, "prompt": "Where does the picture show?\n<image>", "pred_response": "Chest", "answer_id": "9iuewuXM8y5gBk7pDxx4WB", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 513, "prompt": "Which side of lung is abnormal in this image,left or right?\n<image>", "pred_response": "Left", "answer_id": "6y4sWatH6b7ahUwSLvxDz4", "gt_response": "Left", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 514, "prompt": "What part of the lung is the pneumonia located in?\n<image>", "pred_response": "Right", "answer_id": "D79Lsseb23GChpun9k7XkK", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 515, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Lung", "answer_id": "F8nW8AGooXXEPtr6XKW5Wv", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 516, "prompt": "Which organs appear in pairs?\n<image>", "pred_response": "Lung, Heart", "answer_id": "amg3EbTRJVgVMmM3pEJHGG", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 517, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "No", "answer_id": "gS8k8m5bJJWBGicuZDnCtt", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 518, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "9J3nieb2wZtaqLaziv8FaA", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 519, "prompt": "Can pneumonia be observed on the upper right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "Md6qGnhJH7GHk3CbBqLiWy", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 520, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "oQvucXi8sPBiLiSzmUwSwL", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 521, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "gLWMELZ9kbErcpzK7Su6nP", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 522, "prompt": "What is the effect of the center organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "fxBTNZBcG3JmqcUGhZVnjJ", "gt_response": "Promote blood flow", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 523, "prompt": "How to treat the disease located on the lower left of lung in this image?\n<image>", "pred_response": "Lung Cancer", "answer_id": "TGxwrp4tWWy2aamWQ2znaB", "gt_response": "Medical therapy, supportive therapy", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 524, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "gekzFis8pbUptvgvHn5TFx", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 525, "prompt": "What type of medical image is this?\n<image>", "pred_response": "CT", "answer_id": "Tfk7nxtKJxjHp7nppan6mR", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 526, "prompt": "Is this a coronal plane?\n<image>", "pred_response": "Yes", "answer_id": "T9LQHaEkkHEQcEER2C9JdA", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 527, "prompt": "Which part do the organs in this image locate from the aspect of the whole body?\n<image>", "pred_response": "Left", "answer_id": "VerrKrpW3SxJMy2NL8PUFk", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 528, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Right Lung", "answer_id": "ZKwrn37xfCQ85gjH3mbDvh", "gt_response": "Upper Right Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 529, "prompt": "What part of the lung is the pneumonia located in?\n<image>", "pred_response": "Right", "answer_id": "Pc3oh3LMGGAz8PNSe7gGuh", "gt_response": "Upper Right Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 530, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Abdomen", "answer_id": "KFDGZSQAK4GcNqmgFaCZTC", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 531, "prompt": "Is this a study of the neck?\n<image>", "pred_response": "No", "answer_id": "mcPLhDobDuLWmkntHPTGjR", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 532, "prompt": "Does the lung look abnormal?\n<image>", "pred_response": "No", "answer_id": "iReaadMbdboKcVBmnDbFRj", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 533, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "VxNWWtEHoZxSuCrsVGgCKn", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 534, "prompt": "Can pneumonia be observed on the upper right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "S4r4Q3cCV5TzdiPUAXDJms", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 535, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "White", "answer_id": "7qMsVWEoHynsjSsp4KiYtT", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 536, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "1", "answer_id": "ZUSnXU72kTVA6fziBVPxsE", "gt_response": "1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 537, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "Digest", "answer_id": "9DDgf5TZaLPi3w22puTvTY", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 538, "prompt": "How to prevent the occurrence of the disease on the upper right lobe of lung in this image?\n<image>", "pred_response": "Avoid smoking", "answer_id": "bRzxEaNUaVZ9346czgM3g4", "gt_response": "Pay attention to prevent cold and keep warm, enhance physical fitness", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 539, "prompt": "Which organs/organ is part of the respiratory system in this image?\n<image>", "pred_response": "Lung", "answer_id": "Kk6WYzgfiFsNAArYec9HRP", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 540, "prompt": "How was this image taken?\n<image>", "pred_response": "CT", "answer_id": "Kcjt4DDLEzBFZkPfMVPkDT", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 541, "prompt": "Is this a transverse section?\n<image>", "pred_response": "No", "answer_id": "CTQSdqUqJxvwEn4P2kZFtd", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 542, "prompt": "Where does the picture show?\n<image>", "pred_response": "Lung", "answer_id": "6LnkWWXh3pwiSvebqKjA4S", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 543, "prompt": "Which side of lung is abnormal in this image?\n<image>", "pred_response": "Right", "answer_id": "BitaG5ZQtLopPGthZ8Ephz", "gt_response": "Right", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 544, "prompt": "Where is the pneumonia in the lung?\n<image>", "pred_response": "Left", "answer_id": "gkje3kWPCu49qLgf8rindv", "gt_response": "Lower Right Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 545, "prompt": "Is this a study of the chest?\n<image>", "pred_response": "Yes", "answer_id": "aXXZisnMFjqAsCJ4TP8btS", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 546, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Lung", "answer_id": "Ewm56SgoTy3hb6yYEhm8L8", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 547, "prompt": "What disease is/are shown on the right of lung?\n<image>", "pred_response": "Lung Cancer", "answer_id": "m935pPr3DfRMSnjeTYVCPP", "gt_response": "Pneumonia", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 548, "prompt": "Is the lung normal appearing?\n<image>", "pred_response": "Yes", "answer_id": "WAPNKihyKXmnZHBccA5iWv", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 549, "prompt": "Can pneumonia be observed on the lower right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "XXxkPLcfLB7hLjRDjt9q8b", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 550, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "5rFBNnQTtanxaQPuVhx6qU", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 551, "prompt": "How many existing lungs in this image?\n<image>", "pred_response": "2", "answer_id": "ZaVR7kNVYCd8WUfrZqRQjE", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 552, "prompt": "What is the effect of the center organ in this picture?\n<image>", "pred_response": "Lung", "answer_id": "4nGRmUEE8kFrZfK8r5SosE", "gt_response": "Promote blood flow", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 553, "prompt": "How to prevent the occurrence of the disease on the lower right lobe of lung in this image?\n<image>", "pred_response": "Lung Cancer", "answer_id": "TfFB7TXiPf2GTv6F6agNjq", "gt_response": "Pay attention to prevent cold and keep warm, enhance physical fitness", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 554, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Heart", "answer_id": "55wMtLmRurQzhPbgZx5aKU", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 555, "prompt": "Is this image taken via CT?\n<image>", "pred_response": "No", "answer_id": "Lz5AKmBfTGq8BvT6PcuCfR", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 556, "prompt": "Is this a transverse plane?\n<image>", "pred_response": "No", "answer_id": "FeQ94fwuRAUBT6oHnrzq7d", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 557, "prompt": "Where does the picture show?\n<image>", "pred_response": "Lung", "answer_id": "Wv3XgWMsLrevfvcHr6p76N", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 558, "prompt": "Which side of lung is abnormal in this image?\n<image>", "pred_response": "Right", "answer_id": "ZSqtvf7MVFtqoDAwHT3cfP", "gt_response": "Left", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 559, "prompt": "What part of the lung is the infiltration located in?\n<image>", "pred_response": "Right", "answer_id": "PgurHQFgWzZpnh8uRTvdQp", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 560, "prompt": "Which organs appear in pairs?\n<image>", "pred_response": "Lung", "answer_id": "My8fmNPhikfbRH4FPpmoNW", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 561, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "Ya5JqkbTypxGEjz4XWgCFY", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 562, "prompt": "Is/Are there abnormalities in the patient's left lung?\n<image>", "pred_response": "Yes", "answer_id": "6A3EYxpz9R9Uq2RSnK5TFJ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 563, "prompt": "What disease is/are shown on the lower left of lung?\n<image>", "pred_response": "Lung Cancer", "answer_id": "VfWupg2zXKHgAwf3KT3yL9", "gt_response": "Pulmonary Infiltration", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 564, "prompt": "Can infiltration be observed on the lower left lung in this picture?\n<image>", "pred_response": "Yes", "answer_id": "RVgt49QuYy6bPFFeSmYqhV", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 565, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "White", "answer_id": "UyYuM5YR3U7rihDGU99SSP", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 566, "prompt": "How many existing lungs in this image?\n<image>", "pred_response": "2", "answer_id": "cBTV73HrtF2HGBTc2vwZ8g", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 567, "prompt": "What can the organ with black color in this image be used for?\n<image>", "pred_response": "Breathe", "answer_id": "k6KUAnghhTELiU4mXbfVVL", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 568, "prompt": "What kind of symptoms will the disease on the lower left of lung bring to the patient?\n<image>", "pred_response": "Chest pain", "answer_id": "32t2R2QWYAmHDeZ4dXqUG2", "gt_response": "Chest pain, cough, expectoration", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 569, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Heart", "answer_id": "ZmvFAxjbjkTnCF4Lar2nMM", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 570, "prompt": "Is this image taken via X-Ray?\n<image>", "pred_response": "No", "answer_id": "kYjd2TYrA8icfu6HnNGb9A", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 571, "prompt": "Is this a coronal section?\n<image>", "pred_response": "No", "answer_id": "ebQ6rDzKE6sPKwqnRPMvim", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 572, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Lung", "answer_id": "4wPPkMste2Tn9ZERMd77y3", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 573, "prompt": "Which part of chest is abnormal in this image?\n<image>", "pred_response": "Left", "answer_id": "KXZA2iQP2yrvi9eRXyWuUM", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 574, "prompt": "Where is/are the effusion located?\n<image>", "pred_response": "Lung", "answer_id": "kkxX5uGbJePYRyYyCKy7PG", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 575, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Lung", "answer_id": "5zttPcxreosP9r3mJ22CU4", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 576, "prompt": "Is this a study of the head?\n<image>", "pred_response": "No", "answer_id": "CBcJVPMqtBmQgWArSvAmqE", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 577, "prompt": "Does the patient have any abnormalities?\n<image>", "pred_response": "No", "answer_id": "jcNvMNakkT42mxGfg9TsKp", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 578, "prompt": "Does the lung look abnormal?\n<image>", "pred_response": "No", "answer_id": "3qdwkNW4VkptBG9tHXqeto", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 579, "prompt": "Can effusion be observed on the lower left lung in this picture?\n<image>", "pred_response": "No", "answer_id": "dRhdKVMP67wmRF4HJqGZ2N", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 580, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "EKjkGcbuQFhXMddZYjwwHd", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 581, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "jdXiFAgpQM29QZVu4mZKRL", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 582, "prompt": "What can the organ with black color in this image be used for?\n<image>", "pred_response": "Lung", "answer_id": "kV2TjTdTisKA3zNsLzDgS5", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 583, "prompt": "What is the main cause of the disease on the lower left of the lung in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "mMpjydjUvcYMJQXPpYwQse", "gt_response": "Inflammation, malignant tumor, trauma, etc", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 584, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Lung", "answer_id": "EEEUQu98TPA8axXJ9oxPmC", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 585, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "ZpR6vXYgARGAdXfpc2fnGo", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 586, "prompt": "What scanning plane does this image belong to?\n<image>", "pred_response": "Transverse Plane", "answer_id": "ieUuL7PJ39EX4NwE7zmYHR", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 587, "prompt": "Do the organs in the image exist in the chest?\n<image>", "pred_response": "Yes", "answer_id": "FJyNVyMKpjFUy6G6z7QFVG", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 588, "prompt": "Where is the heart in this image?\n<image>", "pred_response": "Left", "answer_id": "RcddXvL7gd9GphsxHCQSBD", "gt_response": "Center", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 589, "prompt": "Where is the pleural effusion?\n<image>", "pred_response": "Left Lung", "answer_id": "KxSrpPKHC9hwXBXR3Z7ntK", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 590, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "8NvBGkowaexzgRNT94snwj", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 591, "prompt": "Is this a study of the pelvic cavity?\n<image>", "pred_response": "No", "answer_id": "jxGq7wfGe7jHDf3gATQ4bz", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 592, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "MJjRMcWjRf7hybHRWBVyhF", "gt_response": "Pleural Effusion", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 593, "prompt": "Is this image normal?\n<image>", "pred_response": "Yes", "answer_id": "hXZ7J8QExQV37XgFNnG94T", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 594, "prompt": "Can effusion be observed on the lower left lung in this picture?\n<image>", "pred_response": "Yes", "answer_id": "hJXGZFHiPZKeJYoprqGFGv", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 595, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "5ntZ52pZCG2Qs8ZkopeGsk", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 596, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "1", "answer_id": "LvbYjJjButAoaiP4tXdxSq", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 597, "prompt": "What is the effect of the main organ in this picture?\n<image>", "pred_response": "Lung", "answer_id": "Ursjrz8LepSsh39AU9Bdv9", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 598, "prompt": "What kind of symptoms will the disease on the lower left of lung bring to the patient?\n<image>", "pred_response": "Chest pain", "answer_id": "HHUPziJqMyHiGauFaidMDy", "gt_response": "Chest tightness, dyspnea, chest pain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 599, "prompt": "Which organs/organ is part of the respiratory system in this image?\n<image>", "pred_response": "Lung", "answer_id": "LYADziSEcgTWVTwwTWKCPW", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 600, "prompt": "Which type of modality is shown about this image, MRI, CT or X-Ray?\n<image>", "pred_response": "MRI", "answer_id": "8k5jthAfRKYB25WvHso3RK", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 601, "prompt": "Is this sagittal plane?\n<image>", "pred_response": "Yes", "answer_id": "R5AzwqYF7kTipACWe2DKRL", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 602, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "NghJs9N8D4e8tGcfjuKfRf", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 603, "prompt": "Where is the brain stem in this image?\n<image>", "pred_response": "Left", "answer_id": "Ri9VAkYtKBjZwafL9cTVHA", "gt_response": "Center", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 604, "prompt": "Which organs appear in pairs?\n<image>", "pred_response": "None", "answer_id": "keYvNHfydPnGnvYMXaF56M", "gt_response": "Temporal Lobe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 605, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "WtiWHtcbyD49uuGeViRmqH", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 606, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "83B4JhF3rfREm9WtAdYnpR", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 607, "prompt": "Does the temporal lobe appear in this image?\n<image>", "pred_response": "Yes", "answer_id": "NMeyYVngZvA6zRjQykiZTS", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 608, "prompt": "What color is the right temporal lobe in the picture?\n<image>", "pred_response": "White", "answer_id": "N3DV8ZT5Lq93wnf9dZzTi5", "gt_response": "Gray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 609, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "axiwjBa9FDNk5PxayvyLbT", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 610, "prompt": "How many ears are shown in this image?\n<image>", "pred_response": "1", "answer_id": "SNf6aCnJ9KwKmPAaS53RhH", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 611, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "9igE2R7XreSgGodUZZUGHx", "gt_response": "Much", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 612, "prompt": "Which organs/organ in the picture belong to the nervous system?\n<image>", "pred_response": "Brain", "answer_id": "UmCo7wc6mC5gisJGkrvfvE", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 613, "prompt": "Are there some organs controlling heartbeat and breathing in this image?\n<image>", "pred_response": "No", "answer_id": "cGK9KXXrc23FCodCoM8eHd", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 614, "prompt": "Do any of the organs in the picture belong to the respiratory system?\n<image>", "pred_response": "No", "answer_id": "d658zHt9jBzRoZvDrpmfqD", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 615, "prompt": "Is this a CT?\n<image>", "pred_response": "No", "answer_id": "6WU8n3SHFQECBfn7dXByeE", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 616, "prompt": "Is this sagittal plane?\n<image>", "pred_response": "Yes", "answer_id": "XtCfFAhoshkJhN5v8rVMXy", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 617, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Head", "answer_id": "NenhtwtC9y2FvjtMM4kFv9", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 618, "prompt": "Where is the brain stem in this image?\n<image>", "pred_response": "Top", "answer_id": "ScVmdMMbSgBUzssBE8LmKi", "gt_response": "Center", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 619, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Head", "answer_id": "o2LiCFXPiujgJBpDSRTpfY", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 620, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Head", "answer_id": "cL7wMjHeETF4ecspkkfuYU", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 621, "prompt": "Does the ears exist in this picture?\n<image>", "pred_response": "No", "answer_id": "LFWxd5VqxFF8XYZgoEtyb6", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 622, "prompt": "Does the picture contain bladder?\n<image>", "pred_response": "No", "answer_id": "XF9ucNiJjyDY5gHq5kYrNJ", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 623, "prompt": "What color is the ears in the picture?\n<image>", "pred_response": "White", "answer_id": "iPJvumF8dmg9y2VWjCc4BE", "gt_response": "White", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 624, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "mSD55KaHMUa3kk6AHt8DJ7", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 625, "prompt": "How many temporal lobes in this image?\n<image>", "pred_response": "2", "answer_id": "5JbRgcXqzy7CXrkSymgmcF", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 626, "prompt": "What is the function of the tissue on the center of this image?\n<image>", "pred_response": "Hearing", "answer_id": "5PcAMnMhzvbxsYPjbU2s46", "gt_response": "Control heartbeat and breathing", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 627, "prompt": "Does the picture contain the organ which has the effect of storing urine?\n<image>", "pred_response": "No", "answer_id": "ZgDKhLNMzXm8GUPDPE37po", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 628, "prompt": "Do any of the organs in the picture belong to the digestive system?\n<image>", "pred_response": "No", "answer_id": "9ZPucphWtHNyVKnvCsbnJR", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 629, "prompt": "What modality is shown in this image?\n<image>", "pred_response": "MRI", "answer_id": "EPL2EaWKTjRfd9iA6q3mKG", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 630, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse Plane", "answer_id": "Z2F4MWRW5EpmejfZoxpbxC", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 631, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "JWhZBC3h29grRircj6AEKa", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 632, "prompt": "Where is the tooth in this image?\n<image>", "pred_response": "Head", "answer_id": "KsNbTrvHTcf8HWzTnCFG78", "gt_response": "Top", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 633, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "Ke5WFtMka2UjmuwnXXkqZn", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 634, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Head", "answer_id": "UmoBMsx9TLho7tj9hLQ9do", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 635, "prompt": "Does the picture contain mandible?\n<image>", "pred_response": "No", "answer_id": "nwRwo3LfDYagijKAwdHzHQ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 636, "prompt": "Does the parotid appear in the image?\n<image>", "pred_response": "No", "answer_id": "5fBSgGXBiQPekUqd3X2xsm", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 637, "prompt": "What color does the brain stem show in the picture?\n<image>", "pred_response": "White", "answer_id": "DVrzQMh3yofwN3ruTL9L9t", "gt_response": "Gray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 638, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "EsudcmfM9eUpzhzivJsMyE", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 639, "prompt": "How many mandibles in this image?\n<image>", "pred_response": "0", "answer_id": "Pr9M7kCyTbtmsupbaBS2BE", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 640, "prompt": "What is the function of the organ on the top of this image?\n<image>", "pred_response": "Hearing", "answer_id": "nzrTJaXF2BMVgG9mnBUNER", "gt_response": "Cutting, chewing, maintaining facial contour and assisting pronunciation", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 641, "prompt": "Which organs/organ in the picture belong to nervous system?\n<image>", "pred_response": "Brain", "answer_id": "TS8v4WXeMhgW528PoAuobe", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 642, "prompt": "Are there organs in the picture that can digest food?\n<image>", "pred_response": "No", "answer_id": "kBBvmAED2DqDuP5p92PiLN", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 643, "prompt": "Do any of the organs in the picture belong to the circulatory system?\n<image>", "pred_response": "No", "answer_id": "SAeVhYqBQW8g2UqRPXs2kW", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 644, "prompt": "What modality is shown in this image?\n<image>", "pred_response": "MRI", "answer_id": "GJtU4HNomSef9Ff5rM3sPG", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 645, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse Plane", "answer_id": "FLpy9xVxBroiBJMJZdaNUP", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 646, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "iVqqFa8w6ZcNmxStrdaioH", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 647, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Head", "answer_id": "9Ns87WUVe6uviCNw3HLHY3", "gt_response": "Neck", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 648, "prompt": "Is there an esophagus in this image?\n<image>", "pred_response": "Yes", "answer_id": "7fXe6k23N9UTwWYysPNMBb", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 649, "prompt": "Does the trachea exist in this picture?\n<image>", "pred_response": "Yes", "answer_id": "DsohcGfZE46JifejTNewKB", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 650, "prompt": "What is the shape of larynx in this image?\n<image>", "pred_response": "Oval", "answer_id": "2upyZ6C6q52nK7LLyQv2k8", "gt_response": "Oval", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 651, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "hXMBrNWK9EtHQbtEwDepB7", "gt_response": "Larynx", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 652, "prompt": "Are there organs in the picture that can deliver food?\n<image>", "pred_response": "No", "answer_id": "mzUVjXXzSn4UNTTggnnv9G", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 653, "prompt": "Does the picture contain the organ which has the effect of gas delivery?\n<image>", "pred_response": "Yes", "answer_id": "eGhofSejJzfD8wtdtfdvbn", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 654, "prompt": "Is this a CT?\n<image>", "pred_response": "No", "answer_id": "58sotYusca3VhLrDyfD2Wy", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 655, "prompt": "What scanning plane does this image belong to?\n<image>", "pred_response": "Transverse", "answer_id": "AiRGJoGNzVnrdqCQRz7R2e", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 656, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Head", "answer_id": "PEAXxdNoiQdcvmk2nLqjiQ", "gt_response": "Neck", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 657, "prompt": "Where is the esophagus in this image?\n<image>", "pred_response": "Left", "answer_id": "F2MGLq9eq9Re34Lh5MhYHN", "gt_response": "Under the trachea", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 658, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Head", "answer_id": "acZyNoi8NTfswtHxxBbpUh", "gt_response": "Neck", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 659, "prompt": "Does the ears exist in this picture?\n<image>", "pred_response": "No", "answer_id": "h4d9wS92LGe9wFx2ZaGQAq", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 660, "prompt": "Is there small bowel in this image?\n<image>", "pred_response": "No", "answer_id": "LTgun4sr47urGp8y67D23v", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 661, "prompt": "What color do the trachea show in the picture?\n<image>", "pred_response": "White", "answer_id": "CfmjsjXojdPsodtDnTTLuj", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 662, "prompt": "What is the shape of trachea about this patient?\n<image>", "pred_response": "Oval", "answer_id": "QY2HYj8HaZtUEJPMPHYvrf", "gt_response": "Irregular", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 663, "prompt": "What role does the black hollow tissue play in the human body?\n<image>", "pred_response": "Brain", "answer_id": "DrMYRj5sFyFkqwRAMoJnZQ", "gt_response": "Gas delivery", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 664, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "B2JQxcMdcDmdw4G5LYnmnX", "gt_response": "Trachea", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 665, "prompt": "Do any of the organs in the picture belong to the nervous system?\n<image>", "pred_response": "No", "answer_id": "oCDdQqfU5a6aSd6hkqpJWN", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 666, "prompt": "Does the picture contain the organ that could be used for detoxicating?\n<image>", "pred_response": "Yes", "answer_id": "GGevo7reXLre68hcNuimqb", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 667, "prompt": "Is this an MRI scan?\n<image>", "pred_response": "Yes", "answer_id": "D8hJCpHgV8wBDbZmLE6ERL", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 668, "prompt": "Is this a transverse section?\n<image>", "pred_response": "No", "answer_id": "LMRzZ8aitYnn5qoyurUso7", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 669, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "jnGfsmmaLU9fM9MwQzPUjL", "gt_response": "Neck", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 670, "prompt": "Which is trachea in this image?\n<image>", "pred_response": "Left", "answer_id": "ReYtRLmzuUUPjmnZct6BtF", "gt_response": "Black Hollow", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 671, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Abdomen", "answer_id": "o43hZiTF8jzPt5Mrv4evPR", "gt_response": "Neck", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 672, "prompt": "Does the picture contain spinal cord?\n<image>", "pred_response": "No", "answer_id": "TuzGDXnLufPyrpNYWwUTC6", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 673, "prompt": "Are/Is there kidneys in the picture?\n<image>", "pred_response": "No", "answer_id": "6NQ24mWUbNdatwTkKJqBYw", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 674, "prompt": "What is the shape of trachea about this patient?\n<image>", "pred_response": "Oval", "answer_id": "RaW828EooAjRrqUDtZm3KD", "gt_response": "Irregular", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 675, "prompt": "What role does the black hollow tissue play in the human body?\n<image>", "pred_response": "Lung", "answer_id": "QthTYQzXCPSVUbFDNGNMCS", "gt_response": "Gas delivery", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 676, "prompt": "Which organs/organ in the picture belong to the digestive system?\n<image>", "pred_response": "None", "answer_id": "NFtKhdXMet8cvse3w5jegC", "gt_response": "Esophagus", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 677, "prompt": "Does the picture contain the organ which has the effect of discharging waste?\n<image>", "pred_response": "Yes", "answer_id": "NDDhMQSZCtZfqhBcv6Mbsa", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 678, "prompt": "Do any of the organs in the picture belong to the nervous system?\n<image>", "pred_response": "No", "answer_id": "7N8urkLTbSadXfDQcCKSWc", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 679, "prompt": "Is this a CT scan?\n<image>", "pred_response": "No", "answer_id": "jRW8nZjvUuR97yjfp54uwc", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 680, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T2", "answer_id": "H6VxoNQkeC6SBzXRtr3sgu", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 681, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "SwfiFcM6oQdNvrFypayMzP", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 682, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "7reKeeQ4Y7rFnVJDCibP8G", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 683, "prompt": "Which hemisphere is abnormal,left or right?\n<image>", "pred_response": "Left", "answer_id": "DxihASrNzSAysykppugGdH", "gt_response": "Right", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 684, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Head", "answer_id": "MR8m4Y2GhnkS6xN3hXFknQ", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 685, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "76bWQCEtAiEyhMtmSSQk9e", "gt_response": "Brain Edema, Brain Enhancing Tumor", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 686, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "1", "answer_id": "6DeBw7wBrKYpBteT6X2pug", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 687, "prompt": "What is the main cause of the disease on the right lobe of brain in this picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "CdfUHVnP4BPMnWY44hdsqR", "gt_response": "Brain embryonic tissue dysplasia, genetic factors, chemical factors", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 688, "prompt": "How was this image taken?\n<image>", "pred_response": "MRI", "answer_id": "NfzmdLoEcwoRMk6KXLHoZP", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 689, "prompt": "Is this an MRI image with T1 weighted?\n<image>", "pred_response": "Yes", "answer_id": "6rvD5UHnUdVNHrF8ypDUnJ", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 690, "prompt": "Is this a coronal section?\n<image>", "pred_response": "No", "answer_id": "8iEg47aTcjw7QeW8Ku9jVY", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 691, "prompt": "Where does the picture show?\n<image>", "pred_response": "Head", "answer_id": "5KDT7iXiGjj7akdEYTHP5y", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 692, "prompt": "Where is the brain non-enhancing tumor?\n<image>", "pred_response": "Right", "answer_id": "ekoF3XAM9744d6TZ9jYL5G", "gt_response": "Upper Left Lobe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 693, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Head", "answer_id": "frquCgd7F3oeshyMyPGWdL", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 694, "prompt": "Is the brain healthy?\n<image>", "pred_response": "No", "answer_id": "LsPPeewYK7z6WNs7XK2xox", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 695, "prompt": "What is the color of abnormality in this image?\n<image>", "pred_response": "White", "answer_id": "MARTfS22SLDwfBtgHXafen", "gt_response": "White", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 696, "prompt": "What is the main cause of the disease on the upper left lobe of brain in this picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "N6CQ6Q45Ph3iJ7wxeXXavS", "gt_response": "Brain embryonic tissue dysplasia, genetic factors, chemical factors", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 697, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "E7ojsU4gfgGjPxPujnvnzT", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 698, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T2", "answer_id": "etgz8gDShgTJBnbGaNSTWY", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 699, "prompt": "Is this sagittal plane?\n<image>", "pred_response": "Yes", "answer_id": "GWkyn2RPgVyzZSuKJpXRCd", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 700, "prompt": "Where does the picture show?\n<image>", "pred_response": "Head", "answer_id": "3FHwuwnNx63otsDHGeXert", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 701, "prompt": "Which hemisphere is abnormal,left or right?\n<image>", "pred_response": "Left", "answer_id": "K2odYRGvozYuBGUdAVek4F", "gt_response": "Left", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 702, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Head", "answer_id": "33NesZcZXG5adgimPCy6cs", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 703, "prompt": "What disease is/are shown on the left of brain?\n<image>", "pred_response": "None", "answer_id": "bbhtZv4fR94Z9jAH2EreLV", "gt_response": "Brain Edema, Brain Enhancing Tumor, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 704, "prompt": "Is the tumor enhancing?\n<image>", "pred_response": "No", "answer_id": "evgw6tCXm3CJJNkgN6eNJY", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 705, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "2", "answer_id": "k2kudW6cWaBteXTD8kHzTd", "gt_response": "3", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 706, "prompt": "What kind of symptoms will the disease on the left lobe of brain in the picture bring to the patient?\n<image>", "pred_response": "Headache, dizziness, nausea, vomiting, confusion, memory loss, difficulty speaking, difficulty understanding speech, difficulty writing, difficulty reading, difficulty with balance and coordination, difficulty with fine motor skills, difficulty with gross motor skills, difficulty with problem-solving, difficulty with decision-making, difficulty with planning, difficulty with organizing, difficulty with multitasking, difficulty with time management, difficulty with social interactions, difficulty with emotional regulation, difficulty with impulse control, difficulty with self-awareness, difficulty with self-esteem, difficulty with self-efficacy, difficulty with", "answer_id": "XwVMCHkFcAXDCjPwUnBYjT", "gt_response": "Visual impairment, vomiting, tinnitus, increased intracranial pressure", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 707, "prompt": "What modality is shown in this image?\n<image>", "pred_response": "MRI", "answer_id": "C8QE9eYxtRti7LMBEtw5Kk", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 708, "prompt": "Is this an MRI image with T1 weighted?\n<image>", "pred_response": "Yes", "answer_id": "QFSu2m8LUVGDAu3NSojpkT", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 709, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "MNCgvfyUNGQoExSyjWXv85", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 710, "prompt": "Do the organs in the image exist in the chest?\n<image>", "pred_response": "No", "answer_id": "ihey93d5ApRc8sxCsKwA5H", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 711, "prompt": "Where is the brain edema?\n<image>", "pred_response": "Right", "answer_id": "BHmEde26RhTYi35yWe5S66", "gt_response": "Left Lobe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 712, "prompt": "Is this a study of the head?\n<image>", "pred_response": "Yes", "answer_id": "aiiuyj9fR8YQ4mrfYQXuj8", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 713, "prompt": "Is the brain abnormal?\n<image>", "pred_response": "Yes", "answer_id": "JcrgybfSRiwdmV5FJn29dx", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 714, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "2", "answer_id": "9EPzQrCAPCc9NCHgeft44m", "gt_response": "1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 715, "prompt": "How to treat the most severe disease in this image?\n<image>", "pred_response": "Brain surgery", "answer_id": "C72xoh6Ag7uWP3jiSrzewi", "gt_response": "Pharmacotherapy, rehabilitation", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 716, "prompt": "What modality is shown in this image?\n<image>", "pred_response": "MRI", "answer_id": "KDGxcR5Dib57x2EPACwZkr", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 717, "prompt": "Is this a T1 weighted or T2 weighted MRI image?\n<image>", "pred_response": "T2", "answer_id": "Cap58aNtKPqF6cs4vjfiNT", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 718, "prompt": "Is this a coronal section?\n<image>", "pred_response": "No", "answer_id": "Ee2GASBc48EuWqiLVpv4Fc", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 719, "prompt": "Do the organs in the image exist in the abdomen?\n<image>", "pred_response": "No", "answer_id": "DGBsyCWABUf7Cf9TTCW3E3", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 720, "prompt": "Where are the abnormalities in this image?\n<image>", "pred_response": "Left", "answer_id": "AQifyVuEbqUKDCf5soWwGD", "gt_response": "Upper Left Lobe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 721, "prompt": "Is this a study of the head?\n<image>", "pred_response": "Yes", "answer_id": "DVYtGKe8Ax6jPxFHYHywTz", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 722, "prompt": "Is there evidence of a brain edema?\n<image>", "pred_response": "Yes", "answer_id": "V5A6Hnub6C96TaLqWpL9FF", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 723, "prompt": "Is the abnormality hyperdense or hypodense?\n<image>", "pred_response": "Hypodense", "answer_id": "3cKPAzfDLCUW5nVsUKcTsL", "gt_response": "Hyperdense", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 724, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "2", "answer_id": "jHRPVWh5wbis92oZa2gbp6", "gt_response": "1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 725, "prompt": "How to treat the most severe disease in this image?\n<image>", "pred_response": "Brain surgery", "answer_id": "nsfCXJMw4k5gb8nVeQ2beL", "gt_response": "Pharmacotherapy, rehabilitation", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 726, "prompt": "What type of medical image is this?\n<image>", "pred_response": "MRI", "answer_id": "9cPZkvxc4T4VUrJgnuqFup", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 727, "prompt": "Is this a T1 weighted or T2 weighted MRI image?\n<image>", "pred_response": "T2", "answer_id": "7VoHuyd9vtcC8VQpUevXgM", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 728, "prompt": "Is this a coronal section?\n<image>", "pred_response": "No", "answer_id": "juxTjgjUNzNAf2ZA8gyZkF", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 729, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "DZq9RVw2KxmjYdkSZ7WWsq", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 730, "prompt": "Where is the brain tumor?\n<image>", "pred_response": "Left", "answer_id": "VyhNTnfSeCzGGNXaQfn4JY", "gt_response": "Upper Left Lobe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 731, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Head", "answer_id": "R2ti9Xd5ZdkHhJLMqRUjgk", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 732, "prompt": "Is this image normal?\n<image>", "pred_response": "No", "answer_id": "L3hhFV32uoerg9EjdpDYV5", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 733, "prompt": "How to prevent the occurrence of the disease on the upper left lobe of brain in this image?\n<image>", "pred_response": "MRI", "answer_id": "Urx6u9QY6PBfZKZomJn5CP", "gt_response": "Pay attention to dietary hygiene, strengthen physical fitness and avoid brain trauma", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 734, "prompt": "What imaging modality was used?\n<image>", "pred_response": "MRI", "answer_id": "PU7xrRDrhaP7BHuRTrsWtb", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 735, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T2", "answer_id": "bNrG6iWPiQWi5Rtbar693y", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 736, "prompt": "Is this a transverse section?\n<image>", "pred_response": "No", "answer_id": "4EbE3JF2s3oTizYsps7Nnq", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 737, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "RVZeyJRcFHEjnMaPRusjoB", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 738, "prompt": "Where is the brain tumor?\n<image>", "pred_response": "Left", "answer_id": "LWucASeuBbYwpjUCiVxPZE", "gt_response": "Upper Right Lobe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 739, "prompt": "Is this a study of the neck?\n<image>", "pred_response": "No", "answer_id": "4aYxD7FGX2XYFEuYWL2nYB", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 740, "prompt": "What disease is/are shown on the upper right of brain?\n<image>", "pred_response": "Brain Tumor", "answer_id": "o2NKULd32qaNLLh9YjGA5J", "gt_response": "Brain Edema, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 741, "prompt": "Is the abnormality hyperdense or hypodense?\n<image>", "pred_response": "Hypodense", "answer_id": "6p6zZeQ5X9mZCXXZhKu2in", "gt_response": "Hyperdense", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 742, "prompt": "What kind of symptoms will the disease on the upper right lobe of brain in the picture bring to the patient?\n<image>", "pred_response": "Headache, dizziness, nausea, vomiting, confusion, difficulty speaking, difficulty understanding speech, difficulty writing, difficulty reading, difficulty with balance and coordination, difficulty with memory, difficulty with concentration, difficulty with attention, difficulty with problem-solving, difficulty with decision-making, difficulty with planning, difficulty with organizing, difficulty with multitasking, difficulty with time management, difficulty with social interactions, difficulty with emotional regulation, difficulty with self-awareness, difficulty with self-esteem, difficulty with self-confidence, difficulty with self-motivation, difficulty with self-", "answer_id": "bJ9DJ3Y3QYDJJ7XpGZevZT", "gt_response": "Visual impairment, vomiting, tinnitus, increased intracranial pressure", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 743, "prompt": "How was this image taken?\n<image>", "pred_response": "MRI", "answer_id": "foA5iDS7rPWjRJMscm7stP", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 744, "prompt": "Is this a T1 weighted or T2 weighted MRI image?\n<image>", "pred_response": "T2", "answer_id": "bnVf9EdkU6hoAT6ALwsLjx", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 745, "prompt": "Is this a coronal section?\n<image>", "pred_response": "No", "answer_id": "gWjYJZZbitiVfe9xfQfUNH", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 746, "prompt": "Do the organs in the image exist in the abdomen?\n<image>", "pred_response": "No", "answer_id": "8byFAXhoeLxKyGSxk5wHVt", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 747, "prompt": "Where is the brain tumor?\n<image>", "pred_response": "Left", "answer_id": "SfxDdi9c5WxSNFztnfwds8", "gt_response": "Upper Right Lobe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 748, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Head", "answer_id": "mX9XqvjqN4rd5qy6miQoK5", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 749, "prompt": "Is the brain abnormal?\n<image>", "pred_response": "Yes", "answer_id": "EMCD8Wr8YumUzLiM22ZSyB", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 750, "prompt": "How to prevent the occurrence of the disease on the upper right lobe of brain in this image?\n<image>", "pred_response": "MRI", "answer_id": "3iBvcoFVLFyrsq5DxtgJtF", "gt_response": "Pay attention to dietary hygiene, strengthen physical fitness and avoid brain trauma", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 751, "prompt": "Is this image taken via MRI?\n<image>", "pred_response": "Yes", "answer_id": "SpYAmmzQFumcDvxcNcssVo", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 752, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "6wunG4X7sHutxN5a6UayM8", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 753, "prompt": "What scanning plane does this image belong to?\n<image>", "pred_response": "Transverse Plane", "answer_id": "YiAmraTtdbgsKUUWVgqgGx", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 754, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Head", "answer_id": "WSjFKM7igARTUkVmQ4zRdt", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 755, "prompt": "Which lobe is abnormal,left or right?\n<image>", "pred_response": "Left", "answer_id": "VYEnedyPgJ8cUVuHY38yWw", "gt_response": "Right", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 756, "prompt": "Is this a study of the head?\n<image>", "pred_response": "Yes", "answer_id": "R8J5ghFCDVKZzU4hzAtMew", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 757, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "JregggByHqmruRbA2PXCnm", "gt_response": "Brain Edema, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 758, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "0", "answer_id": "9h7XetFAMmNmNXjL3MDRPa", "gt_response": "None", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 759, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "XBP3wfCyZuer273uaVCbu9", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 760, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "WoJ83V7fqeDniCfid5Yqr8", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 761, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "4YGBFJJ2KU8c8kPJCzTU6D", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 762, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Brain", "answer_id": "8HTHkPrid6SbE2GtHByXbY", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 763, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "2LuDqzRpVi65isnoymR7eQ", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 764, "prompt": "Does the picture contain brian?\n<image>", "pred_response": "Yes", "answer_id": "GEnWnNSpqH9r8R6kSSAwLA", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 765, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "2wqV3GG6wohBM6k86HYzd3", "gt_response": "Brain Edema", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 766, "prompt": "Does the brain look abnormal?\n<image>", "pred_response": "Yes", "answer_id": "EyE4k2ottnhoapsXmsgjbh", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 767, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Right", "answer_id": "6dwW3gGKxYFX6ypCYcDJ3U", "gt_response": "Lower Right", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 768, "prompt": "Is this a X-Ray?\n<image>", "pred_response": "No", "answer_id": "7CvCsrzJYjeM7cEERqGFxC", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 769, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T2", "answer_id": "Cyp5tvGWj2rcHp6r8fL4Dt", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 770, "prompt": "Is this a coronal plane?\n<image>", "pred_response": "Yes", "answer_id": "FVf9ms4DAY4GShQHAKEkMc", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 771, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "gfUbvjvXvNEwnwqeRYC29o", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 772, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left", "answer_id": "nZWsqS9mxMaKaKpYVKRBwr", "gt_response": "Upper Right Lobe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 773, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Head", "answer_id": "XR9d9Ew8BiubMYUEXQYP36", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 774, "prompt": "Is/Are there abnormalities in the patient's left brain lobe?\n<image>", "pred_response": "Yes", "answer_id": "aMpTrxA9m8nzHJo9G3FXBW", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 775, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "2", "answer_id": "mJTbK9VkQBcemwEgeZRZNk", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 776, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "GD9QGZC8jDHB4EekowS4s7", "gt_response": "None", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 777, "prompt": "Is this a CT scan?\n<image>", "pred_response": "No", "answer_id": "CWsUrMXfS3KT2YS9ih7JaE", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 778, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "fvCGjmPaCXYhW7b7G8JxCY", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 779, "prompt": "Is this sagittal plane?\n<image>", "pred_response": "No", "answer_id": "7TvdNkcyVKMn9sU9ZYKPPW", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 780, "prompt": "Do the organs in the image exist in the pelvic cavity?\n<image>", "pred_response": "No", "answer_id": "mUwiqvdss2XJRjVgzfi9vd", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 781, "prompt": "Where are the abnormalities in this image?\n<image>", "pred_response": "Left", "answer_id": "agZEpoa4L9avZKzoLzWMp9", "gt_response": "Left Lobe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 782, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Head", "answer_id": "fAw2HvxsAv8doWu63UkKue", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 783, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "jyFEiPjtVBRZQkfYGpxqVj", "gt_response": "Brain Edema, Brain Enhancing Tumor, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 784, "prompt": "Is the brain enhancing tumor hyperdense or hypodense?\n<image>", "pred_response": "Hypodense", "answer_id": "VmmmyAnhLhDhkZTd6uAZvn", "gt_response": "Hyperdense", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 785, "prompt": "What kind of symptoms will the disease on the left lobe of brain in the picture bring to the patient?\n<image>", "pred_response": "Headache", "answer_id": "76kPARZ93xKY9fbucrzmqY", "gt_response": "Visual impairment, vomiting, tinnitus, increased intracranial pressure", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 786, "prompt": "Is this image taken via CT?\n<image>", "pred_response": "No", "answer_id": "bsfK5JcBB4USn3NfrofTrg", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 787, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T2", "answer_id": "2zQwscKvKE9rRVf8kFHnsj", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 788, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse Plane", "answer_id": "jiDpzoGMiTXwm86B8fNziz", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 789, "prompt": "Which part of the human body is the organ located in the image?\n<image>", "pred_response": "Head", "answer_id": "4aRmyqqEuRGnHLTNZpbNJi", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 790, "prompt": "Where is the brain tumor located?\n<image>", "pred_response": "Left", "answer_id": "QEn6pGjDWpXjzeJyx2tLnL", "gt_response": "Lower Left Lobe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 791, "prompt": "Is this a study of the chest?\n<image>", "pred_response": "No", "answer_id": "biDRq3EHmhvKWZ2gwpKHcP", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 792, "prompt": "Does the patient have brain tumor?\n<image>", "pred_response": "Yes", "answer_id": "m7TwmUEuMg3jGNnZGVJycZ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 793, "prompt": "What is the main cause of the disease on the left lobe of brain in this picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "hZSrXrNoC8PjCeT4Ak5EUe", "gt_response": "Brain embryonic tissue dysplasia, genetic factors, chemical factors", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 794, "prompt": "How was this image taken?\n<image>", "pred_response": "MRI", "answer_id": "TzjtrtDmhcuPMhEMkRegtg", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 795, "prompt": "What type of MRI is shown in this image?\n<image>", "pred_response": "MRI", "answer_id": "jHENKkTWFVt7GDKyQ3LaN4", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 796, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse Plane", "answer_id": "jnQ7pxRxSf5fUoq6bm9MQC", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 797, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Head", "answer_id": "S8LNvoQbsuMjuCbW6gxTvq", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 798, "prompt": "Where is the brain tumor?\n<image>", "pred_response": "Right", "answer_id": "cKdBVgkj7xPuHwwKwByHfG", "gt_response": "Left Lobe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 799, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "Q3cVNcYWors9zBEFCv6aFH", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 800, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "9gnRrsWqyYu2mCbHz4VcBY", "gt_response": "Brain Edema, Brain Enhancing Tumor, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 801, "prompt": "What density is the brain enhancing tumor ?\n<image>", "pred_response": "0", "answer_id": "k2UFWi9862XuSJ8wz9TMb3", "gt_response": "Hyperdense", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 802, "prompt": "What kind of symptoms will the disease on the left lobe of brain in the picture bring to the patient?\n<image>", "pred_response": "Headache, dizziness, nausea, vomiting, confusion, memory loss, difficulty speaking, difficulty understanding speech, difficulty writing, difficulty reading, difficulty with balance and coordination, difficulty with fine motor skills, difficulty with gross motor skills, difficulty with vision, difficulty with hearing, difficulty with smell, difficulty with taste, difficulty with breathing, difficulty with swallowing, difficulty with urination, difficulty with defecation, difficulty with menstruation, difficulty with sexual function, difficulty with sleep, difficulty with mood, difficulty with emotions, difficulty with personality, difficulty with social interactions, difficulty with work", "answer_id": "QQBiHFusgyhdaMP7bQn4gX", "gt_response": "Visual impairment, vomiting, tinnitus, increased intracranial pressure", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 803, "prompt": "Is this a CT?\n<image>", "pred_response": "No", "answer_id": "ZWPNHGQFR5zZG34Hzvuj4K", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 804, "prompt": "Is this an MRI image with T2 weighted?\n<image>", "pred_response": "No", "answer_id": "mSv5AkA95KwXobYBAtJHGT", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 805, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "AEg3moJe5JEV9zp3JeYgCf", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 806, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "bNbPri6D7nuiVcfwBqzZp8", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 807, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Right", "answer_id": "DNscxf36ideYDto8wEAvGe", "gt_response": "Lower Left Lobe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 808, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Head", "answer_id": "HMGYTPhxbXe96iEenDuWkW", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 809, "prompt": "What disease is/are shown on the lower left of brain?\n<image>", "pred_response": "Brain Tumor", "answer_id": "cn5fNTCwYPJYfyJ9UwrL47", "gt_response": "Brain Edema, Brain Enhancing Tumor", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 810, "prompt": "What is the color of brain enhancing tumor in this image?\n<image>", "pred_response": "White", "answer_id": "V2tdMWm5Wgvr9p42BChw5B", "gt_response": "White", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 811, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "1", "answer_id": "7o5nTgMdptqRzDnsvt6Ezp", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 812, "prompt": "How to prevent the occurrence of the disease on the lower left lobe of brain in this image?\n<image>", "pred_response": "MRI", "answer_id": "jxhBnbwLonmfnjQSsJTrox", "gt_response": "Pay attention to dietary hygiene, strengthen physical fitness and avoid brain trauma", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 813, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "2RDXFjwe6pBaRs9PWUK863", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 814, "prompt": "Is this an MRI image with T2 weighted?\n<image>", "pred_response": "Yes", "answer_id": "Yq4EcuVSDuHqCkoKkEYoem", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 815, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "9NJKzfGVSaqWHgfnooTM2D", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 816, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "EzDqRzq95DeBrQs5VufyBp", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 817, "prompt": "Where is the brain non-enhancing tumor?\n<image>", "pred_response": "Right", "answer_id": "d9WyfBhoKBE29hVhTv7khE", "gt_response": "Left Lobe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 818, "prompt": "Is this a study of the head?\n<image>", "pred_response": "Yes", "answer_id": "4iaQiNoAHRXDFhEWG2WDtU", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 819, "prompt": "What disease is/are shown on the left of brain?\n<image>", "pred_response": "Brain Tumor", "answer_id": "68r4uWgLnMuG5KMk75W9uY", "gt_response": "Brain Edema, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 820, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "2", "answer_id": "U83dQSb9GVz7Sj2XYQSNBv", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 821, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "SxotS6T6TnbewJe5sFkio9", "gt_response": "None", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 822, "prompt": "Is this an MRI scan?\n<image>", "pred_response": "Yes", "answer_id": "QxDZsgNqf5TFXkwC23h2fY", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 823, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "MfXTqWQKPTRNhmRPzddYDa", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 824, "prompt": "Is this a coronal section?\n<image>", "pred_response": "No", "answer_id": "nr9B5dsULD28q3t5GSdGGn", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 825, "prompt": "Do the organs in the image exist in the neck?\n<image>", "pred_response": "No", "answer_id": "ibn6DefgYqSamTGbH5Q2dJ", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 826, "prompt": "Where is the brain edema located?\n<image>", "pred_response": "Right", "answer_id": "49hWgNYkTYcfSzqJZt4z9X", "gt_response": "Left Lobe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 827, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Head", "answer_id": "DjbtQcpiNXR5bBtv7nJrV9", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 828, "prompt": "Is the brain normal appearing?\n<image>", "pred_response": "No", "answer_id": "WXgH6HuftxnMkJksz8B5fZ", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 829, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "B9RQXZ8p4TgQSVB2hMHwmC", "gt_response": "None", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 830, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "kxduyqaTQyEmmoPiZrgEx3", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 831, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "gFTAuxqUUJCGiMe6hJRGKW", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 832, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "ZdgxVDDB9HKfyeN3jbxLj6", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 833, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Brain", "answer_id": "949djjKiF7DYQFLD9QmV68", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 834, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "4cBdj9dwHJB2otP7NX26yg", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 835, "prompt": "Does the picture contain brian?\n<image>", "pred_response": "Yes", "answer_id": "Eotuh6JUGQaKyXqefx7Bdb", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 836, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "AFTor3pBZWEmWs8aDkAiB8", "gt_response": "None", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 837, "prompt": "Does the brain look abnormal?\n<image>", "pred_response": "Yes", "answer_id": "cr9rd6yALH8nb6TL6cXicz", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 838, "prompt": "Is this image taken via MRI?\n<image>", "pred_response": "Yes", "answer_id": "akDhYUJuqWm9uZa8dGsAWG", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 839, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T2", "answer_id": "cyd5Pk4hFdfaTd2rn2gtTU", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 840, "prompt": "Is this a transverse section?\n<image>", "pred_response": "No", "answer_id": "EHDX3r45RzyrizeUkYAkje", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 841, "prompt": "Which part do the organs in this image locate from the aspect of the whole body?\n<image>", "pred_response": "Head", "answer_id": "a54NHj2XHezs6jEVanhy8a", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 842, "prompt": "Where are the abnormalities in this image?\n<image>", "pred_response": "Left", "answer_id": "iQMpEsHX3LeYVDqDuzDtff", "gt_response": "Lower Left Lobe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 843, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Head", "answer_id": "SBrHCbLbbcH98d9u9p3GnH", "gt_response": "Head", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 844, "prompt": "What disease is/are shown on the left of brain?\n<image>", "pred_response": "None", "answer_id": "kxCQPNb8iVpTpFTHSrw7nP", "gt_response": "Brain Edema", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 845, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "2", "answer_id": "4XU4XnzzHPnBMbE6wbFVMY", "gt_response": "1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 846, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "3KEpRGfSDPqaG8NF3TVN6e", "gt_response": "None", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 847, "prompt": "Which is the kidney in this image?\n<image>", "pred_response": "Right", "answer_id": "ahNyVhrbDprC6timcRpW4D", "gt_response": "Symmetrical to the bottom spine", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 848, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "GvqmkhfptteS34Q5uoQNZ2", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 849, "prompt": "Does the stomach appear in the image?\n<image>", "pred_response": "No", "answer_id": "2vGqJ2QAsQ7fAz6pVmSoLj", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 850, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "nFjBtJ2b4mgvoLS8vtVewe", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 851, "prompt": "What color is the spleen in the picture?\n<image>", "pred_response": "White", "answer_id": "UBXHvgqHopjGnMhFyfGf82", "gt_response": "Gray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 852, "prompt": "Which is smaller in this image,liver or right kidney?\n<image>", "pred_response": "Liver", "answer_id": "o7QJzHvsGDjTf8u42XHLLb", "gt_response": "Right Kidney", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 853, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "3", "answer_id": "4SDzZKBem39XfAFYb48DG8", "gt_response": "6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 854, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "Uj3d8bBNHUVXeyhXddwF5D", "gt_response": "Much", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 855, "prompt": "Which organs/organ is part of the lymphatic system in this image?\n<image>", "pred_response": "Liver", "answer_id": "eKEHieqRjSCv3qBaZhaKkg", "gt_response": "Spleen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 856, "prompt": "Which organs/organ is part of the urinary system in this image?\n<image>", "pred_response": "Kidney", "answer_id": "gNCtKGN7ZtYLYy4NPK288C", "gt_response": "Kidney", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 857, "prompt": "Does the picture contain the organ which has the effect of discharging waste?\n<image>", "pred_response": "Yes", "answer_id": "LPqZQsCgGoCWYUzehQYHGj", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 858, "prompt": "Do any of the organs in the picture belong to the respiratory system?\n<image>", "pred_response": "No", "answer_id": "PybNUMvkNTR2cdcNKiDAAa", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 859, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse Plane", "answer_id": "BLHQT4qZXeYMUtmKLWKgfo", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 860, "prompt": "Where is the colon in this image?\n<image>", "pred_response": "Left", "answer_id": "B6gShbMS7LDFwrd5whFUrH", "gt_response": "Upper Right", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 861, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "iTEXr65GdPUkSpgVHfH2AX", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 862, "prompt": "Does the trachea exist in this picture?\n<image>", "pred_response": "No", "answer_id": "DmKQjRVUd5haABhCmBdo9Y", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 863, "prompt": "Does the picture contain bladder?\n<image>", "pred_response": "No", "answer_id": "5Y6SKAqvKVMWrVd6iyhrje", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 864, "prompt": "Which is smaller in this image,liver or lung?\n<image>", "pred_response": "Liver", "answer_id": "N4u76HYtggVRZhsgBiLrZv", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 865, "prompt": "How many lungs have existed in this image?\n<image>", "pred_response": "2", "answer_id": "9BTTnmBfHSoKo5kUL4NyAG", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 866, "prompt": "What is the effect of the organ on the upper left of this image?\n<image>", "pred_response": "Liver", "answer_id": "D2xNLR9Kz7sywY85Aqqrh8", "gt_response": "Biotransformation, detoxification", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 867, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "kw52rFCB7hGnAp8Q3A88vu", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 868, "prompt": "Does the picture contain the organ that could be used for secreting enzymes?\n<image>", "pred_response": "Yes", "answer_id": "MfcHayTp4iPpa2UN6HvVnP", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 869, "prompt": "Do any of the organs in the picture belong to the digestive system?\n<image>", "pred_response": "No", "answer_id": "TPZXN5WeHc4CKdbSSEhucu", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 870, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Liver", "answer_id": "KQqNarSATBcArBLXV4ezow", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 871, "prompt": "Does the larynx appear in the image?\n<image>", "pred_response": "No", "answer_id": "5DgeWoXxaFuYbY72yBFBcK", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 872, "prompt": "Does the lung exist in this picture?\n<image>", "pred_response": "Yes", "answer_id": "WpmxXvNrAw3gYmM9f7Xair", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 873, "prompt": "Which is smaller in this image,colon or liver?\n<image>", "pred_response": "Liver", "answer_id": "QyhAQqQy2Bnpu5YV68bGec", "gt_response": "Colon", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 874, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "1", "answer_id": "6ya4SsPmRnKWbG5ipa592F", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 875, "prompt": "What is the function of the organs appearing in pairs in the picture?\n<image>", "pred_response": "Liver, Kidney", "answer_id": "XWJ9T3qeNNs5LAGStgn67b", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 876, "prompt": "Which organs/organ is part of the digestive system in this image?\n<image>", "pred_response": "Liver", "answer_id": "DEWuWZP7ZdQ4XwvTrjEeUK", "gt_response": "Stomach, Colon", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 877, "prompt": "Which organs/organ in the picture belong to the urinary system?\n<image>", "pred_response": "Kidney", "answer_id": "iSjb8vMnn7cHssipe7monc", "gt_response": "Kidney", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 878, "prompt": "Does the picture contain the organ which has the effect of detoxicating?\n<image>", "pred_response": "Yes", "answer_id": "8sWjvdWka49gxMoTVCA2Ne", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 879, "prompt": "Does the picture contain the organ which has the effect of adjusting water and osmotic pressure balance?\n<image>", "pred_response": "Yes", "answer_id": "gu4a5ShpcyPjJJgdeNpotA", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 880, "prompt": "What organ is the gray part on the right of the image?\n<image>", "pred_response": "Liver", "answer_id": "fL8DfHuMUaNkSSN2YmxsfM", "gt_response": "Small Bowel", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 881, "prompt": "Does the picture contain colon?\n<image>", "pred_response": "Yes", "answer_id": "d6byfrxytUVkoUYfx6HVBu", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 882, "prompt": "Does the picture contain brain stem?\n<image>", "pred_response": "No", "answer_id": "cBRncp4zYu7V7L73vaq8nR", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 883, "prompt": "Which is smaller in this image,kidney or small bowel?\n<image>", "pred_response": "Small Bowel", "answer_id": "V9my42G3jr6ysYC6ePktjk", "gt_response": "Kidney", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 884, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "0", "answer_id": "mYF5xCF8LZyhHzn5NBTrZV", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 885, "prompt": "What is the effect of the organ on the top of this image?\n<image>", "pred_response": "Kidney", "answer_id": "9zNN4ECKN5bgNtkvefo4JG", "gt_response": "Absorb nutrients, secrete enzymes, digest food", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 886, "prompt": "Are there some organs controlling heartbeat and breathing in this image?\n<image>", "pred_response": "No", "answer_id": "GgCYjTtUZaYtTToYvvGmW6", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 887, "prompt": "Does the picture contain the organ which has the effect of digesting food?\n<image>", "pred_response": "Yes", "answer_id": "7UDJSZSFB8APtSSpRyAskM", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 888, "prompt": "Which is the kidney in this image?\n<image>", "pred_response": "Right", "answer_id": "2SbeYnCT9tQyGPnGxXUkzg", "gt_response": "Symmetrical to the bottom spine", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 889, "prompt": "What organ is the black part on the upper left of the image?\n<image>", "pred_response": "Liver", "answer_id": "biNZ3RjbXdPJo5VtFDPCpb", "gt_response": "Small Bowel", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 890, "prompt": "Does the rectum appear in this picture?\n<image>", "pred_response": "No", "answer_id": "iqEAA5C9Y74JFUsuocgeCR", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 891, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "Zy6uUTyazQr6PuHevhr43L", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 892, "prompt": "What color does the left kidney show in the picture?\n<image>", "pred_response": "Gray", "answer_id": "KbDeRTXbvjTTNNAXKHyjLr", "gt_response": "Gray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 893, "prompt": "Which is smaller in this image, liver or right kidney?\n<image>", "pred_response": "Right kidney", "answer_id": "7zAtJMY4uhHr5UV4pW5ArN", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 894, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "2", "answer_id": "Dh7gGYSun7WrB4qAWjEdkd", "gt_response": "5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 895, "prompt": "What is the shape of spinal cord in this image?\n<image>", "pred_response": "None", "answer_id": "kGL2csSFobjuxugguKMtoZ", "gt_response": "Circular", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 896, "prompt": "What is the effect of the organ on the top of this image?\n<image>", "pred_response": "Liver", "answer_id": "U5fszxMstEZiCusvoyoJCW", "gt_response": "Absorb nutrients, secrete enzymes, digest food", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 897, "prompt": "Which organs/organ is part of the nervous system in this image?\n<image>", "pred_response": "Brain", "answer_id": "EkMdg6zW2gp9Rn68pGe4ty", "gt_response": "Spinal cord", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 898, "prompt": "Does the picture contain the organ which has the effect of digesting food?\n<image>", "pred_response": "No", "answer_id": "Mqe9jzqpa4avouvh3QTEiT", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 899, "prompt": "Does the picture contain the organ that could be used for secreting enzymes?\n<image>", "pred_response": "Yes", "answer_id": "MLThdELMi7C5atRHnEQ7bQ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 900, "prompt": "Where is the liver in this image?\n<image>", "pred_response": "Left", "answer_id": "6SKJJqbFDyGExHhGGisGKf", "gt_response": "Left", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 901, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "Yes", "answer_id": "mtcZiqeHzYVGnnkY7GCdjr", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 902, "prompt": "Does the picture contain colon?\n<image>", "pred_response": "No", "answer_id": "hvFPnnCRhFSqrrDJQLKtSt", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 903, "prompt": "Does the humerus head exist in this picture?\n<image>", "pred_response": "No", "answer_id": "P8fifPpJjmQuHWSHCBW8Dd", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 904, "prompt": "What color does the duodenum show in the picture?\n<image>", "pred_response": "White", "answer_id": "n2D8SrZZoPHi6eJLfkcusc", "gt_response": "Gray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 905, "prompt": "Which is smaller in this image, spleen or kidney?\n<image>", "pred_response": "Kidney", "answer_id": "f5eQY7irefYuTUah4M8rFD", "gt_response": "Spleen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 906, "prompt": "How many lungs have existed in this image?\n<image>", "pred_response": "2", "answer_id": "JKqE66xYfniNNqwict6xaK", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 907, "prompt": "What is the shape of spleen in the picture?\n<image>", "pred_response": "Round", "answer_id": "54mbvyNesRWiEngawJh32u", "gt_response": "Oval", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 908, "prompt": "What is the function of the organ on the center of this image?\n<image>", "pred_response": "Digest", "answer_id": "ZjxAhsoTsK5HDiDNtMPxkB", "gt_response": "Absorb nutrients, secrete enzymes, digest food", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 909, "prompt": "Which organs/organ is part of the nervous system in this image?\n<image>", "pred_response": "Brain", "answer_id": "2bMkAXXADfgULoWsi6btLM", "gt_response": "Spinal cord", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 910, "prompt": "Are there organs in the picture that can adjust water and osmotic pressure balance?\n<image>", "pred_response": "Yes", "answer_id": "J86WERSxwvxL5YexMpaokJ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 911, "prompt": "Are there any organs used for breathing in this image?\n<image>", "pred_response": "No", "answer_id": "bd84FF8HQX3PyJhU7zjJQL", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 912, "prompt": "Is this a transverse section?\n<image>", "pred_response": "No", "answer_id": "Bx8MjDhmKEvHxGrpW4Zjf6", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 913, "prompt": "Where is the stomach in this image?\n<image>", "pred_response": "Left", "answer_id": "PwFBs9hspbAb9msiSiMggB", "gt_response": "Upper Right", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 914, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Abdomen", "answer_id": "JB2P9s6ysrGVqRk3q8RHBy", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 915, "prompt": "Does the picture contain eyes?\n<image>", "pred_response": "No", "answer_id": "TH6nTuMF5NKvN5payAeSLr", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 916, "prompt": "Is there femoral head in this picture?\n<image>", "pred_response": "No", "answer_id": "dYSFbacEEiLN2eqiVz2Qf2", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 917, "prompt": "Which is bigger in this image,liver or stomach?\n<image>", "pred_response": "Liver", "answer_id": "NJmWbX6gYXAHUMeanG4oKp", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 918, "prompt": "How many kidneys have existed in this image?\n<image>", "pred_response": "2", "answer_id": "f3PYpEsYhqVoFJc5EDQa9h", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 919, "prompt": "What is the shape of spinal cord in this image?\n<image>", "pred_response": "None", "answer_id": "JsEZPkPfEaxTGWNHNBBt6M", "gt_response": "Circular", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 920, "prompt": "What is the function of the rightmost organ in this picture?\n<image>", "pred_response": "Liver", "answer_id": "h8FaQ3HSQGNYJWeHGRXrMy", "gt_response": "Improve the body's immunity", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 921, "prompt": "Which organs/organ in the picture belong to the lymphatic system?\n<image>", "pred_response": "Spleen", "answer_id": "LyMdnFjqqp7oAeiwYoAmiw", "gt_response": "Spleen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 922, "prompt": "Does the picture contain the organ that could receive sound?\n<image>", "pred_response": "Yes", "answer_id": "BZLU3YLdKY3H6vozvwMctc", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 923, "prompt": "Does the picture contain the organ which has the effect of gas delivery?\n<image>", "pred_response": "Yes", "answer_id": "5oEc6D68DUD25iefhbAhw9", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 924, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse Plane", "answer_id": "Qf8VUXDFdgbo2NisgmB4UL", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 925, "prompt": "Where is the esophagus in this image?\n<image>", "pred_response": "Left", "answer_id": "8sKGSC5sNkU7S3Sct4aCTj", "gt_response": "Center", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 926, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Liver", "answer_id": "9nvr8jp7fY3Tb3HUteYBJG", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 927, "prompt": "Does the esophagus exist in this picture?\n<image>", "pred_response": "Yes", "answer_id": "GyhzhnGb7oQZRr8CJ7jnnA", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 928, "prompt": "Does the stomach appear in the image?\n<image>", "pred_response": "No", "answer_id": "E5e54vbYjfCUYnDfiyiM6t", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 929, "prompt": "What color is the spleen in this image?\n<image>", "pred_response": "Gray", "answer_id": "o2VGndnyFdrftcMuUbZftq", "gt_response": "Gray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 930, "prompt": "Which is the biggest in this image, spleen, heart or liver?\n<image>", "pred_response": "Liver", "answer_id": "i6b3HG6ESUokuxeQt7FmtB", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 931, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "1", "answer_id": "5o7bYHZnqmBzdiqqgpQbzF", "gt_response": "1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 932, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "MqeQkrxEHiDYSHiExeYkPE", "gt_response": "Much", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 933, "prompt": "Are there organs in the picture that can digest food?\n<image>", "pred_response": "Yes", "answer_id": "DWEj73jVUEKAs7Z39VasJJ", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 934, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "9m3sopcWN4FNtvAnFa7f8h", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 935, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "ff8PNuPirqdaYcvrCfsqos", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 936, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "nHSF7DtbKsMU2VRkjhw7P8", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 937, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Brain", "answer_id": "P5iF9SNYBeuGH5vrwCREWG", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 938, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "aLy9JYfJMyxiUYkEZDduig", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 939, "prompt": "Does the picture contain brian?\n<image>", "pred_response": "Yes", "answer_id": "KnBNkFMseHZFmBPnYNqJbp", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 940, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "8XxLgsHKaNfNd8eGub45dk", "gt_response": "Brain Tumor, Brain Edema", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 941, "prompt": "Is the brain healthy?\n<image>", "pred_response": "Yes", "answer_id": "9sabRLkVcDYqgkrL8fXJsi", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 942, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Right", "answer_id": "TP2zS72QZ2yP7cPHADPWvu", "gt_response": "Lower Left", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 943, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "kM9eYsAFwuvi88DT9v2NEJ", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 944, "prompt": "Where is the heart in this image,left,top or upper left?\n<image>", "pred_response": "Upper Left", "answer_id": "LLsH762mbTuGemXSMLncKm", "gt_response": "Top", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 945, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "NXposmu83GhjjUrCEJf4UB", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 946, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "Yes", "answer_id": "iVq2tFnFnmeLqiKBpqhjgF", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 947, "prompt": "Does the picture contain stomach?\n<image>", "pred_response": "No", "answer_id": "D2haaNfxrKJKmnPaJ8zUT9", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 948, "prompt": "Which is smaller in this image, esophagus or liver?\n<image>", "pred_response": "Esophagus", "answer_id": "fnotKCb2mvQ2dCg3gkFqQ6", "gt_response": "Esophagus", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 949, "prompt": "How many lungs have existed in this image?\n<image>", "pred_response": "2", "answer_id": "b9NMeMwCLgqK3nbJR7YSJe", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 950, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "a4UfAXXmUVxdi9UvmPDnqK", "gt_response": "Much", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 951, "prompt": "Which organs/organ in the picture belong to the digestive system?\n<image>", "pred_response": "Liver", "answer_id": "fFATxiETQxyCbjJkRBX4kP", "gt_response": "Esophagus", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 952, "prompt": "Does the picture contain the organ which has the effect of gas delivery?\n<image>", "pred_response": "Yes", "answer_id": "ZaYwcRGhefygoZdrMrwXXL", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 953, "prompt": "Does the picture contain the organ that could promote blood flow?\n<image>", "pred_response": "Yes", "answer_id": "h94xpWhkAMTKEBYLZ5ybBr", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 954, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse Plane", "answer_id": "bKVybhfP5NPLgrEyMeV7Ty", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 955, "prompt": "What is the leftmost organ in this image?\n<image>", "pred_response": "Lung", "answer_id": "EPeNLHWbezGNNGUBzVunnX", "gt_response": "Right Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 956, "prompt": "Does the picture contain spinal cord?\n<image>", "pred_response": "No", "answer_id": "TdV6qpGEL6862Hm68qgVv3", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 957, "prompt": "Does the rectum appear in this picture?\n<image>", "pred_response": "No", "answer_id": "9c8H5FnrZ5et8oCXEqywTc", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 958, "prompt": "What color is the lung in this image?\n<image>", "pred_response": "White", "answer_id": "NrMfijqrFhxNxZQGRnWqKo", "gt_response": "Black", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 959, "prompt": "Which is smaller in this image,heart or esophagus?\n<image>", "pred_response": "Heart", "answer_id": "kE239EWsSNFuBERJj5SUKv", "gt_response": "Esophagus", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 960, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "1", "answer_id": "ae9Ck6Ya3Js6r4jZEUij9z", "gt_response": "4", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 961, "prompt": "What is the function of the organ on the top of this image?\n<image>", "pred_response": "Lung", "answer_id": "nMt7fRfAmycRMv2VCqGirn", "gt_response": "Promote blood flow", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 962, "prompt": "Does the picture contain the organ that could receive sound?\n<image>", "pred_response": "Yes", "answer_id": "nWtCtHG8kcsiiXXrVbZCgD", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 963, "prompt": "Does the picture contain the organ which has the effect of sensing light?\n<image>", "pred_response": "No", "answer_id": "Eu7MK933Hts6hcArNGXvqJ", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 964, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "WBbDGAwfDGw8T488sqSKCk", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 965, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Pelvic Cavity", "answer_id": "VP2Xgp2aNVqRLMgbsi93zo", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 966, "prompt": "Where is the rectum existing in this image?\n<image>", "pred_response": "Left", "answer_id": "MgZ5f5nTvUfdYMyQXZ28tB", "gt_response": "Bottom", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 967, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Urogenital", "answer_id": "n3o2N5aigtWzHA8Sn4UMTr", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 968, "prompt": "Does the picture contain bladder?\n<image>", "pred_response": "No", "answer_id": "VL5rTQxktXyGkdmaqquZGv", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 969, "prompt": "Does the parotid appear in the image?\n<image>", "pred_response": "No", "answer_id": "JAhpjH466gB3jgJp8foZjp", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 970, "prompt": "Which is smaller in this image, bladder or rectum?\n<image>", "pred_response": "Rectum", "answer_id": "X7Sm4vqEdSG6dQRfhEewFm", "gt_response": "Rectum", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 971, "prompt": "How many femoral heads are shown in this image?\n<image>", "pred_response": "2", "answer_id": "NPHsonUZ56HfNXMBtY5UHv", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 972, "prompt": "What is the function of the organ on the bottom of this picture?\n<image>", "pred_response": "Store urine", "answer_id": "kZQUHZQjB9wmkYz26eJfdR", "gt_response": "Store feces, excrete feces", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 973, "prompt": "Which organs/organ in the picture belong to the digestive system?\n<image>", "pred_response": "None", "answer_id": "UGR9CV2GQQhLmie688hBoU", "gt_response": "Rectum", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 974, "prompt": "Does the picture contain the organ which has the effect of discharging waste?\n<image>", "pred_response": "Yes", "answer_id": "KCY57wLVBuVwKL7jUJACmY", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 975, "prompt": "Does the picture contain the organ which has the effect of excreting feces?\n<image>", "pred_response": "Yes", "answer_id": "nYXPNkCdhv3v9zkfgde6v4", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 976, "prompt": "Is this a coronal section?\n<image>", "pred_response": "No", "answer_id": "JqBYSLdoUfd9MS9dHQCsdj", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 977, "prompt": "Which part do the organs in this image locate from the aspect of the whole body?\n<image>", "pred_response": "Left", "answer_id": "c9RhinmhFLSU2tq778Uknv", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 978, "prompt": "Where is rectum in this image?\n<image>", "pred_response": "Left", "answer_id": "CzkbTzfFZ9NJ5jU2LVcYSS", "gt_response": "Bottom", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 979, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Pelvic", "answer_id": "BUXH9EvH4xVFLGcZZu9jzy", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 980, "prompt": "Does the picture contain rectum?\n<image>", "pred_response": "No", "answer_id": "RmXCYGK86zADQdzbiEmB94", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 981, "prompt": "Does the picture contain bladder?\n<image>", "pred_response": "No", "answer_id": "ZqcSLvpUd94Jyou3ZZNkRw", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 982, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Bladder", "answer_id": "5kcqbJjUqHxdtLz6KHr33F", "gt_response": "Bladder", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 983, "prompt": "What is the effect of the gray organ on the center of this picture?\n<image>", "pred_response": "No effect", "answer_id": "J7ZQQT6MNp4CXr9Jf8nW2a", "gt_response": "Store urine", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 984, "prompt": "Which organs/organ is part of the digestive system in this image?\n<image>", "pred_response": "Rectum", "answer_id": "AQ86qnviwRuKoX6YtmroYZ", "gt_response": "Rectum", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 985, "prompt": "Does the picture contain the organ which has the effect of excreting feces?\n<image>", "pred_response": "Yes", "answer_id": "29dxnAwzpe45RJzLdVrbyG", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 986, "prompt": "Does the picture contain the organ which has the effect of detoxicating?\n<image>", "pred_response": "Yes", "answer_id": "UjTwc5ppt3Kfhe7ca8ELRg", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 987, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "fiwj64fDT2LUcf4kf8kiNh", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 988, "prompt": "Is this a study of the pelvic cavity?\n<image>", "pred_response": "Yes", "answer_id": "ZK6umhDeewS2FRxkyfmTLs", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 989, "prompt": "Does the picture contain colon?\n<image>", "pred_response": "No", "answer_id": "kNue6TYpVd7aZZkxcLRMjX", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 990, "prompt": "Which organs/organ in the picture belong to the urinary system?\n<image>", "pred_response": "Bladder", "answer_id": "RrFBw2GMuhRKMxBvYpQJh5", "gt_response": "Bladder", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 991, "prompt": "Are there organs in the picture that can deliver food?\n<image>", "pred_response": "No", "answer_id": "bFjzSadzuzN5vKxmMNh6Hw", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 992, "prompt": "How was this image taken?\n<image>", "pred_response": "MRI", "answer_id": "Uwe7Vz7fmoeLCW7i2xExya", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 993, "prompt": "Where does the picture show?\n<image>", "pred_response": "Abdomen", "answer_id": "hvZrbHuh6LvgJMNvYB4ALc", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 994, "prompt": "Does the rectum exist in this picture?\n<image>", "pred_response": "Yes", "answer_id": "2YN9LW3Uth6oUBBGn4TDTv", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 995, "prompt": "Which is bigger in this image, small bowel or colon?\n<image>", "pred_response": "Colon", "answer_id": "9bXLGJ5E6NdmvUzb6XM6fm", "gt_response": "Small Bowel", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 996, "prompt": "What is the function of the organ on the bottom of this image?\n<image>", "pred_response": "Store urine", "answer_id": "7Htin6UptQGUyUhueTWzPD", "gt_response": "Digest food, absorb water, excrete body waste", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 997, "prompt": "Do any of the organs in the picture belong to the urinary system?\n<image>", "pred_response": "No", "answer_id": "bTzKpNz8pnNpWEFd7CqDiG", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 998, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Abdomen", "answer_id": "VMgmewHgjNALSkTh4QZ5DR", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 999, "prompt": "Is there colon in the image?\n<image>", "pred_response": "No", "answer_id": "azPFGZ324VScoZ3rGohgxV", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1000, "prompt": "Which is smaller in this image, colon or small bowel?\n<image>", "pred_response": "Colon", "answer_id": "7Eo3FP5jg8xtJ3aWhqtvvE", "gt_response": "Colon", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1001, "prompt": "What is the function of the organ on the top of this image?\n<image>", "pred_response": "Digestive", "answer_id": "FM636r67jzgGKg73QtKqb3", "gt_response": "Absorb nutrients, secrete enzymes, digest food", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1002, "prompt": "Do any of the organs in the picture belong to the digestive system?\n<image>", "pred_response": "No", "answer_id": "3WrGVMwDCvL5hHo4FEnFFQ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1003, "prompt": "Is this a coronal plane?\n<image>", "pred_response": "No", "answer_id": "3GAGktFcnJQMcTJYFp8D8g", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1004, "prompt": "Which part of the human body is the organ located in the image?\n<image>", "pred_response": "Abdomen", "answer_id": "SDrn9E3mAMp3JjvotMLUwf", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1005, "prompt": "Where is colon in this image?\n<image>", "pred_response": "Left", "answer_id": "XJmX8kEo3Hg3m7BWt27XB2", "gt_response": "Upper Left", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1006, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Abdomen", "answer_id": "oN3roRiP3fFWJF6hX2oTnu", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1007, "prompt": "Is there spinal cord in this picture?\n<image>", "pred_response": "No", "answer_id": "XEXhtew354UNb4Z8jbMdcv", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1008, "prompt": "What is the effect of the organ on the left of this picture?\n<image>", "pred_response": "Kidney", "answer_id": "8qpa6ZYXKonYcJYc6Fnp2F", "gt_response": "Digest food, absorb water, excrete body waste", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1009, "prompt": "Which organs/organ is part of the digestive system in this image?\n<image>", "pred_response": "Liver", "answer_id": "M7AxHcKihbMc4dRuAnGnET", "gt_response": "Colon, Small Bowel", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1010, "prompt": "Are there organs in the picture that can promote blood flow?\n<image>", "pred_response": "Yes", "answer_id": "Ker22qePEGdEjxDL8gh8kb", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1011, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "8fgvwrNb7LRkKJvaW7gpsM", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1012, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "AwQrPkBqLRhdcMMgChrwNE", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1013, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "MwNutdqoJAhsF9S2qoqpyF", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1014, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Brain", "answer_id": "4pGz2byfk6MwQrMwAEUtZG", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1015, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "kdDnxeFHFoxuMveqe4dG2L", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1016, "prompt": "Does the picture contain brian?\n<image>", "pred_response": "Yes", "answer_id": "8AEHRShvStaQbvAxHDckX3", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1017, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "8ZTdauuQYaBm6ZXrEsRmyQ", "gt_response": "None", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1018, "prompt": "Is the brain healthy?\n<image>", "pred_response": "Yes", "answer_id": "j7GxidwB76qqddveZp3oeZ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1019, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "oXUjSC7hwDYjBidP8CsLSs", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1020, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "Xy7srDkQBQxoxf3mmLPunX", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1021, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "Jj2wPnEgXZXfE2DyG42dJW", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1022, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Brain", "answer_id": "Lhm6hoMnTLkRi224X2rAR2", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1023, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "7zm7sUhD8bqmvog6Q8t3qy", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1024, "prompt": "Does the picture contain brian?\n<image>", "pred_response": "Yes", "answer_id": "TwGA4DTAENPn2YUALSkePi", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1025, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "None", "answer_id": "6i6dSv5dTN5YyTv4StqXvN", "gt_response": "None", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1026, "prompt": "Is the brain healthy?\n<image>", "pred_response": "Yes", "answer_id": "FVxXJZbamRYUjj5pqJtpKD", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1027, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "ewVZHJ3J9iaQA5T6Kth9Q5", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1028, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Lung", "answer_id": "L3HHG4Q8yoo4i2rgNsXZvo", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1029, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "6uWAJ8bvfy4ELvzzK6m9DH", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1030, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "X7Z5i8ENEnzYKUJ9HeA7AQ", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1031, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "kS3CBUpqoTBE8ENJtopN4L", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1032, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "hHofNrgzraEHMXQeVyv844", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1033, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "DXC72FMxXuGysRdAAZUJCm", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1034, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "73DGDDe8yyaJegNJdpWhuz", "gt_response": "None", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1035, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "YbRAsRcHVAuJVS5y6cePF2", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1036, "prompt": "Which is the biggest in this image,lung,liver or heart?\n<image>", "pred_response": "Lung", "answer_id": "es2Z2XF8mX7m5E4GWHoJYc", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1037, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "4NaRuq4qPgWwFd6rZM5PNt", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1038, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "PtnyGFzMBUNDgiBtbCRkNj", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1039, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Liver", "answer_id": "hZZrZtyd6CGkMZEZvJEmvX", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1040, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "CoVvSxQX5iaA2F84oTAFhf", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1041, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "JxgMeG395iHiDTLAo8cuja", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1042, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "joxHX8DpWwpfEAEUnqv3hT", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1043, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "Yes", "answer_id": "nBFUky9vsn3jR5cqarQQtN", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1044, "prompt": "Does the lung look healthy?\n<image>", "pred_response": "Yes", "answer_id": "SG7FpAoTJ5Xdub6sqZDqhL", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1045, "prompt": "Which is the biggest in this image,lung,liver or heart?\n<image>", "pred_response": "Lung", "answer_id": "mEBQ3vpcSLTuXhj9Wmmo6d", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1046, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "LVSPnrWFQpoddwnAHz38Ve", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1047, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Lung", "answer_id": "CM77PuvNnve5wKiST5dGDx", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1048, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "KpWjvHeHJRtL5L46Mby6n3", "gt_response": "Lung, Heart", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1049, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "bpwDAxsRLiMBwF5Amj3FPK", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1050, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "m8GcymL9TTDnBSCjrW7xSQ", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1051, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "R8p743yuouFAfqR8QFYRCZ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1052, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "gC8puVANh2uLvV77eKqJgU", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1053, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "oYb98wgv5aCTucp2wgC9MW", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1054, "prompt": "Which is the biggest in this image,lung,liver or heart?\n<image>", "pred_response": "Lung", "answer_id": "mxWrF6nhfbwxQi6GiRXEB3", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1055, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "76dvnRSjQTmWThbVfJkFa6", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1056, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "6jRqh75egyoaNhvSqEjcr9", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1057, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Larynx", "answer_id": "EjKgDbtVUjEdnuojzyakC3", "gt_response": "Spinal Cord", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1058, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "MsY6rba7sYTHtn9nb2pj5t", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1059, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "CusdKV9Bu6sHdVwAM23dPo", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1060, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "DpgTJ3sMjdqtnx9aV2HFtG", "gt_response": "No", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1061, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "6JMfJyg2xBehv7LxnyL6So", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1062, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "oWhKSqcpkpbpNZhVNiJ2LP", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1063, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "k5BQTp8LU8avGdhyTtmFxA", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1064, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "Komvzndi7RpzaZi4VTH2Fj", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1065, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "UJ2ERg8vJ7RTuYRiNPMyqq", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1066, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "oNGoAoyRbF3QwH6yvUtcPE", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1067, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "WJHEdw7niZkzxyRVNy8MTa", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1068, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "VA8VTKrUhgq5kQwwURdZFU", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1069, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "TZWnrt3uaQxxj6kFPE9Spz", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1070, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "RPmwZXU9nvdmoAWo8nSXzg", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1071, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "HS4jxuVbCDWp7Hz5PE7R2e", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1072, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "muLL2HHchrzrKnCgfafRJ8", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1073, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "87cnZVeZVqZkKdosXp4VeM", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1074, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "kBmqJAzhquzT3gQaUwWZZL", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1075, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "XHvMDnRmqMmehP7VJRZRrx", "gt_response": "\u80ba\u764c", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1076, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "3fKzpqFQwNarCEWBvuLNqh", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1077, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0a\u90e8", "answer_id": "oPQFHcWeiPGwyGbZZ3Lmqr", "gt_response": "\u53f3\u80ba, \u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1078, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "ZEFLCDJXM6fVTq8XcxGr2h", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1079, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "7hcUhgAWsdQVCRAb93PRWs", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1080, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "ZX9xcpcxTaUVLfMkZCHz6D", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1081, "prompt": "\u813e\u810f\u662f\u5426\u5b58\u5728\u5f02\u5e38?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "dLnVLARQVXreQt94q7Dt4e", "gt_response": "\u5426", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1082, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "FeNUWf7fd7VC8obbPR8tE8", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1083, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "A6ZmPV923cV2v2D6NCcaiQ", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1084, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "X3LfBDBvFVCB6i8uVFYqC2", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1085, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "fTo9VKkewQvJrVYnXPSn7y", "gt_response": "\u53f3\u4e0b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1086, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "NbcSfH9DMXbsupwmB7wrjU", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1087, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "jFFiaehoBXqoSToNRnUFsr", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1088, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "Ag3hgwBBhxuzZaZ36QszgC", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1089, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "8hLqWbog5gcX945Ujn4cws", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1090, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "RxHwkgfhtHb8762xvMWCJf", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1091, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "nj39Hjq2sU7w5LzGwHG8W8", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1092, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "B6GsoVU4jggG9DcUNfFvj5", "gt_response": "\u80ba\u764c", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1093, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "mvTxYMPEQ75ZNDWJxYGr7S", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1094, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0a\u90e8", "answer_id": "5YpQe7k89PmsPnCsXa96dr", "gt_response": "\u53f3\u80ba, \u5de6\u4e0b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1095, "prompt": "\u56fe\u4e2d\u80ba\u3001\u5fc3\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u80ba", "answer_id": "nZxSwBcuyks3PSrx8ivL8b", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1096, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "BfcWsmreK9ChSRPKGoUnpa", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1097, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "iztFcyP275PkQFj9cH6eTQ", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1098, "prompt": "\u56fe\u4e2d\u5b58\u5728\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "4jEYwujDRGFMaHP2GqraPv", "gt_response": "\u5b58\u5728", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1099, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "WJbePpn69MiX5ZkQkWRkiA", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1100, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "SwvExCiHuP6bvcZkMKVd5B", "gt_response": "\u5fc3\u810f\u80a5\u5927", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1101, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0a\u90e8", "answer_id": "XhKCNkDrGNtLUtkCpLQvHc", "gt_response": "\u4e2d\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1102, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u662f\u4e0d\u6b63\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "RCbitYd7nncgQ9ZJAoLwH7", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1103, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "6BrwW97rCAKkTS4qUFQJNs", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1104, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "F5rG4yNh7DEerGDPoRu4SE", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1105, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "L3jrwyvAAqajJcunMewKZa", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1106, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "ByHeTQ44wBiQS7HbXuR2Rv", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1107, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "GQHkp4XcSFv75jYSZzBYTK", "gt_response": "\u5fc3\u810f\u80a5\u5927", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1108, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "PFEeLVzGR8AF3VKYvfLBWi", "gt_response": "\u4e2d\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1109, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "RmwyLH97fYwa7FZdtmMuFG", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1110, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "APpZEdyv4t3zHzvrH5kSox", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1111, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "mh6egW6JM5iaYo7fQEdLij", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1112, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "VX3SgbRUjLBsxdC5k8pbcE", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1113, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "hRHD6AJU5dEexa6jnZwuP3", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1114, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "CTX29gvMpTFh6Hy89bFTB4", "gt_response": "\u80ba\u4e0d\u5f20", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1115, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "V38RRi7oDp2y3jopZJuncZ", "gt_response": "\u5de6\u80ba,\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1116, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "5zJXNyFmXT8Antj7H9Xur2", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1117, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "E98xDoszX2Dwx83fRNDiWN", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1118, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "75a9cpTsokxuDpgoBYc7LQ", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1119, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "gyXx77hR8FXQ3UQ5DmPnpR", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1120, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "SH7SMi4i77vZSvCmSAquNz", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1121, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "W3QTctmx5YwWJoyCmEAYTa", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1122, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "97dTbec2yg3inhmcyabGDr", "gt_response": "\u80ba\u7ed3\u8282", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1123, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0a\u90e8", "answer_id": "Sfx3ninLUXWBdsguR8VZkY", "gt_response": "\u53f3\u80ba, \u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1124, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "NomUvoesw4wYChwPLSn7FQ", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1125, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "Kn2Z2XVbiwmrHBJRwpjY7h", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1126, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "bB4a6TBxm7xW578LAEpVi7", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1127, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "9arw5pR923pCVhZ2fXy6AV", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1128, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "gSeDsX4wuC77ZX5cyCfBtw", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1129, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "dpuLTx6YuJw2FwAyhvUcZc", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1130, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "Y46LdojWk8ZfsxHDsWd3gw", "gt_response": "\u80ba\u7ed3\u8282", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1131, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u53f3\u4e0a\u90e8", "answer_id": "WDCrg6V9jojEhoYCbXm6co", "gt_response": "\u53f3\u80ba, \u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1132, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "b3qCoBkU3DQ3mMpJkQWUkL", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1133, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "PyKbav4oz2ojo8zmrVGe5P", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1134, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "ABdZhapBAFYubu9RoH5LqS", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1135, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "nvFdCvGhSnF7jZpCGZG9q5", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1136, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "DdRQCRvRFbHK8TmyS8WBsh", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1137, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "nvbe82bcxafiPX2B7DJztv", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1138, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "LfWvmaASizvVqqPTQPBV7b", "gt_response": "\u80ba\u7ed3\u8282", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1139, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u53f3\u4e0a\u80f8\u90e8", "answer_id": "nt6waSxkcim3cf8WnYNLJ2", "gt_response": "\u5de6\u80ba,\u53f3\u4e0a", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1140, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "7As3AgAtoAJ6nMMcE2aP93", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1141, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "SKRQhrNXFWFD9cnLsSDCAR", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1142, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "RHGuTeDLwF7cqniK3ArjE7", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1143, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "oGYpTyRSTKszCKZRbsvPvp", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1144, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "CF7yEvTAJuFFHaWDin7a7V", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1145, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "SWcY47aVL2Z3HPgQUruTfC", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1146, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "5DWCGufRbu33nvBxLASTg5", "gt_response": "\u80ba\u708e", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1147, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "SmvHTu8qPvvczKtkYihvKB", "gt_response": "\u5de6\u80ba, \u53f3\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1148, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "BKQa9KySJ8oQbsZMbBn533", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1149, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "KZ6J2gy9e6RaebV3bk3HZU", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1150, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "iRWfvppUNsdq43MEKVTMva", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1151, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "ZsrLkZRkJJFNuyqBRNcp5B", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1152, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "eBzQ7sKBmGWmg9PghiSC8L", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1153, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "j3WQVpniYFzcVrjBZ44DxV", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1154, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "DSBhUwNgAVrmiUVrhFWdTW", "gt_response": "\u80ba\u80bf\u5757", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1155, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "Zj3c8NANseCQihfdz3tbdn", "gt_response": "\u53f3\u80ba,\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1156, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "kJxAFdbAFbQDcNW8wWGUkS", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1157, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "GroecDR3xrKZb7mEXBCokp", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1158, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "ftBdrMPeA7zX3eYpcxJzM6", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1159, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "GCcWMrX8e7j5azgzVX4kLX", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1160, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "6Q8DYPgxkonowgErSEy3NE", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1161, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "YNGjQcAZey7Rd9iLjtkXY8", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1162, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "KKf2gcM4V9bHfaNmtiyQp9", "gt_response": "\u80ba\u7ed3\u8282", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1163, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u53f3\u4e0a\u90e8", "answer_id": "9UAvUGTKiUueenL7fmCmEy", "gt_response": "\u53f3\u80ba,\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1164, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "2jWnGf3vpyZkGkTRKFBFAT", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1165, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "TnY82L8noq46iCdE9qyk7X", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1166, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "FE7zqFPnua3CVoxMx6tZqz", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1167, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "MkmyUzbyrktqdiRKL2JpsC", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1168, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "L5oqjBzL39ydrHStWkAD7n", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1169, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "NPoDctmCHBoJmJUtZChQ4W", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1170, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "EcJn4RCsnmRPLKfDwV9qxA", "gt_response": "\u6c14\u80f8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1171, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u80f8\u90e8", "answer_id": "UdDDfyvwJTTKu7eYo6H2Vc", "gt_response": "\u53f3\u80ba,\u5de6\u4e0a", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1172, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "AC2hTzjqHWTByDJLVpgWbK", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1173, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "NwWkfzKw8HeFqeNDgtavSj", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1174, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "J8tw8G5799yFJRsyCan3eh", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1175, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "cDT6vtGkHwt58TNU8eVMkW", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1176, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Ay2hdFS7Dg5Bwg7WsApXLZ", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1177, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "MUGpVWDoeS3UwZRQa5tfPV", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1178, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "Zb6NqTsof57UPFuWAFChAT", "gt_response": "\u6c14\u80f8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1179, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0a\u80f8\u90e8", "answer_id": "Uha5wteaLDpJK2Lz9ar7EF", "gt_response": "\u53f3\u80ba,\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1180, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "fTxjNU8buHPaeCma3qAysB", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1181, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "SQEWdvx6oZLhiDC7fmUFFV", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1182, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "VtTykxALTDHtXtLpzbyYU2", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1183, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "5WBsPvDV84eC3aGDgV2Feb", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1184, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "RyJwv9pN6gc9CBU8FYB6Jd", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1185, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "HRVQXbAKmWUSoTHpq2sBWm", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1186, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "gZueouTWkASBeb8f2H7qTT", "gt_response": "\u80ba\u4e0d\u5f20", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1187, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u80f8\u90e8", "answer_id": "Eq3GpmniHQzuMtqxNxtVKy", "gt_response": "\u5de6\u80ba,\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1188, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "QfiCxgm4hjw4VxD8EYJcyB", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1189, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "6xK5daKiefUWSz7ymVAqKn", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1190, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "ZxxW2L8AtsAyRnHJr72rfv", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1191, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "mC2JZwGLRSdt3tnSqTLTTJ", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1192, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "PBm4Kg4WvfArzRBmfSRFVV", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1193, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "8KXmnAE2Yefd6z23o7LEhh", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1194, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "95p6GqspRs5ugRiXyhVnQj", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1195, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "SwnaH72TCjEGksL8zTnsdo", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1196, "prompt": "\u809d\u810f\u548c\u813e\u810f\u8c01\u7684\u4f53\u79ef\u5927?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "gwaBk99JPMKtTmEQeXdLxj", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1197, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "H9xphzzzMwkHb3JFWpqHvU", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1198, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "HR2DCfx8zoJuUe9vnvBTn9", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1199, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u662f\u5426\u6b63\u5e38?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "XSgCDX9uhmjCpaQaHFhS6a", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1200, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "WFpN2EvC8YeWZmZB27q5jF", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1201, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "Pcv394MUc2L23wYAGNEsnV", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1202, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "c9dqsWZuSTDgcryMEYJbWN", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1203, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "3H34UN5kNdMDyre24iMdyz", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1204, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u809d\u764c", "answer_id": "6r6bqw3usuinuganqNAUrS", "gt_response": "\u809d\u764c", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1205, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "mhdFEDUMqMBC5cccUav7K4", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1206, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "EHR4wiDUBeJLS2xAjY6EZ9", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1207, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "TThF4k4sHFXXxu8XfsVHSe", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1208, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u53f3\u4e0a\u90e8", "answer_id": "CaPEtMxMbNg9vPRe7JgA9y", "gt_response": "\u809d\u810f,\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1209, "prompt": "\u56fe\u4e2d\u80be\u810f\u548c\u813e\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "Gpc5D3KJtCeipH3kgocNVm", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1210, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "JBbTooMBAkPyLn24mxQJTP", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1211, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "RYoevhrSBLxU7YnyqHUadu", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1212, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "NrPtR6yz8t8SYpYMyJ8Wvc", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1213, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "h2qGcsBfwdmCjtGCY8xejM", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1214, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "afoukacuxcpdkD39hERMGu", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1215, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "d9HmtVqfpFYzFjpgkJf9dM", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1216, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "FPgpfh8Yzd3s82wBnRxuUL", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1217, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Svf5n592bqSPLp5JHMzPd9", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1218, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0a\u90e8", "answer_id": "XM9D3EDGwSuN96rparZ4bb", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1219, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "BrfTjFRLvvSWAzAzrjP5zZ", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1220, "prompt": "\u56fe\u4e2d\u80be\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "424wcZJtin9GvHc2tqEo23", "gt_response": "\u80be\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1221, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u80be\u810f", "answer_id": "6YZ7hbvvN5Qh2jsH9HTQdk", "gt_response": "\u5de6\u80be", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1222, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "o6US74xhKYiw6qrLNPauyD", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1223, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "knMpN2xwkK8a35AiHPYVeh", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1224, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "J7Axvor6kHornDJoceZdgx", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1225, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "YusVE7MEKad7VMPQJsSRsG", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1226, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u662f\u5426\u6b63\u5e38?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "FYESQTveJVh9u9SAMvFs4e", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1227, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "meEEGQLWuti3hisksrRsDm", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1228, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "32x2kaiTsU779G3h9T6Rv7", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1229, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "XU9JAhfkmBjDrJAzfXEhvZ", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1230, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "YtedFRuvbqbFnZzRMLNh4U", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1231, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "LFSuajDLRn65gkminbLZXq", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1232, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "TWmkukmpjzR5Mdud8ajcjd", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1233, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "9zySs7mQQGWPdLLY5uwZhL", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1234, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "oTytkn7YDvkbdQWMdL6u9N", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1235, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "jPFy7cYbxMgmqfL5HKvVX4", "gt_response": "\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1236, "prompt": "\u56fe\u4e2d\u80be\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "8AoAVnk9EZF4nctZ4ScwW2", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1237, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "HkMMxkpWrG4YXFsdJ8jLVa", "gt_response": "\u5de6\u80be", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1238, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "2PdQzQVvgwX6KRTNoAU3Sk", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1239, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "PEFGkYpFZNRccNpJBGuy7w", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1240, "prompt": "\u8fd9\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "CSA3Xvj4zMjDLBPB5kD8XZ", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1241, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "J6hH5EWawZ5ZNJShCBwu6Q", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1242, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "BJcbW8zm9LdgcG7ebtQCbu", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1243, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "8jucHBL4mKdxYXUnjERYFw", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1244, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "oKrwSSvXUMNqrWmDQ2LSis", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1245, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "UL7cCwkQdVZNB8jaaiszNE", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1246, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Je3ACVYh2Kmfa8QR2t7Rjx", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1247, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "9by9LXFjR8dgzEG4wvKzZK", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1248, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "jjffTsanEQUF5z9MH8U3ip", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1249, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "KjpAkss7xT9wBGrzqDbJAJ", "gt_response": "1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1250, "prompt": "\u56fe\u4e2d\u80be\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "iBjit5FeSrj2WtoA2S5Y7v", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1251, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u6700\u53f3\u4fa7\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "6ygBKC8bJkbbnQyV53uR8n", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1252, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "iAyr2odJQoYZ6sF8qXrMjC", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1253, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "QzSVpKfABFMKNGD26dt3F7", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1254, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "SZnX7GkR4o2rdXYg6iqaQB", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1255, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "gYVTEdRJF6Ht2Kbn5zXJfY", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1256, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "8NSuJDHgrWzgyeNLaQDHJQ", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1257, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "FCmtsZWSWUs3sPtMsNb2SM", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1258, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "SSdVttcPUSoJgYVu3dx5PX", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1259, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "aspQRhZawP7aK5CsWFg6Sg", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1260, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "ZKshWLd4WdyJtujjPjusCV", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1261, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "3pRfydfg9QgwLXRQTtfng4", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1262, "prompt": "\u813e\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "dtqAbH8GAqaruva8BDPCgz", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1263, "prompt": "\u56fe\u4e2d\u809d\u810f\u548c\u813e\u810f\u54ea\u4e2a\u4f53\u79ef\u66f4\u5927?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "i5MaV6cCTGqEuMRqx6Jwd9", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1264, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "QhNq9qwhmKA4dsM2UExrzL", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1265, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "Q9Mf4d3kRRGuCtxcpQF8N9", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1266, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "U3FrFCD9eT9tcx42Hz2Fjn", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1267, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "mK7GyM6D3EjNXQBiUdXhQH", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1268, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "VvzXgPoExTaQHeFCYUmxCm", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1269, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "4PpQzq6WnV7w9TejnPJZYM", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1270, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "mSUeQG63V4Pts7GPrJYAeQ", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1271, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "WQCG6xhknQXdLunx3BVu9k", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1272, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "fkhUAj9Cn2UXqFAUcGFrVQ", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1273, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "93CbMUFqUoBwsEJZEx6qGr", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1274, "prompt": "\u56fe\u4e2d\u5de6\u80be\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u5de6\u80be\u810f", "answer_id": "fjLp6X3udnixDFExv3jV8h", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1275, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u80be\u810f", "answer_id": "kwajLLekFtrRiBoHUhUdJj", "gt_response": "\u5de6\u80be", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1276, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u80be\u810f", "answer_id": "8xg4rJumWtrHjNf9WR3SKy", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1277, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "KTW9sSAJMPaEvVcWGKowCC", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1278, "prompt": "\u56fe\u4e2d\u80be\u810f\u7684\u5f62\u72b6\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u692d\u5706\u5f62", "answer_id": "MDZTdMmnH768QmNxsU4k6V", "gt_response": "\u4e0d\u89c4\u5219", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1279, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "C3fJjNDbsZUAGnvDwoPWsQ", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1280, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "ntof54RaQYGYmkw9N8WgBX", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1281, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "YQJGATgeZvdxPTx84iR2bg", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1282, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "cRGH6PaPwVCkeNJqqScQoP", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1283, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "c9GAuq8dCVGxbFyJQPHmJv", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1284, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "RtNm5bC7Swn9AzB5im278G", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1285, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "aPbQeR4Cike3bC6vjSMu2X", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1286, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "7GvqbCVadQCmME6G9MUbrd", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1287, "prompt": "\u56fe\u4e2d\u809d\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "2kz4jaDVXTffAV9cXMhLiN", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1288, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "ANSwsj2fdkn3we3a2p58KA", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1289, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "4ieBzHh3Y8y5iCSgQEHFTL", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1290, "prompt": "\u56fe\u50cf\u53f3\u4fa7\u7684\u9ed1\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "gDQLVcAfZbWMjoEiFYwm7T", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1291, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u7070\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "n3uWXbTfdJLJgqiMwwhwNc", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1292, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u9ed1\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "cmTqSZSXBzTevDBiEyowG2", "gt_response": "\u53f3\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1293, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "VAfEhezQ33fjYjFX82ALHa", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1294, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "nApR2eKMHHHeCpMVv4spKj", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1295, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "apbkJXJ3TGeTgMz5CqnusU", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1296, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u662f\u5426\u6b63\u5e38?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "nsaFKBv7ZvSkCwBSpQ8u2a", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1297, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "aCVhUqi6UKNAaz3py9yhTH", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1298, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "LGqs8V8JB4igLXLUQ5yzfa", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1299, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "nj5RSrtMQtw5ZYRjXJb8Eb", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1300, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "nFw2UTsLQutyf56x3xqHoX", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1301, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "9ynohKxHqo6C8Fo4qmsSkP", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1302, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "Y9EEBw8KNZuNpxEkcfGpw7", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1303, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u809d\u764c", "answer_id": "LjaHD4rd8nSKrembWpNhUd", "gt_response": "\u809d\u764c", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1304, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "jDmSyaGx3NRfkASU9MH2GC", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1305, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "MmYmWsKNoikvdcPg8Xms65", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1306, "prompt": "\u56fe\u4e2d\u809d\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "kKUM4R658L3ufzShVqqrNU", "gt_response": "\u5de6\u4e0b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1307, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u53f3\u4e0b\u8179\u90e8", "answer_id": "NduPdpjXnVdcSEFboF9R5R", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1308, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "T2oAQg8ZCewuHGDniNnLtk", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1309, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u7070\u989c\u8272\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "FHvNmGdozKvhWwzQBMGpZa", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1310, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u7070\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "2hTUffTcoSZ8zigdbC4Tje", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1311, "prompt": "\u56fe\u50cf\u4e2d\u8eab\u4f53\u9876\u90e8\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "5prmHxfBqFAEjoWi5sq9Uh", "gt_response": "\u80a0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1312, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "avHFRfYK5vFgNnUGPizRzd", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1313, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "QgQ5y8BgisDrq5aWHWt6mm", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1314, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "ajCXZgCb6Tr3wcwVseEygu", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1315, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u662f\u5426\u6b63\u5e38?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "ZXDsove4qhnXaeYzQWUiCz", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1316, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "j6nFC7xQh5HS2HFAKzpnUi", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1317, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "iwRdHpS6XiZe8otpcMeyTF", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1318, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "5Dev8j2RM6j5KDVGPoHu85", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1319, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "mMoSn5wLoEGoCGxaeQNZtq", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1320, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "RRDEyT4gdeyWKJjKLqBo79", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1321, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "4hp2XjTzxiJ2VU8sY942Qt", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1322, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u809d\u764c", "answer_id": "asZackvHHrVscmak3HkNsM", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1323, "prompt": "\u56fe\u4e2d\u809d\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4e0a\u90e8", "answer_id": "bxNJVBUmKx6sai2fJ53pKy", "gt_response": "\u5de6\u4e0b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1324, "prompt": "\u54ea\u4e00\u4e2a\u662f\u809d\u810f?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "XptUALzrzUJCTnc9hwLrj6", "gt_response": "\u5de6\u4fa7\u5448\u7070\u72b6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1325, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "YzvpoTEVdP3puYwRVf2YNU", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1326, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "U3Mua4CTZqWZpe8TtrdeNP", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1327, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u8179\u90e8", "answer_id": "kSiCukKtXaoydtzwjbFfck", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1328, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "gtPqYAbfXCBZJd57yAJ5GD", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1329, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "FGS8v5XSfvSmNuk8Kyy2nx", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1330, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "ML4kaMDfYGctdpE9DP7irv", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1331, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "SYf4uFqufA56q4Af8gdkUg", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1332, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u662f\u5426\u6b63\u5e38?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "R3YvhdgXi43rxHErQy2kM5", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1333, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "9GYARfqxVn9RPMo62AdRmN", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1334, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "KxcTNkQj2hqwcnakngnC9b", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1335, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "cxdqfgriRizTnHsE4UhiwN", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1336, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "MfTGqR4pAe5dfuYGXejLtA", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1337, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "VtpTZaNDtaTkvHMLzgsb2i", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1338, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "ZPz5MTWH3LLiwAWPyHeUU7", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1339, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u809d\u764c", "answer_id": "T9s8FjD6hRonCbiJgFc3sj", "gt_response": "\u809d\u764c", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1340, "prompt": "\u54ea\u4e00\u4e2a\u662f\u809d\u810f?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "Yazb3pFhvCbAMLjQpCQoe5", "gt_response": "\u5de6\u4fa7\u5448\u7070\u8272\u72b6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1341, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "fpJkNBfdP5VBNJkbKtLMFW", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1342, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u8179\u90e8", "answer_id": "gmGCVV5exvmcdGJPWgmCFz", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1343, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u80be\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "WzQvWagxQ9xMMLS9bncMgq", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1344, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "7yrvj2nKbv64sAbvf56nEp", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1345, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u7070\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "JYnasWsQkhctPLNA3MG8ZD", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1346, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "NqBVM9uktnRe9ZwjLNFtBm", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1347, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "ciy7qkDqm9Uf9kbHmJcvz4", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1348, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "JRoRMfhSWur2FRZnRqZKz7", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1349, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "nfhUUeLztF3T2u2DkkYsT5", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1350, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "2EfgZ8SqxTyH9ZEHucNV5b", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1351, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "MHfEaxiirCGbc7JxrcpknG", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1352, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "EmcjLkFZP75PYNsyRDqMRb", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1353, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "9fJvBHYhGApk59ooLvsMJw", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1354, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "XwtTyu3UbCa8YrKK6wMZJg", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1355, "prompt": "\u56fe\u4e2d\u5fc3\u810f\u548c\u80ba\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u80ba", "answer_id": "TuBxcVSCe58SywLyPg6oeG", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1356, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u6700\u53f3\u4fa7\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "8bzPUaAdsHgEWJrSPKZyVN", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1357, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u9ed1\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u80ba", "answer_id": "bf6ucQN797NwszPNo4L9PX", "gt_response": "\u53f3\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1358, "prompt": "\u56fe\u50cf\u4e2d\u8eab\u4f53\u9876\u90e8\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "n7joZria5LJSr6GHaHHKH7", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1359, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "g8PYdF92b2dW6vxP45rgHK", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1360, "prompt": "\u56fe\u4e2d\u8eab\u4f53\u9ed1\u8272\u90e8\u5206\u5185\u7684\u767d\u8272\u8109\u7edc\u72b6\u7269\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "9pveRDPkXcvywhkPuPSGoc", "gt_response": "\u80ba\u652f\u6c14\u7ba1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1361, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "MKqbvgR9njLS8ZtkvHEW9s", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1362, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "AFZfqgbvsq4pLPw3z9kXgh", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1363, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "NmD6nBNqwD8uhVL2yVuLkp", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1364, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "2VxzjEBs4SfVhs8W6d6pct", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1365, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "Pxg4nh9wieK3QewWAT6gyB", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1366, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "TZ6brU5PxtEqWVDPt6vdkE", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1367, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "k7CkNBpBn44H5jDYM6bVGx", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1368, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "hmdstN4xUJpn3Wo6HRP3oW", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1369, "prompt": "\u80c3\u5728\u56fe\u7247\u4e2d\u51fa\u73b0\u4e86\u5417?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "NrLU3ZhLchaYSjpURaqnY3", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1370, "prompt": "\u56fe\u4e2d\u809d\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4e0b", "answer_id": "c627XcsbtJdqrMr6H4AfG4", "gt_response": "\u5de6\u4fa7\u548c\u4e0a\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1371, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "4DJwbg8xGHi68WvpdnhkoX", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1372, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "VH9jefdE3A7mEJWZXKPzo5", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1373, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u9ed1\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "BfFQkx2rPtfMwsnGJWc57Z", "gt_response": "\u53f3\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1374, "prompt": "\u56fe\u50cf\u4e2d\u8eab\u4f53\u9876\u90e8\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "HonhpAqAZH3zGi5pJ86fje", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1375, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "35ySjPikqwmeJdJwoWunvR", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1376, "prompt": "\u56fe\u50cf\u53f3\u4e0a\u89d2\u7684\u9ed1\u8272\u7a7a\u6d1e\u4ee3\u8868\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "i8YteAVwUF7e62tFvkhDYW", "gt_response": "\u80c3", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1377, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "dynagEsMhiRbmfcaWX5S9R", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1378, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "by72FrrvAty9FVda2Q8ZEn", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1379, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "NQ4ULC6RXSBYjvATe6tEHP", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1380, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "5szZwpE7RusLUsEYp9WEN8", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1381, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "82qjRtvV4o7fGR5gjtkAp6", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1382, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "ZRTvKXzDmyngmrsPYAANZU", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1383, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "NPjryPomNQfEy8Xi9y9GfH", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1384, "prompt": "\u56fe\u4e2d\u809d\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "hoGKRD4inorcBH7RtLrDn5", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1385, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "T9ErmfnFQcoXE4JdKvXJ5Q", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1386, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "3HESNRmPdUaY7S5KkvGfMy", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1387, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "32QExjHoYgfgPjCnFk3ugy", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1388, "prompt": "\u56fe\u50cf\u4e2d\u8eab\u4f53\u9876\u90e8\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "e2q5YhJzdw65F3hfwyyQdi", "gt_response": "\u5927\u80a0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1389, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "TNwacQqtQTiC4fNiFyzt2a", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1390, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "LphVRq4x9AurRPhzKhffpN", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1391, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "PbDkA6wHmUQC2kjMXiraRx", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1392, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "k38ZuZz9t2QPUHpMYtYrop", "gt_response": "\u5c0f\u80a0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1393, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "dYpMPrqD4VHX22BLQGwNnh", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1394, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "br8vcuBRA7DuehimxEefiM", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1395, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "XjBMk3NxyEWqZMX5jbAk4n", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1396, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "oKS75J8oPa8VeGffeBRVFh", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1397, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "5nYYPZGfzssCvrRAS4dYiB", "gt_response": "\u5de6\u4fa7\u80be\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1398, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "NUCBa5yZcGPcHbuG937A7z", "gt_response": "\u5927\u80a0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1399, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "jvzMxhEZg6SaWVxq8zqqZ3", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1400, "prompt": "\u8eab\u4f53\u5de6\u4e0b\u89d2\u7070\u8272\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80be\u810f", "answer_id": "a9LsEaWMWeB4RZ5fa2Yck4", "gt_response": "\u53f3\u4fa7\u80be\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1401, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "3sojQgMcZFJEo6qvbn3zWW", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1402, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "hBsGn2u5sbsJpE2nwA2EXp", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1403, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "Xz8xNp3MmN3oWoQ6aWiuAM", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1404, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "iGYPpwRs3WPunkxX7nL4eM", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1405, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "ew7CAyeFDwbaZugpSu2f26", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1406, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Z89dcdcW589N2bkWWng2bj", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1407, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "cXfTnxYmgC3XndvENEAzZE", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1408, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "5Ci5EBYhPiUY88aVmzLsen", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1409, "prompt": "\u56fe\u4e2d\u5fc3\u810f\u3001\u80ba\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "mfTt7QLrC4c9MpM6DCr7Yb", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1410, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u6700\u53f3\u4fa7\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "Mj34QBs5DB42zdYSGd9DzA", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1411, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u7070\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "LUMxKnMkfWG9emkhZEmTGA", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1412, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u9ed1\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "fHz9pbKyKA7MJa9FU8Zp4D", "gt_response": "\u53f3\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1413, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "Fn9DUacdmatkk6pcTSLGzk", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1414, "prompt": "\u56fe\u4e2d\u75c5\u4eba\u662f\u5426\u6709\u809d\u764c?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "E2CFfct6tgfdr3GaDAsQcr", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1415, "prompt": "\u8fd9\u662fCT\u626b\u63cf\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "LhQGU5Mvu9p9y59mQsFmpv", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1416, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "igEuCsRrG3CtAJ6xou6vLh", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1417, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "bQMNhydRetVQuccWFVtDVF", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1418, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "kkUZJhtPz9L5Ep9oDu3UTS", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1419, "prompt": "\u80bf\u5757\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0a\u90e8", "answer_id": "9vYeBh7ke4CKun9R2XXUi6", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1420, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "ZeuRdNaQJ87vG9suMmJYw4", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1421, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u8179\u90e8\u4e48?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "VNn2iJwYQV8UcgzRMhuaSM", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1422, "prompt": "\u75c5\u4eba\u7684\u5de6\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Aqt4wR7CbVF52FNy8cneT8", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1423, "prompt": "\u80ba\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "B9ayEMoUpT9Sx4VSNURUBN", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1424, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80bf\u5757\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "gV4tvysdQfGQJLTenUxqRo", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1425, "prompt": "\u56fe\u4e2d\u7684\u80ba\u662f\u4ec0\u4e48\u989c\u8272\u7684?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "J6DeQsTMaG3V8i6HyPYUFp", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1426, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u5fc3\u810f?\n<image>", "pred_response": "2", "answer_id": "fLqnX6pE52c6hFPTmcnc5x", "gt_response": "1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1427, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u5927", "answer_id": "9vBwG6ZQx5jbzC35xUvS4P", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1428, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u80ba\u90e8\u5de6\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u90e8\u6162\u6027\u75be\u75c5", "answer_id": "NykDwmuCxZNoKcamerPNR9", "gt_response": "\u836f\u7269\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1429, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "KcWphL9Vh59Ts6T5jzNUHc", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1430, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "2tFk2WL9VREvAJzsXS7s8V", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1431, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "2MbMfSPk33t3Bqd8VQsPVu", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1432, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "LWkA38ZTms3S499jww5jGm", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1433, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "FV7oCHfvKy4dnZgkYPjs6B", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1434, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "nuip8LBF3aTvcjN4WzP6pf", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1435, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "m2LwiZQHAqzLao3mYGnrps", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1436, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "WTmq3fcb3w9s82ZmLbPMBr", "gt_response": "\u5de6\u4e0a", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1437, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "dbZEogrMHrS8ShyZFmgGMu", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1438, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "4CVoBdNmqsaVifjjzxdQiC", "gt_response": "X\u5149", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1439, "prompt": "\u8fd9\u662f\u6a2a\u65ad\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "NcC9sqCvD4WiiCvUsQfQKY", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1440, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "JW9kK9Xt5yXZRkQrAN3tY9", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1441, "prompt": "\u56fe\u4e2d\u5fc3\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "LHgcnzZdAgvnLNNbpHdqhk", "gt_response": "\u4e2d\u5fc3", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1442, "prompt": "\u80ba\u4e0d\u5f20\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6", "answer_id": "a8ziNce7FVVLnXeQrEoBja", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1443, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "dw4uofKLWeUXh2FYgYwA83", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1444, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "CkU8MthFxTPTcqNZMmgWYC", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1445, "prompt": "\u8fd9\u5f20\u56fe\u7247\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "ckMat4YeKPsDJECtPA2LrC", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1446, "prompt": "\u75c5\u4eba\u7684\u5de6\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "VkczAwjegcqmJG9WDYwzpY", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1447, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u4e0d\u5f20\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "6KWx5q6jf5KjkHXadyKpyG", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1448, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "iYLY6Qj3rgZVkDLoFJRS2Q", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1449, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "mS6oySMNrh2rbirPUCUWpS", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1450, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u662f\u5f88\u5927\u7684\u4f24\u5bb3", "answer_id": "9hTnhVP5pEJpcNzFSBWtoc", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1451, "prompt": "\u56fe\u4e2d\u53f3\u4e0b\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u547c\u5438\u56f0\u96be", "answer_id": "Lf5ibgkThLA2AYqVNMYBNq", "gt_response": "\u80f8\u95f7,\u6c14\u6025,\u547c\u5438\u56f0\u96be,\u5e72\u54b3", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1452, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "bqRZXEd4qiZA4Lp9qFwdPV", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1453, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u663e\u793a\u7684\u662f\u54ea\u79cd\u7c7b\u578b\u7684\u6210\u50cf,\u6838\u78c1\u5171\u632f\u3001CT\u8fd8\u662fX\u5149?\n<image>", "pred_response": "\u6838\u78c1\u5171\u632f", "answer_id": "HSH35WHgjGqPqnTRi3aYXj", "gt_response": "X\u5149", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1454, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "fd9FT8NimMRpgQf7CbnHH7", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1455, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "ZKx4oaPKLUrjSsbZEugFjt", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1456, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u6b63\u5e38\u7684?\n<image>", "pred_response": "\u5de6", "answer_id": "SBLZ53BavHVYi8WwEWPtQf", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1457, "prompt": "\u80bf\u5757\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0a\u90e8", "answer_id": "KAbZDNQidPcwF65RtGWh9p", "gt_response": "\u53f3\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1458, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "7zPAVVevvZmRMBwR34tKVB", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1459, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "TSqoXrARmRAdVo5cF2tFEF", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1460, "prompt": "\u56fe\u7247\u4e2d\u5305\u62ec\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "Z2TKUXH57Zz9cmFCFeQphC", "gt_response": "\u80ba\u80bf\u5757", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1461, "prompt": "\u80ba\u7684\u53f3\u4fa7\u662f\u4ec0\u4e48\u75c5?\n<image>", "pred_response": "\u80ba\u708e", "answer_id": "LwZmCDw9ovpvxhzfquRGaj", "gt_response": "\u80ba\u80bf\u5757", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1462, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80bf\u5757\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "RcNEJyPB9ZAjG2zpaWj8KJ", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1463, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "D3WHoKDe7xXxZ4acV67V7m", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1464, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "hBtJytfFBtrzvoaqzcs6Cp", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1465, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "fi7TbxWkHkohviLqWpdhPh", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1466, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u547c\u5438\u56f0\u96be", "answer_id": "chVKypDrmWizBJAUtVQtXc", "gt_response": "\u547c\u5438\u56f0\u96be,\u54b3\u8840,\u80f8\u75db", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1467, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "SxvuRS57ENMVYbiTcjvtVm", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1468, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "CT", "answer_id": "VAuXBY3iRSeseNGnRTqFYV", "gt_response": "X\u5149", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1469, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "gVmEFJdDuXrExRvp2ctm9Q", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1470, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u80f8\u8154?\n<image>", "pred_response": "\u662f", "answer_id": "45tNaiPvuBDSjnFtrPVeeo", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1471, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u53f3\u4fa7", "answer_id": "Wjd7WEw9ThfHjPxXsNJgs4", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1472, "prompt": "\u56fe\u4e2d\u80bf\u5757\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0a", "answer_id": "hmnPNCGQG3Y2RvqtqcGqPU", "gt_response": "\u53f3\u4e0a\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1473, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "RTRGdnokzDU9YFBysfmS7J", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1474, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "ep8xzGyoAKCQxm4sS2tVHW", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1475, "prompt": "\u80ba\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "2MY5B8EvK2GcoD6yAZJPxJ", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1476, "prompt": "\u56fe\u4e2d\u80ba\u7684\u53f3\u4fa7\u662f\u4ec0\u4e48\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "9UZ7XishHPasP389sNW4B8", "gt_response": "\u80ba\u80bf\u5757", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1477, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80bf\u5757\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "g4HUUg7DT2vEcnNZcuU9ms", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1478, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u84dd\u8272", "answer_id": "7kW6usY2h2LSEkEYKXzDFa", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1479, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "Tg8y4AYdE9q5oUGLMbKxrs", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1480, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "QeqbdkKHNMf3xv82LQiKUt", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1481, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u80ba\u90e8\u53f3\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u90e8", "answer_id": "jFfdRpSpCT9JxQ65b7Sv3t", "gt_response": "\u836f\u7269\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1482, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "B9DetXcvTZpjwkr5CxXtWY", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1483, "prompt": "\u8fd9\u662fCT\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "FFoHMTWDxrGEKUiR4CTaXx", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1484, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "X4syGu5ZiWwPcb425nWBFU", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1485, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u80f8\u8154?\n<image>", "pred_response": "\u662f", "answer_id": "2potBxCxmjSJBkdcEedZrr", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1486, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "Nv9UZga6K99YAAoPoXeNK3", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1487, "prompt": "\u56fe\u4e2d\u80ba\u7ed3\u8282\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0a\u90e8", "answer_id": "JnKV9UjgSzaeQfE9KL35r7", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1488, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u9888\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "QqYyYywNvjGnVVsV4Ct6Hy", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1489, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "GQART2anxnGY6JjsANbHuu", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1490, "prompt": "\u56fe\u4e2d\u80ba\u7684\u53f3\u4fa7\u662f\u4ec0\u4e48\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "NFRzdRCktQytnaoFeVXcE7", "gt_response": "\u80ba\u7ed3\u8282", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1491, "prompt": "\u80ba\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "bk7AfQ9kHfjZ824NNJ8zWM", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1492, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u7ed3\u8282\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "VG9dWnErn9ZyDZSPwvRuu7", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1493, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u7070\u8272", "answer_id": "g9Lxqy9BAA6Kx5q3doGK2C", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1494, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "XR8nEH9E2pktzo99agbo4r", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1495, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u662f\u5f88\u5927\u7684\u4f24\u5bb3", "answer_id": "2VPc4JmBbssW5qMjV6Wssu", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1496, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u80ba\u90e8\u53f3\u4e0b\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u62a4\u7406", "answer_id": "2DkCuVmio85qewDHjhgWnh", "gt_response": "\u836f\u7269\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1497, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "VsESa7TtuXuhBR7ez5VxAy", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1498, "prompt": "\u8fd9\u662f\u6838\u78c1\u5171\u632f\u6210\u50cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "ThTNpCmbBCanBkEzjr4UpM", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1499, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "W7SnPG5t5BkrnLrYzzZS5a", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1500, "prompt": "\u4ece\u6574\u4e2a\u8eab\u4f53\u7684\u89d2\u5ea6\u770b,\u8fd9\u5e45\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "azcwszVTLJXJWRN9fVXJzq", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1501, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0b\u80f8\u90e8", "answer_id": "ijGdskv5mNN2UZouTQKepW", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1502, "prompt": "\u56fe\u4e2d\u80ba\u4e0d\u5f20\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "SSnbcbxqwfSyXhANspMSKX", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1503, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "PMiPtssqSM8yKLw2pSATuW", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1504, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "JM5mkczj3bdjAyQvJn47k8", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1505, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "gYJssdAo8aJivmnYkgV53s", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1506, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "MnUG8A4xm3G3kBSWofLdMu", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1507, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u4e0d\u5f20\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "9EBYedkt4CzuvaHM7GAhJk", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1508, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u7070\u8272", "answer_id": "bBWnXDcqXhaFYkB2aLVkPA", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1509, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "VzTwuWG4uvdk5WNtuSVVCD", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1510, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u662f\u5f88\u5927\u7684\u4f24\u5bb3", "answer_id": "Q7KnHYsiFFeeEx6bRcqxv8", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1511, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u80ba\u90e8\u5de6\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u90e8\u6076\u6027\u80bf\u7624", "answer_id": "iE4nqS7B9hPeqG6jrFvH8o", "gt_response": "\u7269\u7406\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1512, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "VNjBh8WDZMpsYJviZX4tkS", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1513, "prompt": "\u8fd9\u662fCT\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "hTZndEFDKggf3ByNknUFFP", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1514, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "b59E896Xnovx7vdHatNCXr", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1515, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "bQAPn7pdsKxzuzoG6xVRAh", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1516, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "KDQPXiWXmVSi5Pn3rkzDZr", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1517, "prompt": "\u6d78\u6da6\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0a", "answer_id": "2rzvhyxxg6yh7jCXWxe4XT", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1518, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "6E8fnSnsT63bD5mYpKP2wD", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1519, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "QbZdGiDUAw62NFtEYL8Q46", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1520, "prompt": "\u75c5\u4eba\u7684\u5de6\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "2unQqHXcuPMEXEa25aAtQC", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1521, "prompt": "\u56fe\u7247\u4e2d\u5305\u62ec\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "HdbghZ9Ck2V79JDo83kCKv", "gt_response": "\u80ba\u6d78\u6da6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1522, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u6d78\u6da6\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "2xw2CEaJk6ur6QapU6tSxq", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1523, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u7070\u8272", "answer_id": "KVgb5RhHP4njryGGKjhNbU", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1524, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "M3J3xuLC3RSpSRneptXk8r", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1525, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "ejVfx7mUWBd8MjHAUKyCSo", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1526, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u80ba\u90e8\u5de6\u4e0b\u89d2\u7684\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u90e8\u6c27\u5316\u7f3a\u9677", "answer_id": "TvPDe8EorehCqq8UmpJkc6", "gt_response": "\u836f\u7269\u6cbb\u7597", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1527, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "VtRkhDjRNNGNvDg47oaRFF", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1528, "prompt": "\u8fd9\u662fCT\u626b\u63cf\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "mji2CB3odgCugeHgrEjDeu", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1529, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "GBDXaiNJMphYpWZcQ2jr9P", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1530, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "iemPwUp59MP7KHZmuwRYGq", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1531, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u4e0d\u6b63\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4e0a\u90e8", "answer_id": "DqDKLDAMLWSvvSubUTVDRa", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1532, "prompt": "\u56fe\u4e2d\u7ed3\u8282\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0a\u90e8", "answer_id": "AVBTpr3jPoD5P5QSjbMt4f", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1533, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "UFNisKkNyd5DmrL7A9gzcj", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1534, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "4u9oZuNqmBDQVQta8489F9", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1535, "prompt": "\u75c5\u4eba\u7684\u53f3\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "YXnPBm9cnVwQ3uXqvE4rJM", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1536, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "QeuVoTxvb2XzcXHms92ohp", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1537, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u7ed3\u8282\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "3roNzmEytZhK6UBFVUeM8Y", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1538, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "gHS3LiFWxdGJGw6yDHcgwg", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1539, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "mMeUmXmAw5XTieiaggxTWA", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1540, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u9ed1\u8272\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u68c0\u6d4b", "answer_id": "TKUMnmwU72mWXDoX3YtVJ9", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1541, "prompt": "\u56fe\u4e2d\u80ba\u90e8\u53f3\u4e0b\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba\u90e8", "answer_id": "ftb3U9o6WNdR2N8y3KxjxX", "gt_response": "\u80ba\u90e8\u611f\u67d3,\u6162\u6027\u523a\u6fc0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1542, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "MMBgCUd6wMSYzDQ9NV55aq", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1543, "prompt": "\u8fd9\u662fX\u5149\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "fhqYrxegK57HZvzLQ9TJqh", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1544, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "dnLt2XeJ9j8GcPHuyMiGug", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1545, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "WWjhMXBJSdWtZvxfzTmvZP", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1546, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "VB7bPvgdZ3xnWU4umSTMcg", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1547, "prompt": "\u56fe\u4e2d\u6d78\u6da6\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "Ji8UnztnBucB6Hzewzci3p", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1548, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "ZkeDtiyPXMfJcmJWhKsXKW", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1549, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "RQpm2GSVBVvMwevyqaF4q4", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1550, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "aU9YiHeRByzDtdso8bx54z", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1551, "prompt": "\u75c5\u4eba\u6709\u4efb\u4f55\u7684\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "GEXa9dNXDQQtwmn6JPWATM", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1552, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u6d78\u6da6\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "RVRaXtTyk5k3igZToR2eaY", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1553, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "DXd3assoDiP2Ye7Nhb875K", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1554, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "kZt4hZyxda9kPcQeZXdhSX", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1555, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "QPB3UuTQf9bphpv3SRQgYU", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1556, "prompt": "\u5982\u4f55\u9884\u9632\u80ba\u90e8\u5de6\u4fa7\u75c5\u53d8\u7684\u53d1\u751f?\n<image>", "pred_response": "\u907f\u514d\u5438\u5165\u6709\u5bb3\u7269\u8d28", "answer_id": "kyveqLRJNjCJoNKQKrVoPx", "gt_response": "\u5065\u5eb7\u751f\u6d3b,\u4fdd\u6301\u8eab\u4f53\u5065\u5eb7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1557, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "YrmTqkDxHPvueFxo4iGzCY", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1558, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7\u6838\u78c1\u5171\u632f\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "jUUuAkiYqPkzjEPdhYwNFi", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1559, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "PaozKTML7RGmLAKFFcwEm3", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1560, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "PbKcbceWpFChD4sM96xnng", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1561, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "cvSAyv2sM9nnBvuN3mEYHJ", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1562, "prompt": "\u56fe\u4e2d\u7ed3\u8282\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0a\u90e8", "answer_id": "Qe5tdYBBEBsw78Eig9Hstv", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1563, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "dCCMUqwXMiXSUUHB2tMYGx", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1564, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "6Mmtrh5zenpkJdScSQ3tdb", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1565, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "DuvMdZHcdMDnmQhTUeENqB", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1566, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "ac3mEp5ttAUMqNmPWhgUdn", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1567, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u7ed3\u8282\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Zc2WWfmGRbDtWmgCvzbidh", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1568, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "fQ9MKHLk7AP6p9QHHe3Aii", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1569, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "giSvV32mXK33txvU2XFzNx", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1570, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u9ed1\u8272\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u68c0\u6d4b", "answer_id": "Ly5VdWHw8vyZUVvp36Gy2M", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1571, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u547c\u5438\u56f0\u96be", "answer_id": "BiQCypwggNZP44FW9apTML", "gt_response": "\u80f8\u95f7,\u4e4f\u529b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1572, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "fqbxoumdhS8gCeow65NwQj", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1573, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "4j65bVzH3iEsYUUiQEa4FF", "gt_response": "X\u5149", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1574, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "fwC3Zk7PrzQYNR897zwg4A", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1575, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "8RhSBNDUJA4vfJcEPVqxyj", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1576, "prompt": "\u56fe\u4e2d\u5fc3\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "Te8wuHypVLfwQRJDcg69hk", "gt_response": "\u4e2d\u5fc3", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1577, "prompt": "\u5fc3\u810f\u80a5\u5927\u4f4d\u4e8e\u80f8\u8154\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6", "answer_id": "5CQGTeUbNKjyu5wrmSvetD", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1578, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Uz9tj7thZCSnoVrVF8oD2v", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1579, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "kGLSgu8LCzdo7Nu7fzUnLy", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1580, "prompt": "\u80ba\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "QcbFhFowkkWkaMUchD3j5R", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1581, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "cVV9ZH9Uvzui7wqhwps9Cd", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1582, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80bf\u5757\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "kJhHp8HZngCrXAdEwJmMvi", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1583, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "WkeKn48nsRrToF88ezrDRr", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1584, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "n7i2V2G5jsNKdEz9johSX9", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1585, "prompt": "\u56fe\u7247\u4e2d\u5fc3\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "UXwBxBbus65W7nax8MsdGc", "gt_response": "\u63a8\u52a8\u8840\u6db2\u6d41\u52a8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1586, "prompt": "\u56fe\u4e2d\u5fc3\u810f\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "UDztkdXWZ4mFrar4naag7C", "gt_response": "\u9ad8\u8840\u538b,\u6269\u5f20\u6027\u5fc3\u808c\u75c5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1587, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "krD3KKoPqtgtf7WkiRKuGM", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1588, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "CT", "answer_id": "CvsayMZFuqtddJT5iwhQgQ", "gt_response": "X\u5149", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1589, "prompt": "\u8fd9\u662f\u6a2a\u65ad\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "jDtzMwK7AHac3LSwcCSWWw", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1590, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "HX5rmCGfaFKc9JmFywmB8T", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1591, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "fW7qWDDBzyM8fQR5XGYNSa", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1592, "prompt": "\u6c14\u80f8\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0a", "answer_id": "oHa4xb6ZBaV2gvsZQc6wmp", "gt_response": "\u5de6\u4e0a\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1593, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "7eP2YuRHq4hh7Y76cjLGeg", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1594, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u9888\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "U4fySstV7NZ4Wo73SojtHG", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1595, "prompt": "\u8fd9\u5f20\u56fe\u7247\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "dnNXRa9nrB5ags6dCHsDdw", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1596, "prompt": "\u56fe\u7247\u4e2d\u5305\u62ec\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "AR2LbCTQ95TVP93PgoeTPp", "gt_response": "\u6c14\u80f8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1597, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u6c14\u80f8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "LwQbZQQ2VxzLuQT9zH79dX", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1598, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u7070\u8272", "answer_id": "mgCFms9ob6LtV2eF8JUB7e", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1599, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "TruJbX9ar4zvkY2Vtms3nL", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1600, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u9ed1\u8272\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u68c0\u6d4b", "answer_id": "44Jz4fnwG8DpswgNjy2s95", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1601, "prompt": "\u5982\u4f55\u9884\u9632\u80ba\u90e8\u5de6\u4e0a\u53f6\u75c5\u53d8\u7684\u53d1\u751f?\n<image>", "pred_response": "\u907f\u514d\u5438\u5165\u6709\u5bb3\u7269\u8d28", "answer_id": "S7xhAwYyXUuhG3hMnjexpd", "gt_response": "\u5fcc\u70df,\u907f\u514d\u5267\u70c8\u8fd0\u52a8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1602, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "CmLRMisovY3egHwKo7uW4R", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1603, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "c7afTeaNVU2shTFtw8PDaz", "gt_response": "X\u5149", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1604, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "PnQfmHzhYDbXBWHGPvFwky", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1605, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "Fw7oyXzkhPz3HQvFLydKGj", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1606, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "Mh9g3YXwyTpLg4fD5H9Jmp", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1607, "prompt": "\u56fe\u4e2d\u80ba\u708e\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4e0a\u90e8", "answer_id": "Lr9RVDQ2vZPZgxW62TD3ag", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1608, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "bG2Qd2HdbkuQpWYG6C5fKM", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1609, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u8179\u90e8\u4e48?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "AjWAw25zaDgKrUfGF9LmnF", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1610, "prompt": "\u75c5\u4eba\u7684\u5de6\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "QVYxNyaaVFqQajbP6JLsaZ", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1611, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "VAhbmkKbqopyTk554zSmjn", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1612, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u708e\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "QidvdPvgbqnVbkuKdvbLEX", "gt_response": "\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1613, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u7070\u8272", "answer_id": "bUC8KWiepehuD6SZ44ja4A", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1614, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u5668\u5b98?\n<image>", "pred_response": "12", "answer_id": "gKfoxxvALJKBKGjzaDZyF5", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1615, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u5fc3\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "UsWbYMkYSxkbyPT4PaooZ3", "gt_response": "\u63a8\u52a8\u8840\u6db2\u6d41\u52a8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1616, "prompt": "\u5982\u4f55\u9884\u9632\u80ba\u90e8\u5de6\u4e0b\u53f6\u75c5\u53d8\u7684\u53d1\u751f?\n<image>", "pred_response": "\u4e0d\u5438\u70df", "answer_id": "5KLePvsvut643TZeTo4DHB", "gt_response": "\u9632\u5bd2\u4fdd\u6696,\u589e\u5f3a\u4f53\u8d28", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1617, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "Eud9Gt3SPgfT6PkVdAZbQD", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1618, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "gV22zfhWBMp9zcq9PBPj3s", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1619, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "LjCNSEPsFHcxihYby5Twz5", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1620, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "hzRoet68bWK9N8JJKCf8YZ", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1621, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "E3Y6mRrrgY38kEZ6976NJP", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1622, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "2RAXNNKjHDds4AfCAZMKbp", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1623, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "fXmgvF2SwiY6TEzThEgm2C", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1624, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "2CGf2TEa3hQvfQdfD8vL2x", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1625, "prompt": "\u813e\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "NHgoriDp4v9YevfYiUV5dv", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1626, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7X\u5149\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "iteVsdwds8pVWST2NGZBaj", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1627, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Srd6oTKdh2TrpQxW3jyC43", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1628, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "eqdDMfhUrgWvrNX7yLjEaF", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1629, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "PGYxL7XfTnt3S9p74S7WPg", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1630, "prompt": "\u56fe\u4e2d\u80ba\u708e\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "PByG2cVwPbW47WWbChabon", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1631, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "9wcMvTwJuDMMxhSJ7RnDdR", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1632, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "5ZLQ5mzrpDQ8mfTvReXnBM", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1633, "prompt": "\u75c5\u4eba\u7684\u5de6\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "fBpGGJ4cLL8Nk2kJweFtdw", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1634, "prompt": "\u56fe\u7247\u4e2d\u5305\u62ec\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80f8\u8154\u708e", "answer_id": "PY9Mo94BXXKNoUa3YC3wp8", "gt_response": "\u80ba\u708e", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1635, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u708e\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "czRYE44LSwmMaKbkvEgwDv", "gt_response": "\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1636, "prompt": "\u56fe\u4e2d\u7684\u80ba\u662f\u4ec0\u4e48\u989c\u8272\u7684?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "KmxuLcX2vQVgquF9hkn5ne", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1637, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "HhxwoofYVdB54HsVthSD95", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1638, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u9ed1\u8272\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u68c0\u6d4b", "answer_id": "KwVzYXK8VR4NKQ6pXC9BgP", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1639, "prompt": "\u56fe\u4e2d\u53f3\u4e0b\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u547c\u5438\u56f0\u96be", "answer_id": "d3A9MrxSf4C4EZtr6LcdMw", "gt_response": "\u54b3\u6ce1\u6cab\u9ecf\u6db2\u75f0,\u547c\u5438\u56f0\u96be,\u54b3\u55fd,\u80f8\u75db", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1640, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "2Sk8kv3NXpxcuHfvEnP628", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1641, "prompt": "\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u88ab\u8fd9\u5f20\u56fe\u7247\u4f7f\u7528?\n<image>", "pred_response": "CT", "answer_id": "RkoocbocZRWV2V5mWfouKF", "gt_response": "X\u5149", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1642, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "TSeqyFxjp6iNYExXGb2zMv", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1643, "prompt": "\u56fe\u50cf\u5728\u8eab\u4f53\u91cc\u4ee3\u8868\u4ec0\u4e48?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "jeUcrsm5s5m3bth6hjoeTL", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1644, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u53f3\u4fa7", "answer_id": "kRFCqHNVgHMUfQtwJZ4gjd", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1645, "prompt": "\u56fe\u4e2d\u80ba\u708e\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0a\u90e8", "answer_id": "RYH6u4ki4ptQUYstuE9rNU", "gt_response": "\u53f3\u4e0a\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1646, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "hBTqrj3J7pfbPm9VJSwGib", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1647, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "8qmz9X55uYyBeRMRiJsNVp", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1648, "prompt": "\u75c5\u4eba\u7684\u53f3\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "C2gsvoJrPMSUes3TzMPQms", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1649, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "SYrFPjk3cUJEwEavXEUUza", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1650, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u708e\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Yz8u4PeTKdZVyd6avtkhQD", "gt_response": "\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1651, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u7070\u8272", "answer_id": "6vUnhyYPhs3QQm9zZqxPWR", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1652, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u5fc3\u810f?\n<image>", "pred_response": "2", "answer_id": "56QG5TEs3TaHJnrPKcxkK5", "gt_response": "1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1653, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "LH5cqiFCnFxhSwQNH9Lkpa", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1654, "prompt": "\u56fe\u4e2d\u53f3\u4e0a\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u80ba\u708e", "answer_id": "QGUhmeYa7J9jMCARJwzosB", "gt_response": "\u54b3\u6ce1\u6cab\u9ecf\u6db2\u75f0,\u547c\u5438\u56f0\u96be,\u54b3\u55fd,\u80f8\u75db", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1655, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "L6hK3WQRudBpchyJF9LWfn", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1656, "prompt": "\u8fd9\u662f\u6838\u78c1\u5171\u632f\u626b\u63cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "jJhgWJCnQ3NuSNJDqtUsiu", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1657, "prompt": "\u8fd9\u662f\u6a2a\u65ad\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "FMqMJQtx7JNAomHLV6Ur5B", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1658, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "btw99kzwQKZcMvFdVj3phf", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1659, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "gd3VdsxY4TueYAW3GxSYqi", "gt_response": "\u53cc\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1660, "prompt": "\u56fe\u4e2d\u7ed3\u8282\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0a", "answer_id": "jGNwFTYATUyVebXAweTnU5", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1661, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "bmRnEWsTDCwEPP9Z3PLLP6", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1662, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "4mXbjrYr7Hcnfs2BfEpz8G", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1663, "prompt": "\u80ba\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "hegE6va9L2Bhi4ikgqjcT7", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1664, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "KbKGZLujy2vW35vDyPocJG", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1665, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u7ed3\u8282\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "gE7dk2f4VyL7aj8jYVkK9L", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1666, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "PG49hdesNKDzq6mtwve763", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1667, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u5668\u5b98?\n<image>", "pred_response": "12", "answer_id": "HPgyJ8j6tb2oxSwDbpr89m", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1668, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "hJhJAQKXasNvP5C7SUkFZh", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1669, "prompt": "\u56fe\u4e2d\u80ba\u90e8\u5de6\u4e0b\u89d2\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba\u90e8\u6162\u6027\u75be\u75c5", "answer_id": "C5uB37g67mFj6bUGiqgQCT", "gt_response": "\u80ba\u90e8\u611f\u67d3,\u6162\u6027\u523a\u6fc0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1670, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "YmRBU4AUc7k2hipJmjAURa", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1671, "prompt": "\u8fd9\u662f\u6838\u78c1\u5171\u632f\u626b\u63cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "gqiVaweVJb2LJZ9qvqbmkN", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1672, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "STz34kssNrcvGFbD5XUFUm", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1673, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "c9UNWARsz3yDE7XzhTgehk", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1674, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "AVim22Jdtvb5P5F9U64NWB", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1675, "prompt": "\u56fe\u4e2d\u80ba\u4e0d\u5f20\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "buiLgEH2EiLciZr8xMdvMA", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1676, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u8179\u90e8\u4e48?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "4QoXovmbRQ8BXobAkdSj2N", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1677, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "Uvo4nNM3SGhzJLWzQ7XEMw", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1678, "prompt": "\u80ba\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "ezhtxkwrGLiG8wVJ3RVUS3", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1679, "prompt": "\u75c5\u4eba\u7684\u53f3\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "jNbEEuXT3xGZA5D9XjaEkc", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1680, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u4e0d\u5f20\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "cSKGE6kNBhMdU59ZsvhhCr", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1681, "prompt": "\u56fe\u4e2d\u7684\u80ba\u662f\u4ec0\u4e48\u989c\u8272\u7684?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "EBRLWBF7uwTuyxMRyE4W9n", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1682, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u5fc3\u810f?\n<image>", "pred_response": "2", "answer_id": "VxLJiNqjwfM5uBpNkzDDbG", "gt_response": "1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1683, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "dirUZ2buZMHEevY9n4LuBZ", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1684, "prompt": "\u5982\u4f55\u9884\u9632\u80ba\u90e8\u53f3\u4e0b\u53f6\u75c5\u53d8\u7684\u53d1\u751f?\n<image>", "pred_response": "\u907f\u514d\u5438\u5165\u6709\u5bb3\u7269\u8d28", "answer_id": "UcTUWTqMM2WSVjT9KwNdSA", "gt_response": "\u5fcc\u70df,\u589e\u5f3a\u4f53\u8d28", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1685, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "i6AuQ4T5uUxcx5uBwSr3qe", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1686, "prompt": "\u8fd9\u662fX\u5149\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "CbEWvjDtFWjRy2YDdDcyGd", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1687, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "VQzHPnMghScCfcSJEagp3K", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1688, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "FTgUZZSBNdbHmgD3ABuLvm", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1689, "prompt": "\u56fe\u4e2d\u80f8\u8154\u54ea\u4e00\u90e8\u5206\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u53f3\u4fa7", "answer_id": "6srzbeCMebZwgZKp3pbwhd", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1690, "prompt": "\u5fc3\u810f\u80a5\u5927\u4f4d\u4e8e\u80f8\u8154\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6", "answer_id": "PznVLrAo6uwbEeyZMdQMBF", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1691, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u9888\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Rfj4viZ8YaVdqSyxkyTBa2", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1692, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "5p3yTyuDk4z8dKFR6tiLAe", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1693, "prompt": "\u56fe\u4e2d\u80f8\u8154\u5b58\u5728\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "eecUAK7wYaUuyG8Cbn2qKJ", "gt_response": "\u5fc3\u810f\u80a5\u5927", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1694, "prompt": "\u56fe\u4e2d\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "CNesyWMV6jnKsFjs7Mpsd8", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1695, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53ef\u4ee5\u89c2\u5bdf\u5230\u5fc3\u810f\u80a5\u5927\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "jYmzFZ9TeCYTvAHXcfpebo", "gt_response": "\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1696, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "mLfqQ8A4VkETuLoaxxVYgm", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1697, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u5fc3\u810f?\n<image>", "pred_response": "2", "answer_id": "BGLmiesxTViFE8sLBDorz6", "gt_response": "1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1698, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u7684\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "MKpkXNYEUUSJZ8XXxT8K8C", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1699, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u5fc3\u810f\u7684\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "NER3cGNLaNJohSqiJRQazq", "gt_response": "\u7269\u7406\u6cbb\u7597,\u836f\u7269\u6cbb\u7597", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1700, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "kjiGC3wSM8B9wn24BwdgdS", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1701, "prompt": "\u8fd9\u5f20\u7167\u7247\u662f\u600e\u4e48\u62cd\u7684?\n<image>", "pred_response": "CT", "answer_id": "TgnErib67SPgPPKrobvP58", "gt_response": "X\u5149", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1702, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "P5SbUTT2vsBJu7emUmxcwz", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1703, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u80f8\u8154?\n<image>", "pred_response": "\u662f", "answer_id": "RxSnRJvZYHHjVA3LMi6Qvo", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1704, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u53f3", "answer_id": "HUgfu3jJoHtEFDBzRM9dGc", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1705, "prompt": "\u56fe\u4e2d\u79ef\u6db2\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0a\u90e8", "answer_id": "T6jyABWUmhzaq4dkd72RJU", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1706, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "StWnpVJPLMWwcdPBptTw5r", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1707, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "Z9svmGUPZkZ6Rsi3vB6Zzk", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1708, "prompt": "\u56fe\u7247\u4e2d\u5305\u62ec\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "Tgma6m6PuPQCaKZz6s9uAi", "gt_response": "\u80f8\u8154\u79ef\u6db2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1709, "prompt": "\u56fe\u4e2d\u80ba\u7684\u53f3\u4e0b\u4fa7\u662f\u4ec0\u4e48\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "cpUr4xC9VcxkUnGmrT6ve3", "gt_response": "\u80f8\u8154\u79ef\u6db2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1710, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u79ef\u6db2\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "ih8oKMuxP7HkQwR9yiDUY6", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1711, "prompt": "\u56fe\u4e2d\u7684\u80ba\u662f\u4ec0\u4e48\u989c\u8272\u7684?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "6BGCAUfRuSCeEB5nzagkYU", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1712, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u5668\u5b98?\n<image>", "pred_response": "12", "answer_id": "jronQzNnCmYYPVPbGeo48o", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1713, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u9ed1\u8272\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u68c0\u67e5", "answer_id": "dV7VFCvyd7JM8jGP5sPT35", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1714, "prompt": "\u56fe\u4e2d\u53f3\u4e0b\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u80ba\u708e", "answer_id": "Xdrwm2Zw38kU2mrxVorp2n", "gt_response": "\u80f8\u95f7,\u547c\u5438\u56f0\u96be,\u80f8\u75db", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1715, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "Ldmv4awHJ2HSSWquBiz7xF", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1716, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "QDCQQsPGbgaHd5wRCQFEih", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1717, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "DC9pkJqkep7k6jdXiifdUY", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1718, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "VXg9dKKHEmnzs6K3TVWmCt", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1719, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "L7AdSf5HLBQwo7pCgDZbu6", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1720, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "Vsp532yxjrKQ9CLHLHH2UR", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1721, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "fUA3j4E8NKNpJHjrjS4Pyu", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1722, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "hb56nDZt7EBrJKsZg5oNhX", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1723, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "hFeZDg4UB4VKfhf6iifY6W", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1724, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "BEtHitFxrkX4skeYswxtNV", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1725, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "RBcBcZ3WKMEqxCbAU4MDuU", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1726, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "QTU84V4erQwe8DxVpwTjmu", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1727, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "3PPtVRrDNtCRyfJHkktxeu", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1728, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "gZ2HXrWtZKR8Eoq8d8c3Cc", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1729, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "Thb38YARRPcSAxv4qCrd3a", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1730, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "m53FxT7Ahd59LKA9oxdknJ", "gt_response": "\u5065\u5eb7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1731, "prompt": "\u80be\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "7LiofvwZk9LCqdibkRPWvG", "gt_response": "\u5065\u5eb7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1732, "prompt": "\u8fd9\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u533b\u5b66\u56fe\u50cf?\n<image>", "pred_response": "\u6838\u78c1\u5171\u632f", "answer_id": "eWrrwCz4yXrpfsnBFLE5cy", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1733, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "AFqXYWPUBCyk66jUhdnUut", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1734, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u9888\u90e8?\n<image>", "pred_response": "\u662f", "answer_id": "UUmfXqxnR5WqovZanUbQtj", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1735, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u662f\u5589?\n<image>", "pred_response": "\u5de6", "answer_id": "fYfQyeizWJ3BKV89ntGM67", "gt_response": "\u9ed1\u8272\u7a7a\u6d1e", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1736, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "Da52yYNNkXj7sUm9sfGSzm", "gt_response": "\u9888\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1737, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u9888\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "V9UKayecRvt2vY9ajfkBsd", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1738, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u98df\u9053\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "mrzFmpgZqT3HSXnkBPvEPd", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1739, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "ZYioSbGegsRe8V8kLmSbus", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1740, "prompt": "\u56fe\u4e2d\u5589\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "hxVVFS6nnuG2kCd6h4k6QP", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1741, "prompt": "\u56fe\u4e2d\u5589\u662f\u4ec0\u4e48\u5f62\u72b6\u7684?\n<image>", "pred_response": "\u5706\u5f62", "answer_id": "bER3sVFD9uZ2utxACbLrLi", "gt_response": "\u692d\u5706", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1742, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u4e0a\u65b9\u7684\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "TADdBsQ8hGZq58zHDnqQzq", "gt_response": "\u901a\u6c14,\u53d1\u97f3", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1743, "prompt": "\u4f4d\u4e8e\u56fe\u50cf\u9876\u90e8\u7684\u5668\u5b98\u5c5e\u4e8e\u4ec0\u4e48\u7cfb\u7edf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "4sWBLZXhpsvm3PVJrnkTsB", "gt_response": "\u547c\u5438\u7cfb\u7edf,\u6d88\u5316\u7cfb\u7edf", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1744, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u795e\u7ecf\u7cfb\u7edf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "dj5wXFHZgWgXA55RwQUBoo", "gt_response": "\u810a\u9ad3", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1745, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "KjcN5vhZvdJBDeK2MJEFRn", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1746, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u5206\u6ccc\u9176\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "TUV5vxgmzUgBtSsADnFW6s", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1747, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "LF3fsi4etzoyeEKvLrGpzF", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1748, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "c3BEoh99F5ba6JRkRtjZt6", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1749, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "GhCwZqCNmsrZVBWhrrXAFu", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1750, "prompt": "\u4ec0\u4e48\u5668\u5b98\u6807\u5fd7\u7740\u5668\u5b98\u7cfb\u7edf\u7684\u4e0b\u7aef?\n<image>", "pred_response": "\u8111", "answer_id": "4FGKkQhwXyMDFCFMEzkZxv", "gt_response": "\u4e0b\u988c\u9aa8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1751, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "7hyc6o9a5hKopvhW9juzMv", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1752, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u5c0f\u80a0\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "WCWMXZn9VNMrq8oSmqiamV", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1753, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u662f\u5426\u5305\u542b\u8180\u80f1?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "E5Yb7z9AvKPsHW4iBpBfHU", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1754, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "3d5Abwz8XBPsr5WQ5swzXv", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1755, "prompt": "\u56fe\u4e2d\u6709\u6ca1\u6709\u4e00\u4e9b\u5668\u5b98\u63a7\u5236\u5fc3\u8df3\u548c\u547c\u5438?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "KF7ekbmXzcq74odGjBEWHf", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1756, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7CT\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "RMs5zS9NvqvKfLKULMpxz7", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1757, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "epwhBUoMe3V6fmucgmiiQc", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1758, "prompt": "\u4ece\u6574\u4e2a\u8eab\u4f53\u7684\u89d2\u5ea6\u770b,\u8fd9\u5e45\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "UuUo3XeUXnaxHuZ4AtoyA2", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1759, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "JDNqNZL3yTvKqGF7VvMaAt", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1760, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u9888\u90e8", "answer_id": "2zj7v47Uo46N6XakKdX8Wk", "gt_response": "\u816e\u817a", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1761, "prompt": "\u8fd9\u5e45\u56fe\u50cf\u91cc\u6709\u816e\u817a\u5417?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "CkLXBE3YsJu8kz32fpZhua", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1762, "prompt": "\u4e0b\u988c\u9aa8\u51fa\u73b0\u5728\u8fd9\u5f20\u56fe\u4e0a\u4e86\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "AkQTNE6kprGp3JhgTLfKUR", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1763, "prompt": "\u56fe\u7247\u4e2d\u7684\u4e0b\u988c\u9aa8\u662f\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "9WdDq6NPsYCwyigCsT3MfQ", "gt_response": "\u767d\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1764, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "fpZRdpg4kJtvR3a9SebqmZ", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1765, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u795e\u7ecf\u7cfb\u7edf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "37ndMdCnC6qrPtJCGoUo6F", "gt_response": "\u810a\u9ad3", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1766, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u53ef\u4ee5\u6392\u6cc4\u673a\u4f53\u5e9f\u7269\u8d28\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "emEMpNk9pbkkWcmAiECZyt", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1767, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "FjixfMmeB5Z6wZ3Ysrzk33", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1768, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "MRI", "answer_id": "mfvSrLJx2H6Qz3AxFtGgjt", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1769, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "KMYTyU65z2nvAe9yQ39gxq", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1770, "prompt": "\u56fe\u50cf\u5728\u8eab\u4f53\u91cc\u4ee3\u8868\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "ZXxyVfQ7qNQZHpSgUYhq4P", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1771, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "ikHqHhApmCjkoJvA9udzkc", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1772, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "HvWSxP8khYyyoYZSEL4Ddz", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1773, "prompt": "\u8fd9\u5e45\u56fe\u50cf\u91cc\u6709\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "XjKYjtWZyJFsmXd5eRHVEC", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1774, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u76f4\u80a0\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "NKAfj4zgTHRr997fNLPGYW", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1775, "prompt": "\u810a\u9ad3\u5728\u56fe\u7247\u4e2d\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "CTzR2MtTqP4pWiq3geWyU3", "gt_response": "\u7070\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1776, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "MLnr8D7UBfP3zKRtSbs98Q", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1777, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u795e\u7ecf\u7cfb\u7edf?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "KRaWp42FEu3zUqLsKwBKSW", "gt_response": "\u810a\u9ad3", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1778, "prompt": "\u8fd9\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u533b\u5b66\u56fe\u50cf?\n<image>", "pred_response": "\u6838\u78c1\u5171\u632f", "answer_id": "iSUxvYZGLVzb84m3pNNt9G", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1779, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "7yK87uCw2aR7cXYCETJC6s", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1780, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u9888\u90e8?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "43v8qL27rhCNwT8A49ZaBu", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1781, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u662f\u6c14\u7ba1?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "6Ksu2KFhKxqSViAkYFWkVY", "gt_response": "\u9ed1\u8272\u7a7a\u6d1e", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1782, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u810a\u690e", "answer_id": "KUByQMWQxcRYneU8QEqukj", "gt_response": "\u9888\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1783, "prompt": "\u816e\u817a\u51fa\u73b0\u5728\u56fe\u50cf\u4e2d\u4e86\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "NP73ZzqRkwmpYeu7Lytgf7", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1784, "prompt": "\u8fd9\u5e45\u56fe\u50cf\u91cc\u6709\u6c14\u7ba1\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Bu8C6xmgqdi6rmd6ho4sfs", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1785, "prompt": "\u56fe\u4e2d\u98df\u9053\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u7070\u8272", "answer_id": "7d4UJjDTch5gMFQ77QCVG2", "gt_response": "\u7070\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1786, "prompt": "\u8fd9\u4e2a\u75c5\u4eba\u7684\u6c14\u7ba1\u662f\u4ec0\u4e48\u5f62\u72b6\u7684?\n<image>", "pred_response": "\u5706\u5f62", "answer_id": "N5M9Wk7wfNVyLGYCdCTFuy", "gt_response": "\u4e0d\u89c4\u5219", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1787, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u9876\u90e8\u7684\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "QTHHM685GgJrgs5Gr5FSVM", "gt_response": "\u8fd0\u9001\u6c14\u4f53", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1788, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u795e\u7ecf\u7cfb\u7edf?\n<image>", "pred_response": "\u8111", "answer_id": "gGJWi5XCumzidgvFD7igYb", "gt_response": "\u810a\u9ad3", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1789, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "MwbKJVKYR9SECwKEKYYbzC", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1790, "prompt": "\u56fe\u4e2d\u662f\u5426\u6709\u53ef\u4ee5\u611f\u77e5\u4e8b\u7269\u7684\u5668\u5b98?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "gHRmeXUzHtwp7JtekNpQkb", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1791, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7X\u5149\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "VxcskFBsVxcGMKDtx9h98D", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1792, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "JhSPaK98kqoJjiiQNBq6xd", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1793, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "G52BQRFspcEL4ct4b22uNk", "gt_response": "\u9888\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1794, "prompt": "\u56fe\u4e2d\u98df\u7ba1\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "jrFgKU4hq2RqXRGKerMv33", "gt_response": "\u6c14\u7ba1\u4e0b\u65b9", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1795, "prompt": "\u810a\u9ad3\u5728\u8fd9\u5f20\u56fe\u4e0a\u51fa\u73b0\u4e86\u5417?\n<image>", "pred_response": "\u4e0d\u51fa\u73b0", "answer_id": "bGL7XKQD2V2TFPJhn6DZrp", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1796, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u810a\u9ad3\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "mVrQ2eu5ZX39ShbWswKuiN", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1797, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u80ba\u5417?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "mLXjbffwPYsevi6bCdNdWJ", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1798, "prompt": "\u8fd9\u4e2a\u75c5\u4eba\u7684\u98df\u9053\u662f\u4ec0\u4e48\u5f62\u72b6\u7684?\n<image>", "pred_response": "\u5706\u5f62", "answer_id": "Wg74ArPR3bgd5nKSzGQnYc", "gt_response": "\u4e0d\u89c4\u5219", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1799, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u7684\u9ed1\u8272\u7a7a\u6d1e\u7ec4\u7ec7\u6709\u4ec0\u4e48\u529f\u80fd?\n<image>", "pred_response": "\u5438\u6536", "answer_id": "FUsTCDE3rHzBen7zADczuu", "gt_response": "\u98df\u7269\u8fd0\u9001", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1800, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "ms8v2LhyKqipiYaxbSPANE", "gt_response": "\u6c14\u7ba1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1801, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "EUJCyWj6FZxVyVkMmpc63T", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1802, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u53ef\u4ee5\u8fd0\u9001\u6c14\u4f53\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "YrzbzB9yEbDtY9HJ8P8ED3", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1803, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "DUaBUKwMCSpmGLBBihVqr9", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1804, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "dunVpJE3uQZ6jwyW6cKPYK", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1805, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "KjUnppDqok5Rdk628ewS3c", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1806, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "Ccz9pRojdkTF5KemJwevq7", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1807, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "ZEr9La6WeKGRFXggpPe58P", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1808, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u8111\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "J4pCVkPn5hi7iTV3VjU4GP", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1809, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "eEJXa8bWiHKvEdbNMvXkXh", "gt_response": "\u8111\u6c34\u80bf", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1810, "prompt": "\u5927\u8111\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "EmCFB4gqRqPUnQ3iE7uf7r", "gt_response": "\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1811, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u53f3\u8111\u5ba4", "answer_id": "DmzxphXnVh4fpurxCg4HBS", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1812, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "MRI", "answer_id": "oUpXK79fSHMwa6vqAWuzKT", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1813, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "GgxEmsXyuK8rcPUmPAQ5PG", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1814, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "U6zv4GvoTb92SpejkPJY23", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1815, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u9888\u90e8?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "gtMxDceuZ9d4u3pDtWa2BB", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1816, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0a\u90e8", "answer_id": "5Lzy3FHgL3pT7xi3CZ8Jip", "gt_response": "\u9876\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1817, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "KHXHBPTgX5byLj8fFnH7pL", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1818, "prompt": "\u56fe\u7247\u4e2d\u53ef\u4ee5\u89c2\u5bdf\u5230\u8111\u6c34\u80bf\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "SF2mgMrk7KkMD5vMfBYVet", "gt_response": "\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1819, "prompt": "\u5982\u4f55\u6cbb\u7597\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u9876\u90e8\u7684\u75be\u75c5?\n<image>", "pred_response": "\u6838\u78c1\u5171\u632f", "answer_id": "daZKeYWuGuJUusvovN8wuD", "gt_response": "\u836f\u7269\u6cbb\u7597,\u652f\u6301\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1820, "prompt": "\u8fd9\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u533b\u5b66\u56fe\u50cf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "23Kjxp8yLYBB4ovGGQpRmE", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1821, "prompt": "\u8fd9\u5f20\u6838\u78c1\u5171\u632f\u56fe\u50cf\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "L2fvxZHcWbNxWuAHD2qcma", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1822, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "hCKbeUf7ifLcr6awcV9vpZ", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1823, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u662f", "answer_id": "8AwCaUroMm9LeFa2Z2X5Aj", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1824, "prompt": "\u56fe\u4e2d\u8111\u6c34\u80bf\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "8vhWJFLNeditcjkDgurzWm", "gt_response": "\u5de6\u4e0a\u53f6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1825, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "KTmmqV2Eve42hMy5gbZhzK", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1826, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u8111\u6c34\u80bf", "answer_id": "KHzPP9JnT9wGLcHuKspRqZ", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1827, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "2", "answer_id": "cDjZyiZzHsoBNUV3gBfMCt", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1828, "prompt": "\u5982\u4f55\u9884\u9632\u56fe\u7247\u4e2d\u5927\u8111\u5de6\u4e0a\u53f6\u75be\u75c5\u7684\u53d1\u751f?\n<image>", "pred_response": "\u907f\u514d\u9152\u7cbe\u548c\u5438\u70df", "answer_id": "J2RZdHBfQpzSKVmNVbuXdC", "gt_response": "\u6ce8\u610f\u996e\u98df\u536b\u751f,\u52a0\u5f3a\u8eab\u4f53\u7d20\u8d28,\u907f\u514d\u8111\u90e8\u5916\u4f24", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1829, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "MRI", "answer_id": "ZmfQs3tXPy6nD3ZgDg8yHK", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1830, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528\u4ec0\u4e48\u6837\u7684\u6743\u91cd\u751f\u6210\u7684,t1\u8fd8\u662ft2?\n<image>", "pred_response": "T2", "answer_id": "JPB2PDHVUJoFfacJfiLS3w", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1831, "prompt": "\u8fd9\u662f\u6a2a\u65ad\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "fkehoQvskocZGVq6wSZpxF", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1832, "prompt": "\u56fe\u50cf\u5728\u8eab\u4f53\u91cc\u4ee3\u8868\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "RRaHihkfb46iduf2BPQmw8", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1833, "prompt": "\u56fe\u4e2d\u8111\u90e8\u589e\u5f3a\u80bf\u7624\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "n79bpxdzYq59w5sEtWk7qk", "gt_response": "\u5de6\u53f6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1834, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "JVymjTQK6dGHbXTmUVtjJp", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1835, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "F5fZSgCJmFayAPBD25SZL7", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1836, "prompt": "\u76f8\u5bf9\u4e8e\u5176\u4ed6\u7ec4\u7ec7,\u8111\u80bf\u7624\u662f\u767d\u8272\u7684\u8fd8\u662f\u7070\u8272\u7684?\n<image>", "pred_response": "\u7070\u8272\u7684", "answer_id": "Au43rvWT4dKePt629EfjfR", "gt_response": "\u767d\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1837, "prompt": "\u5982\u4f55\u6cbb\u7597\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u5de6\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u68c0\u67e5", "answer_id": "X56G7EKHdneDjQuXgq74Dy", "gt_response": "\u836f\u7269\u6cbb\u7597,\u652f\u6301\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1838, "prompt": "\u8fd9\u662f\u6838\u78c1\u5171\u632f\u626b\u63cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "fo8TzckZ3WjXVAZEmtX783", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1839, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "i5TVRCkDXgY9zRu4qeMcov", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1840, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "6u75uranoSMRiDTujnWEoq", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1841, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "A8yvWVKSwpsYo87NR7xJfz", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1842, "prompt": "\u56fe\u4e2d\u8111\u90e8\u589e\u5f3a\u80bf\u7624\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "YUTwwWQFciEDibyg4e26n7", "gt_response": "\u5de6\u53f6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1843, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "eTezpLMXzusiYiGvS7ihy3", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1844, "prompt": "\u5927\u8111\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "KrkmchcssXeo9zyZxWFULA", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1845, "prompt": "\u5982\u4f55\u6cbb\u7597\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u5de6\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u62a2\u6551", "answer_id": "ZEuCcswmnnwpCXBouuyUZF", "gt_response": "\u836f\u7269\u6cbb\u7597,\u652f\u6301\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1846, "prompt": "\u8fd9\u662fCT\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "ABt5hzWZ2uppU9pWKhkhwi", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1847, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "H5TkfsGGDKeSWcf9qTJESa", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1848, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "XBne2uXK472g8QPujqXKyj", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1849, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u662f", "answer_id": "b4M5KczfswWxfUZYyae7HD", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1850, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0a\u90e8", "answer_id": "GXJ5dFcc7kF6RV9WwVE2Jh", "gt_response": "\u5de6\u4e0a\u53f6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1851, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "V36zbUbUzPj5fn4WGfHjPb", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1852, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf\u7624", "answer_id": "ZsE6AHQ7KiLzckDrXQZ8d3", "gt_response": "\u8111\u6c34\u80bf", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1853, "prompt": "\u4ea7\u751f\u56fe\u4e2d\u5927\u8111\u5de6\u4e0a\u4fa7\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8111\u8840\u7ba1\u75be\u75c5", "answer_id": "7HxYwwnEP8jyRSknfwEZAh", "gt_response": "\u9885\u8111\u635f\u4f24,\u9885\u5185\u5360\u4f4d\u6027\u75c5\u53d8,\u9885\u5185\u708e\u75c7,\u8111\u8840\u7ba1\u75c5\u53d8,\u8111\u7f3a\u6c27", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1854, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "AfeNuekE5UZTEtRVMVpB3m", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1855, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528\u4ec0\u4e48\u6837\u7684\u6743\u91cd\u751f\u6210\u7684,t1\u8fd8\u662ft2?\n<image>", "pred_response": "T2", "answer_id": "J7M5TVPhbrkFjVpWXibHJM", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1856, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "JHUU3CoaViXdcVmEh7koH2", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1857, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "Yb6df4VuiHyS4a7GXnEPZs", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1858, "prompt": "\u56fe\u4e2d\u8111\u54ea\u4e2a\u53f6\u662f\u6b63\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u53f6", "answer_id": "gXYfrXKjZ6eGyXfepw6Vhz", "gt_response": "\u5de6\u53f6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1859, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "VujyVVvAorAgLMUFzGTaYQ", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1860, "prompt": "\u56fe\u4e2d\u8111\u90e8\u53f3\u4fa7\u663e\u793a\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf\u7624", "answer_id": "9F2iLDTKiVKHELztQ7jWRa", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1861, "prompt": "\u5982\u4f55\u6cbb\u7597\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u53f3\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u672f", "answer_id": "cRy5oNY3C7g4cFUPHJVTKU", "gt_response": "\u836f\u7269\u6cbb\u7597,\u652f\u6301\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1862, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "MRI", "answer_id": "dvbATm8eqLg4VQCtukQ9b2", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1863, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u8fd8\u662fT2\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf?\n<image>", "pred_response": "T2", "answer_id": "63EoHKHRQvYtk7PWpNfZCj", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1864, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "2jN2afsJeH4FVTFs3D6i3T", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1865, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u80f8\u8154?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "Uy3ibHf4ie66MDcceEZm66", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1866, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0a\u90e8", "answer_id": "EiN2sYHy65NR2zubiNgheV", "gt_response": "\u53f3\u53f6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1867, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "kVzLwTFjUhgQUq7YpSggFS", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1868, "prompt": "\u5927\u8111\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Lb6AaysedDUJbCZ9Jaf5yB", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1869, "prompt": "\u56fe\u7247\u4e2d\u5927\u8111\u53f3\u4fa7\u7684\u75be\u75c5\u4f1a\u7ed9\u60a3\u8005\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u5934\u75db\uff0c\u542c\u529b\u51cf\u9000\uff0c\u89c6\u529b\u4e0b\u964d", "answer_id": "VZdHFoFhqUzv7uiMgt32Jy", "gt_response": "\u89c6\u529b\u969c\u788d,\u5455\u5410,\u8033\u9e23,\u9885\u5185\u538b\u589e\u9ad8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1870, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u663e\u793a\u7684\u662f\u54ea\u79cd\u7c7b\u578b\u7684\u6210\u50cf,\u6838\u78c1\u5171\u632f\u3001CT\u8fd8\u662fX\u5149?\n<image>", "pred_response": "\u6838\u78c1\u5171\u632f", "answer_id": "TsGS65tnfBSyhi6bNpZZXd", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1871, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528\u4ec0\u4e48\u6837\u7684\u6743\u91cd\u751f\u6210\u7684,t1\u8fd8\u662ft2?\n<image>", "pred_response": "T2", "answer_id": "K6EednAhNgcQS9QKL9ZkXB", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1872, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "hbYLWoan4XF4UQpghUQ5VB", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1873, "prompt": "\u4ece\u6574\u4e2a\u8eab\u4f53\u7684\u89d2\u5ea6\u770b,\u8fd9\u5e45\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "R2T6WQWMsq6vcdnCGPds2E", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1874, "prompt": "\u56fe\u4e2d\u54ea\u4fa7\u7684\u8111\u534a\u7403\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "4AEhsWsHMLz4E4jPbYiDnL", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1875, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "BxpAivKmFymnhxAJ8WZgEZ", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1876, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf\u7624", "answer_id": "7zSac5p2wHNYvXErZaunbH", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1877, "prompt": "\u5982\u4f55\u9884\u9632\u56fe\u7247\u4e2d\u5927\u8111\u53f3\u4e0a\u53f6\u75be\u75c5\u7684\u53d1\u751f?\n<image>", "pred_response": "\u65e9\u671f\u8bca\u65ad", "answer_id": "FBUiXSQkCP6onwDtTTnL63", "gt_response": "\u6ce8\u610f\u996e\u98df\u536b\u751f,\u52a0\u5f3a\u8eab\u4f53\u7d20\u8d28,\u907f\u514d\u8111\u90e8\u5916\u4f24", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1878, "prompt": "\u8fd9\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u533b\u5b66\u56fe\u50cf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "72KHkoSdrx7Npp6xVMsmTU", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1879, "prompt": "\u8fd9\u5f20\u6838\u78c1\u5171\u632f\u56fe\u50cf\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "DACURYYWFVVedo9maDPTVF", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1880, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "jRChvYxwP4zuxKq2Fi6uLJ", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1881, "prompt": "\u4ece\u6574\u4e2a\u8eab\u4f53\u7684\u89d2\u5ea6\u770b,\u8fd9\u5e45\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "9ievETWnHVx6N7uPQ55HJN", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1882, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "L7QkrpGN3KhtCGfL8H6GAo", "gt_response": "\u53f3\u4e0a\u53f6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1883, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u76c6\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "LUYoxDzwaKwAPQu5EcxRxk", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1884, "prompt": "\u56fe\u4e2d\u8111\u90e8\u53f3\u4e0a\u4fa7\u663e\u793a\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u8111\u8840\u7ba1\u75be\u75c5", "answer_id": "CZkLaAktZrnG2EZ4jkb4DQ", "gt_response": "\u8111\u6c34\u80bf,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1885, "prompt": "\u56fe\u4e2d\u80bf\u7624\u662f\u589e\u5f3a\u6027\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "2RrF9AmwUhFMpu94n9ttGW", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1886, "prompt": "\u4ea7\u751f\u56fe\u4e2d\u5927\u8111\u53f3\u4e0a\u4fa7\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8111\u8840\u7ba1\u75be\u75c5", "answer_id": "PAvdrugan44GRTTdwZTdas", "gt_response": "\u8111\u80da\u80ce\u7ec4\u7ec7\u53d1\u80b2\u5f02\u5e38,\u9057\u4f20\u56e0\u7d20,\u5316\u5b66\u56e0\u7d20", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1887, "prompt": "\u8fd9\u5f20\u7167\u7247\u662f\u600e\u4e48\u62cd\u7684?\n<image>", "pred_response": "MRI", "answer_id": "KoMt2QapLf8NHB2u9caX3k", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1888, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528\u4ec0\u4e48\u6837\u7684\u6743\u91cd\u751f\u6210\u7684,t1\u8fd8\u662ft2?\n<image>", "pred_response": "T2", "answer_id": "MJ8TQEZh6StNtFjryrdZ8H", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1889, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "hZNUiwd7k88R9PHLvsxgaA", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1890, "prompt": "\u4ece\u6574\u4e2a\u8eab\u4f53\u7684\u89d2\u5ea6\u770b,\u8fd9\u5e45\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "jBDeCyKR5QQHDPGaVofyz3", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1891, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0a\u90e8", "answer_id": "evpak2VHaT3W8iQdURKbUv", "gt_response": "\u53f3\u53f6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1892, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "eFJnbTfXnbzim6QXTpR9kW", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1893, "prompt": "\u56fe\u4e2d\u8111\u90e8\u53f3\u4fa7\u663e\u793a\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf", "answer_id": "GKyPsS2VewxXasPbJYQNkK", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1894, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "2", "answer_id": "XpJnZEp3M9ur7wq26TKHv2", "gt_response": "3", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1895, "prompt": "\u4ea7\u751f\u56fe\u4e2d\u5927\u8111\u53f3\u4fa7\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8111\u8840\u7ba1\u75be\u75c5", "answer_id": "YWBrVmkuqVSCFXospFTYS5", "gt_response": "\u8111\u80da\u80ce\u7ec4\u7ec7\u53d1\u80b2\u5f02\u5e38,\u9057\u4f20\u56e0\u7d20,\u5316\u5b66\u56e0\u7d20", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1896, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7X\u5149\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "hDz9vfaZgr5mMb8ZUMZWhM", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1897, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528\u4ec0\u4e48\u6837\u7684\u6743\u91cd\u751f\u6210\u7684,t1\u8fd8\u662ft2?\n<image>", "pred_response": "T2", "answer_id": "m7PtfA5Q5WUuWewustATkb", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1898, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "oNMmmn4JA9LFQc5GrUjzE8", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1899, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "UakCr5z3SUzQ6V9YwZQth2", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1900, "prompt": "\u56fe\u4e2d\u8111\u54ea\u4e2a\u53f6\u662f\u6b63\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u53f6", "answer_id": "ckcSk5AhyRFEBERxnBywq4", "gt_response": "\u53f3\u53f6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1901, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "ACsMgXJUXFFDfAiG5wf7Ko", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1902, "prompt": "\u56fe\u4e2d\u8111\u90e8\u5de6\u4fa7\u663e\u793a\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf\u7624", "answer_id": "Wrwc8wqQNSXrFNrke4Pd7K", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1903, "prompt": "\u56fe\u4e2d\u8111\u6c34\u80bf\u662f\u4ec0\u4e48\u5bc6\u5ea6?\n<image>", "pred_response": "0.32", "answer_id": "mLpaziohwFbNoGZqQYfriD", "gt_response": "\u4f4e\u5bc6\u5ea6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1904, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u662f\u5f88\u5927\u7684\u4f24\u5bb3", "answer_id": "c4g23r6d26ATCgDHgfboMh", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1905, "prompt": "\u8fd9\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u533b\u5b66\u56fe\u50cf?\n<image>", "pred_response": "\u6838\u78c1\u5171\u632f", "answer_id": "iVBo72hDXw9ivaS5PxR3wb", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1906, "prompt": "\u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u6838\u78c1\u5171\u632f\u7684\u54ea\u79cd\u7c7b\u578b?\n<image>", "pred_response": "\u6838\u78c1\u5171\u632f", "answer_id": "Xh7uN3jFxu6A98GkzpKFz5", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1907, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "nRpzWKpqLvW7QN2mT4Ms35", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1908, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u8179\u90e8?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "dqF7dwNiakoGLtA7cBUoc6", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1909, "prompt": "\u56fe\u4e2d\u54ea\u4fa7\u7684\u8111\u53f6\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "Kz5a95GCrYEjjLxEEokfoU", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1910, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "7i2yLEKyPwryxNb5PbtJBe", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1911, "prompt": "\u56fe\u4e2d\u75c5\u4eba\u60a3\u6709\u8111\u7624\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "YXWBtESUPnZwpyQqZDxozE", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1912, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "2", "answer_id": "3iqHSgmLpUVp6srxUoH9KW", "gt_response": "3", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1913, "prompt": "\u5982\u4f55\u6cbb\u7597\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u5de6\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u672f", "answer_id": "etuTYAz9n2uUgMANC5zFe5", "gt_response": "\u836f\u7269\u6cbb\u7597,\u652f\u6301\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1914, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u663e\u793a\u7684\u662f\u54ea\u79cd\u7c7b\u578b\u7684\u6210\u50cf,\u6838\u78c1\u5171\u632f\u3001CT\u8fd8\u662fX\u5149?\n<image>", "pred_response": "\u6838\u78c1\u5171\u632f", "answer_id": "jx22MPRcEbnf8rHnkTkiGW", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1915, "prompt": "\u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u6838\u78c1\u5171\u632f\u7684\u54ea\u79cd\u7c7b\u578b?\n<image>", "pred_response": "T2", "answer_id": "mVeaKFYfZo95CqKriWGwmz", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1916, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "bQptLW9iznyP6veyRowbQN", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1917, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "ighFHxRtsBvPGNZuykEc7x", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1918, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u53f3\u4e0a\u89d2", "answer_id": "WujzXfeetuVZBt3s36xYCQ", "gt_response": "\u5de6\u4e0b\u53f6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1919, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "9NreZ3BkhDeoezHcDSjSBL", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1920, "prompt": "\u56fe\u4e2d\u8111\u90e8\u5de6\u4e0b\u4fa7\u663e\u793a\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u8111\u8840\u7ba1\u708e", "answer_id": "hPKhjMqDSb2A8r84CV6tCr", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1921, "prompt": "\u5982\u4f55\u9884\u9632\u56fe\u7247\u4e2d\u5927\u8111\u5de6\u4e0b\u53f6\u75be\u75c5\u7684\u53d1\u751f?\n<image>", "pred_response": "\u907f\u514d\u9152\u7cbe\u548c\u836f\u7269\u7684\u996e\u98df", "answer_id": "a5jkpzKE6uNUCAJ5sGNnDU", "gt_response": "\u6ce8\u610f\u996e\u98df\u536b\u751f,\u52a0\u5f3a\u8eab\u4f53\u7d20\u8d28,\u907f\u514d\u8111\u90e8\u5916\u4f24", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1922, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "MRI", "answer_id": "NccbMLKRohDVibNpRodFoV", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1923, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u8fd8\u662fT2\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf?\n<image>", "pred_response": "T2", "answer_id": "fbj4DwghT2CHgAVrKnAyPd", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1924, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "RwmqECzbk8epJddgcifpmK", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1925, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "DgTuaMDAWqaBiq7hCbXwgD", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1926, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "WzWffxHQxgHzYWcm3csiQd", "gt_response": "\u53f3\u53f6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1927, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "9PWiPMZLq7tNR5BxxcY5pc", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1928, "prompt": "\u60a3\u8005\u7684\u5de6\u8111\u53f6\u662f\u5426\u5f02\u5e38?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "aQqhAhEaJNa3o7QFNQJNTw", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1929, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u662f\u9ad8\u5bc6\u5ea6\u8fd8\u662f\u4f4e\u5bc6\u5ea6?\n<image>", "pred_response": "\u9ad8\u5bc6\u5ea6", "answer_id": "3LMwjxBuf8LRQhfNxXSbjZ", "gt_response": "\u4f4e\u5bc6\u5ea6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1930, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "2", "answer_id": "5VMtXPrY9m5mrQc4prC2i2", "gt_response": "2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1931, "prompt": "\u56fe\u7247\u4e2d\u5927\u8111\u5de6\u4fa7\u7684\u75be\u75c5\u4f1a\u7ed9\u60a3\u8005\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u5934\u75db", "answer_id": "dnJHcBMgDpGhUtmruQESYi", "gt_response": "\u89c6\u529b\u969c\u788d,\u5455\u5410,\u8033\u9e23,\u9885\u5185\u538b\u589e\u9ad8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1932, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7CT\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "NfDRnvSbNZwH6GvWmTZRnx", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1933, "prompt": "\u8fd9\u5f20\u6838\u78c1\u5171\u632f\u56fe\u50cf\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "3auiXRrUGB6oM44XfrS3Mx", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1934, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "fTLeSuYhHHLqdFWMtxiQbG", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1935, "prompt": "\u56fe\u50cf\u5728\u8eab\u4f53\u91cc\u4ee3\u8868\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "fYUoMBzrUBB73PrLT7iEQQ", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1936, "prompt": "\u56fe\u4e2d\u54ea\u4fa7\u7684\u8111\u53f6\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "6MSeLXZziKYPQskjkvv5Ci", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1937, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u76c6\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "7BypzDhK5LXSsjGFXS9ZjZ", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1938, "prompt": "\u56fe\u7247\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Q4kvqUtKedLWPbzAeBeBNQ", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1939, "prompt": "\u56fe\u4e2d\u80bf\u7624\u662f\u589e\u5f3a\u6027\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "mbVoBrhN5kCseobtw7ezRJ", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1940, "prompt": "\u5982\u4f55\u9884\u9632\u56fe\u7247\u4e2d\u5927\u8111\u53f3\u53f6\u75be\u75c5\u7684\u53d1\u751f?\n<image>", "pred_response": "\u907f\u514d\u9152\u7cbe\u548c\u836f\u7269\u7684\u996e\u98df\u548c\u4f7f\u7528", "answer_id": "DVeyvHb5fFLab6gX6rAsHv", "gt_response": "\u6ce8\u610f\u996e\u98df\u536b\u751f,\u52a0\u5f3a\u8eab\u4f53\u7d20\u8d28,\u907f\u514d\u8111\u90e8\u5916\u4f24", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1941, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "GoffyJCCpWUaZgsK7T7ZHu", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1942, "prompt": "\u8fd9\u5f20\u6838\u78c1\u5171\u632f\u56fe\u50cf\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "DbLxei7QqW8XJkQPbqVZMG", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1943, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "cMwdsm4fz3tq4LcXU9pEZh", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1944, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u80f8\u8154?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "MEkukH7skxBWdKas7cDbTt", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1945, "prompt": "\u56fe\u4e2d\u8111\u6c34\u80bf\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0a", "answer_id": "HRm7T4Q9gxLjeJ4U6GkXf9", "gt_response": "\u53f3\u4e0b\u53f6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1946, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "A6Gu9MAB3fjJd9XSEYsnBo", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1947, "prompt": "\u5927\u8111\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "m78uqor8sAwCYN6eXMdVNt", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1948, "prompt": "\u56fe\u7247\u4e2d\u5927\u8111\u53f3\u4e0b\u4fa7\u7684\u75be\u75c5\u4f1a\u7ed9\u60a3\u8005\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u5934\u75db", "answer_id": "dqdN9SpWt9y9ptHaJ7bbX4", "gt_response": "\u89c6\u529b\u969c\u788d,\u5455\u5410,\u8033\u9e23,\u9885\u5185\u538b\u589e\u9ad8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1949, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7X\u5149\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "FJ5GVBDBGZ3Jj7HNsLDKsQ", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1950, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u8fd8\u662fT2\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf?\n<image>", "pred_response": "T1", "answer_id": "5cf6RNyVp8DgLYuK8UbYLQ", "gt_response": "T1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1951, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "dFC37HKYxcXPLUbqsvhRb5", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1952, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "DeWmmmJtLof6UPaKyihYAJ", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1953, "prompt": "\u56fe\u4e2d\u8111\u6c34\u80bf\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0a\u90e8", "answer_id": "85LpZYwygNGEo9bZZgbPKr", "gt_response": "\u53f3\u4e0b\u53f6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1954, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "MvH9F3DDTVdiDWjSBVQscd", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1955, "prompt": "\u5927\u8111\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "kwBw9svTPqVZvemz335UHn", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1956, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u7684\u589e\u5f3a\u578b\u8111\u80bf\u7624\u662f\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "FeRtr2NmE694fFg6eVViKc", "gt_response": "\u767d\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1957, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "2", "answer_id": "YaqnQARgjkxphrwEewdoh2", "gt_response": "3", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1958, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "HyRbQkhAfjLscpXg7DpH5Q", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1959, "prompt": "\u8fd9\u5f20\u7167\u7247\u662f\u600e\u4e48\u62cd\u7684?\n<image>", "pred_response": "MRI", "answer_id": "kVbFdp2M9wy86L2rrLAco3", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1960, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "Q3Ry4mpDND5VmC4L4Lb69N", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1961, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "kzscgsVjTvfDxuGHBpNaKR", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1962, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u662f", "answer_id": "5porfE8kta6attzh4eUjeY", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1963, "prompt": "\u56fe\u4e2d\u8111\u90e8\u589e\u5f3a\u80bf\u7624\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "fqvtMJLVDJfFscvkE4VW6E", "gt_response": "\u53f3\u4e0a\u53f6", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1964, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "Ay72Hch65ee9KP9aQKcq9C", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1965, "prompt": "\u60a3\u8005\u7684\u5de6\u8111\u53f6\u662f\u5426\u5f02\u5e38?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "HBZdSHxsgz3GTN3g2LcYyH", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1966, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "2", "answer_id": "H2mD4BHEmAauMJvqjV6Dqe", "gt_response": "3", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1967, "prompt": "\u4ea7\u751f\u56fe\u4e2d\u5927\u8111\u53f3\u4e0a\u4fa7\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8111\u8840\u7ba1\u75be\u75c5", "answer_id": "ZWyrEnNKkhszsDqWz4qaxt", "gt_response": "\u8111\u80da\u80ce\u7ec4\u7ec7\u53d1\u80b2\u5f02\u5e38,\u9057\u4f20\u56e0\u7d20,\u5316\u5b66\u56e0\u7d20", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1968, "prompt": "\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u88ab\u8fd9\u5f20\u56fe\u7247\u4f7f\u7528?\n<image>", "pred_response": "MRI", "answer_id": "NsShunRWA4YfYkKwL52hZ4", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1969, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u8179\u90e8\u4e48?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "EdbdA24RTRnQ8pi3NwYFEg", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1970, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u80ba\u5417?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "cRSxatrM7N4wK5UHeXZFgQ", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1971, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u76f4\u80a0\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "hZmjQBscYcJ883yWp8m6Nz", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1972, "prompt": "\u5728\u8fd9\u4e2a\u56fe\u50cf\u4e2d\u5de6\u80ba\u548c\u809d\u810f\u54ea\u4e2a\u4f53\u79ef\u66f4\u5927?\n<image>", "pred_response": "\u5de6\u80ba", "answer_id": "26VEtG2N7Njd6zmcXv4tCN", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1973, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "YH68io5cQzQR6zB9RevZJn", "gt_response": "0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1974, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u810a\u9ad3\u7684\u5f62\u72b6\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u4e0d\u89c1", "answer_id": "9dRCf4ZQYRjr8V4WuvbsHP", "gt_response": "\u5706\u5f62", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1975, "prompt": "\u8fd9\u5f20\u56fe\u7247\u5de6\u8fb9\u7684\u7070\u8272\u5668\u5b98\u6709\u4ec0\u4e48\u4f5c\u7528?\n<image>", "pred_response": "\u6392\u6cc4", "answer_id": "LDcRLetZe3HyQaEikFJMXg", "gt_response": "\u751f\u7269\u8f6c\u5316\u3001\u89e3\u6bd2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1976, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "CyD5oJbeDdhJ2SbMiuNxJu", "gt_response": "\u80c3,\u7ed3\u80a0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1977, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "WbSUMM8vw3cYKsuiFZFgse", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1978, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u53ef\u4ee5\u5206\u6ccc\u9176\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "ap7bCWYvEP36VKNGbqwcFh", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1979, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u76c6\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "nsGj5G6zuuVp2izGQGfJ64", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1980, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "L7uvwHhb3fB8yfzepqxLLR", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1981, "prompt": "\u8fd9\u5e45\u56fe\u50cf\u91cc\u6709\u6c14\u7ba1\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "3MpjHuPimJiTZPSYe6dXkd", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1982, "prompt": "\u56fe\u4e2d\u80be\u810f\u7684\u989c\u8272\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "TPMbqTY5JAMH2YRZUaR5rM", "gt_response": "\u7070\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1983, "prompt": "\u5728\u8fd9\u4e2a\u56fe\u50cf\u4e2d\u5c0f\u80a0\u548c\u80be\u810f\u54ea\u4e2a\u4f53\u79ef\u66f4\u5c0f?\n<image>", "pred_response": "\u5c0f\u80a0", "answer_id": "5ruLjTudFYg2yQ8VqyBTEN", "gt_response": "\u80be\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1984, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u5668\u5b98?\n<image>", "pred_response": "2", "answer_id": "WQc6wfAbfNPNq7XxcqnNBm", "gt_response": "3", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1985, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u4e2d\u6700\u53f3\u8fb9\u7684\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "PNJKja5gRSQf7qi9R5yHFq", "gt_response": "\u5438\u6536\u8425\u517b,\u5206\u6ccc\u9176,\u6d88\u5316\u98df\u7269", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1986, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u5206\u6ccc\u9176\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "mSDtyVwDTbpWeDCemNn2o3", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1987, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u89e3\u6bd2\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "mGSWzJLMgb99AQJ8EAZVM6", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1988, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7CT\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "P3atCgmobhZCwicpEXLYCD", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1989, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "4htqe6FGF4FRnniwHmvFg4", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1990, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "ifvTYpWaw654MdhDRgrLVy", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1991, "prompt": "\u56fe\u50cf\u662f\u5426\u5305\u542b\u7ed3\u80a0?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "739UNRFXuBoSWZoy2LXzDD", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1992, "prompt": "\u8fd9\u5f20\u56fe\u7247\u91cc\u6709\u80a1\u9aa8\u5934\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "WMmo9aGPWpdTmCTYPMfjRq", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1993, "prompt": "\u5728\u8fd9\u4e2a\u56fe\u50cf\u4e2d\u53f3\u80ba\u548c\u5fc3\u810f\u54ea\u4e2a\u4f53\u79ef\u66f4\u5c0f?\n<image>", "pred_response": "\u53f3\u80ba", "answer_id": "bXE9JVkXXR962KLrNTiLfM", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1994, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u5668\u5b98?\n<image>", "pred_response": "2", "answer_id": "YHSzjYcxowsU6Hw5mzCj3r", "gt_response": "4", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1995, "prompt": "\u56fe\u50cf\u53f3\u4fa7\u7684\u9ed1\u8272\u5668\u5b98\u6709\u4ec0\u4e48\u4f5c\u7528?\n<image>", "pred_response": "\u6392\u6cc4\u5e9f\u7269", "answer_id": "7vnkmxV7mc2RhtbiUgquhZ", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1996, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "7uE5J3a4HVfLNocps8pRpe", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1997, "prompt": "\u56fe\u4e2d\u6709\u6ca1\u6709\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u547c\u5438?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "cYkYTuiTVFthiwVgwaoudC", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1998, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u795e\u7ecf\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "cVEtw4kzQUZy7Pvp6jjoom", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 1999, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "V4XRzi2aXjdCkmguyxejkS", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2000, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u80ba\u5417?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "HrQyx2ZgStrcMkXxTKxWGv", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2001, "prompt": "\u56fe\u50cf\u91cc\u6709\u80be\u810f\u5417?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "AniEwYtLTiYngPjd29mHcE", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2002, "prompt": "\u56fe\u50cf\u53f3\u4fa7\u7684\u9ed1\u8272\u5668\u5b98\u6709\u4ec0\u4e48\u4f5c\u7528?\n<image>", "pred_response": "\u6392\u6cc4", "answer_id": "hzMNHZNF676iiXaCmLXsTb", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2003, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "bvmHmttDqjxH5SzPWjxmpm", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2004, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Fva8s3G5VgXhUwzuUGQH2A", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2005, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "oYhyDqXCN3oGbckaQqW4LC", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2006, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "VocHQN8XP49zRj9Kmh2TWZ", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2007, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "CArc6kD3VXY9VwUfuRNx4v", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2008, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "TWmnFUANDvUY6vpWw7KUVu", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2009, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "KW2FpuShQSF6jHnZW4KasL", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2010, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "DnBC8CUJWAXm2RxAxZ6HoU", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2011, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "gvrrKxQmexgX79dyP9whNA", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2012, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "k8icSn2XMT2KcUvCyYX492", "gt_response": "\u76c6\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2013, "prompt": "\u8fd9\u5f20\u56fe\u7247\u91cc\u6709\u76f4\u80a0\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "NFxCau36PBKhFwtWQMGJzt", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2014, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u98df\u9053\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "PnCSyT3v4qcV6bB9nbXkWC", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2015, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf?\n<image>", "pred_response": "\u80a0", "answer_id": "ZgRTkkJzig9Qw63GEk2RT2", "gt_response": "\u76f4\u80a0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2016, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u9aa8\u76c6\u90e8", "answer_id": "jHWgH6gPkh7qR57DrsbdTK", "gt_response": "\u76c6\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2017, "prompt": "\u56fe\u4e2d\u76f4\u80a0\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "hP9ayM37igj5AxSfWyeXmB", "gt_response": "\u5e95\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2018, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "J3fXkgcoXUE5JHKrEF26xL", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2019, "prompt": "\u8fd9\u5e45\u56fe\u50cf\u91cc\u6709\u773c\u775b\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Kj5k24aokCr8wyfe9rxpbd", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2020, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u5c0f\u80a0\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "cWA8TbtyFoThBMgegytUhq", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2021, "prompt": "\u5728\u8fd9\u4e2a\u56fe\u50cf\u4e2d\u8180\u80f1\u548c\u76f4\u80a0\u54ea\u4e2a\u4f53\u79ef\u66f4\u5c0f?\n<image>", "pred_response": "\u8180\u80f1", "answer_id": "Swmz67ioYgQhVUn4Ztfe5C", "gt_response": "\u76f4\u80a0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2022, "prompt": "\u4f4d\u4e8e\u56fe\u50cf\u5e95\u90e8\u7684\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6392\u6cc4", "answer_id": "kd5NANNc4XScJweRmdkqGe", "gt_response": "\u50a8\u5b58\u7caa\u4fbf,\u6392\u6cc4\u7caa\u4fbf", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2023, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "kgfLT87XKYfrrckEfdTcej", "gt_response": "\u5c0f\u80a0,\u76f4\u80a0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2024, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u547c\u5438\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "cKu83fCAyoYsSfRVky5AGK", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2025, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u53ef\u4ee5\u63a5\u6536\u58f0\u97f3\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "PYV4FPgB84TeQCw6KQ9qRx", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2026, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u9aa8\u76c6\u90e8", "answer_id": "hsGNcptS9NE2FWicdy9eWV", "gt_response": "\u76c6\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2027, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u5c0f\u80a0\u5417?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "eSwQ4MPfBfKS4rcvT4vc37", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2028, "prompt": "\u56fe\u7247\u4e2d\u7684\u8180\u80f1\u662f\u4ec0\u4e48\u989c\u8272\u7684?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "e2fCEHSR3LhvUzLH9DZB36", "gt_response": "\u6df1\u7070\u8272", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2029, "prompt": "\u56fe\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "9jeobBBaqTnAFcbytWUavQ", "gt_response": "\u8180\u80f1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2030, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u6ccc\u5c3f\u7cfb\u7edf?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "HZxRzoPwUw7gZuziSmnF3E", "gt_response": "\u8180\u80f1", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2031, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "F427hpVqaUynHdSQakK3NQ", "gt_response": "\u76c6\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2032, "prompt": "\u56fe\u4e2d\u76f4\u80a0\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0b\u90e8", "answer_id": "BC9qYM62nwbbnL2sxHtf3F", "gt_response": "\u5e95\u90e8", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2033, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u76c6\u8154\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "AX8iNmm9iumWr9HhYYAEyr", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2034, "prompt": "\u56fe\u50cf\u662f\u5426\u5305\u542b\u7ed3\u80a0?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "5oDNzakPm3QXCpgSHSYbr2", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2035, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u76f4\u80a0\u5417?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "Jv9cP26f4pCcLGCks6RmAE", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2036, "prompt": "\u5728\u8fd9\u4e2a\u56fe\u50cf\u4e2d\u76f4\u80a0\u548c\u7ed3\u80a0\u54ea\u4e2a\u4f53\u79ef\u66f4\u5927?\n<image>", "pred_response": "\u7ed3\u80a0", "answer_id": "HPhUmCUaiLKryttk7KK5Hu", "gt_response": "\u7ed3\u80a0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2037, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "2Fy6Ts8EeFe2DTZnDyVNMK", "gt_response": "\u5c0f\u80a0, \u76f4\u80a0, \u7ed3\u80a0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2038, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "6yQdAWiRhULThaq7jqrG2c", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2039, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "3uBfZnV5bLWQJAFYTMYomC", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2040, "prompt": "\u56fe\u50cf\u662f\u5426\u5305\u542b\u76f4\u80a0?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "5JvM7UUyXmgDvR5Vj55Mio", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2041, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u5927", "answer_id": "KEjjN4Uk72C76C7J6CxmAc", "gt_response": "\u8f83\u591a", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2042, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6ccc\u5c3f\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "98szu3VsHQyHQjgqgLt7k6", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2043, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "2e8U6zL7uA6SkrnLBzEgDP", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2044, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "FFoZYUbooENLhDbptpEN28", "gt_response": "\u76c6\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2045, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u76c6\u8154\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "e6hYc3ofm6bpBRLki35J2r", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2046, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "5pRcReutBZ7dWa2nBxXud6", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2047, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u6d88\u5316\u98df\u7269\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "9uG6xZBAmwThHHRrM2aidj", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2048, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "bJ2s5JiEsSM7L8MK5fmSdd", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2049, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "BDSYS64FiFcTzit9tAn6ko", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2050, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "XgotHa8mUmsPxFLpEKxqsr", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2051, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "bwDVqZZkmfKEouu5YHiVvU", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2052, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "EwFpSG6GA5hkHW9tRYgB6q", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2053, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u8111\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "Jx6PdhbfCQGxiSuS49Shkc", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2054, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "Wx7xCxXLAvvZVs3hLyoW4s", "gt_response": "\u8111\u80bf\u7624,\u8111\u6c34\u80bf", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2055, "prompt": "\u5927\u8111\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "oQUGmC4ui3npdy3wNpRfwR", "gt_response": "\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2056, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0a\u90e8", "answer_id": "DFMAx6BtRPkVpKFDMCPt8J", "gt_response": "\u53f3\u4e0b\u89d2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2057, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "5U4V3jsgpHHFL77gFZud58", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2058, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "V7YxPyqAyEXnLZ7VpFC69J", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2059, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "BkVB39A9eG6dTqJfEaU6e5", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2060, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "9YNNTmq6AfpmTMpKmTbN9a", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2061, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "Q3a8F4YwwDmrABWJhG9PNQ", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2062, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u8111\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "bYcgbywKgB3kA3qG2eJuye", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2063, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf\u7624", "answer_id": "3vbwCMnWzqgV5jw3whyBrs", "gt_response": "\u8111\u80bf\u7624,\u8111\u6c34\u80bf", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2064, "prompt": "\u5927\u8111\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "6nqio73HL3CdCkN4trhYg7", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2065, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0a\u90e8", "answer_id": "dwJZpjm5ECDAKquriwT5ng", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2066, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "Bpjy2a5G9JXcm5HenWopTs", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2067, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "YvbgyAccRJoSm9X5Yk5snR", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2068, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "An8FRViRH3Ebtjw7hfyK6b", "gt_response": "T2", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2069, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "gC6emuuqRXXZ4aVqNoTjz6", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2070, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "PmdEN3B6xHc7m86WGukr7j", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2071, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u8111\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "j7icYugFMEHBPA4cKSkLJC", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2072, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u8111\u6c34\u80bf", "answer_id": "HS2pjzD6eMvaY6dDXJEHwd", "gt_response": "\u65e0", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2073, "prompt": "\u5927\u8111\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Z799jgfDxqyT9zGDxX39Gp", "gt_response": "\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2074, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "YLfhFstKTQuLysEauKYLnT", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2075, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "KKt8ysPdAF4cPDYicrp38G", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2076, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "nSJyCsNkWNRfQ89jBENjHQ", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2077, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "kCAsAZ8HGnnPAc5CCwGkWm", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2078, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "YgBMavhLLZLfWVFtzaf2JX", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2079, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "RjYKdPC8yot5RfFVqdCw5u", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2080, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "hcwWfhuCjBgzk8EDjNQR7k", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2081, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "GmwyvEEVBq8X8P9Gp474oT", "gt_response": "\u80ba\u764c", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2082, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "8kVJUKVybijyekxkFV4qnw", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2083, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0a\u90e8", "answer_id": "K9uWAq6T6mid8AnhZJZERf", "gt_response": "\u53f3\u80ba, \u5de6\u4fa7", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2084, "prompt": "\u56fe\u4e2d\u80ba\u3001\u5fc3\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u80ba", "answer_id": "BJ7uibc5nisxWjVJnyHN8T", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2085, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "JE3WwFo6o33cyYECYut4or", "gt_response": "CT", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2086, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "7oxrbJzMyJ2utgjMHj6Kuh", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2087, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "SNumTsCKn5uiNNcbsnqcra", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2088, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "KqtfHagBGPU9TYPUVGSMc8", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2089, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "7kyg8LmbZYEjgZQvYg6UvF", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2090, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "ALwQREbykRzmsB22F3F47h", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2091, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "ScXhYH449tkgsNb5H5YTyf", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2092, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "fbJFEvEYQBKumPWogh6noS", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
{"question_id": 2093, "prompt": "\u56fe\u4e2d\u80ba\u3001\u5fc3\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "W97C4cLvmSbjutELuqahUm", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri", "metadata": {}}
