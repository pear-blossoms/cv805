{"question_id": 0, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "X-Ray", "answer_id": "eGoiw6owS54bfneZ2X3wMN", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "L5sVjiEHKoZ8h8raCqneqn", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "EZEdkLoqCyeTvcaKVWNwBG", "gt_response": "Lung, Spinal Cord", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 3, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "f8CrusVg4NGCtnxbecsDzo", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 4, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "R6LoUwwK5UWaDH8xhgGeDB", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 5, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "5JFC4bND7dmsmUm23WEvpN", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 6, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "GVa6pWSRReFjPGVUhSKXKv", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 7, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "dEnmdPyrZAUZjnsvXGWWLR", "gt_response": "Lung Cancer", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 8, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "UkbnFmThh59yuvgD6mJtWp", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 9, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Right Lung, Left Lung, Right Chest, Left Chest", "answer_id": "T5XPEuz7pPTCHKLsGdrkEt", "gt_response": "Left Lung, Right", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 10, "prompt": "Which is the biggest in this image,lung,liver or heart?\n<image>", "pred_response": "Lung", "answer_id": "maizB9xHMBLDHnqxYctvS8", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 11, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "XbXjQbm3E2FGunVm8uuLqj", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 12, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "AJwAXWCfMgtQqAEcpJLnN8", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 13, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "CY3XPii7VXc7ikkeNu6zmH", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 14, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "iGz699B58T933NcAgvLsQV", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 15, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "mGqYqzh78obEDRp6LmGUVG", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 16, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "V8aLGWZaL2U5uSwi4u5icM", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 17, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "6LQMLjAmUXFr7WBjzffWHK", "gt_response": "Lung Cancer", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 18, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "g8N6RHXuS84gdVs4PLEUpe", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 19, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Right Lung, Left Lung, Right Heart, Left Heart, Right Kidney, Left Kidney", "answer_id": "mjpfR5SxYE4wp6yrRDPSCd", "gt_response": "Left Lung, Right", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 20, "prompt": "Which is the biggest in this image,lung,liver or heart?\n<image>", "pred_response": "Lung", "answer_id": "BvVyL5Y7j4AfKCCB4Su9xk", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 21, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "9t3dEmVDebZSGAx3FHvkhr", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 22, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "PMPQfM4hA6amtHU4un486X", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 23, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "FpaeE7o9bNNeGRkYgJVo5L", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 24, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "LxEr642CQCnNC68hyGeDib", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 25, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "oJuFMGcDrLCiTLJeHzajCd", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 26, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "TDVf7ZDYFG8b3rrz4vktfW", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 27, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "SmMf6ZMpoBSy8kWtgBBbUu", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 28, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "d2LmLt5ZhN2qMBfSkExBSH", "gt_response": "Lung Cancer", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 29, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "jnEHo2We8cjQoVrhdFMEcL", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 30, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Right Lung, Lower Lung, Upper Lung", "answer_id": "V9W8aptzkmwqhLrzvTaYvm", "gt_response": "Right Lung, Left", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 31, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "X-Ray", "answer_id": "iwP9oK4chLmEXZo5DeCc2U", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 32, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "VGxDSS4tiUnw8bewmXJZBm", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 33, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "e64zwG97r3yXuHxcuGwvCm", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 34, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Pneumothorax", "answer_id": "hAKo7jxhjVHrV6K7hwfDaZ", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 35, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left Lung", "answer_id": "QHWtHBkcczCozeTipiG26V", "gt_response": "Center", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 36, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "UZBQ6DEHpcav8JEpkpP3sa", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 37, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "mCa4KCkkpapNkby2WT4ezN", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 38, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "2HUoXmmXCnLoKqh9fRsTSu", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 39, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "myk5oZgu5xdgXTVwNLj63v", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 40, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "R2cUk97eQdURZ5WzASqxZk", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 41, "prompt": "Is there any abnormality in the spleen?\n<image>", "pred_response": "No", "answer_id": "JhGmtP2He77dztYPzd4GyK", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 42, "prompt": "Does the liver look normal?\n<image>", "pred_response": "No", "answer_id": "NPMcJXtyeCr3tunmaifDdq", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 43, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "8QYwbPg8DostmujCGoJpfn", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 44, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "TanoWmKVQRpDDan7ddp569", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 45, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "8bW3mrf5Bs8rCNQbehCfFF", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 46, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "X-Ray", "answer_id": "3vK3AALTNh29zGMm7UeFiR", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 47, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "TFBSv6KSq9cme65sgpFYsg", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 48, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "FhvZRHBWXXoiUNwd6ZB8JV", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 49, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "EsTo47fxhJnnHLmyQkVSg4", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 50, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Pneumonia", "answer_id": "nV7oKMC3rEJtm6By5v6wB2", "gt_response": "Pneumonia", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 51, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Right Lung", "answer_id": "WNdNbKGosuaxSj3CUrD68q", "gt_response": "Left Lung, Lower Right", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 52, "prompt": "Does the lung look healthy?\n<image>", "pred_response": "No", "answer_id": "LvduZzy3NqDzig9AQbapeN", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 53, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "X-Ray", "answer_id": "VW7AZoabwd4Bcocj6RTfyi", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 54, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "GzsHiY8FqpktGAfe6LKp7N", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 55, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "hSw8kQijkAUB5R22VMy6hM", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 56, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "FCTvyscgJLRfLcfubZVZDJ", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 57, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Pulmonary Edema", "answer_id": "Wf2UcJjFwjb3Nrze2mfiep", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 58, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left Lung", "answer_id": "ft8nXbpWZGt7gicYFRBoeo", "gt_response": "Center", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 59, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "EnyjenoL4aHAEYBr9qVmDg", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 60, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "X-Ray", "answer_id": "FqLot7YQduxukHZ9jUV6Tq", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 61, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "KsNCRwW56qc2GqFogfJfHc", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 62, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "dVruLPgSTkRReu5vwHas2z", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 63, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "fwWqfikfHw3EVpoFqvtrgp", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 64, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Pulmonary Atelectasis", "answer_id": "WL3iW4am9262ueieMx7zWj", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 65, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left Lung", "answer_id": "698tMWwvLugcc7LxS3PDKJ", "gt_response": "Center", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 66, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "6NqoKcjQdJmPagM8PPcvhn", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 67, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "X-Ray", "answer_id": "K7bDWrHLiaBuvFTPcrZ8Rb", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 68, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "KcG6CpCpSbbpBFwf7p5F7v", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 69, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "gFDxDmSwSEBedfe7KCFkJf", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 70, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "WAm6UwdGL9pndDH2hxdS3N", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 71, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Pulmonary Edema", "answer_id": "6KKrX64hsCYsTeMz699BEN", "gt_response": "Atelectasis, Mass", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 72, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left Lung", "answer_id": "hHbJDPDt8NHDUva9mcV6wU", "gt_response": "Right Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 73, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "FJdrUQL7D2BtXFeumaA9uK", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 74, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "MLXECfMDjfvYxWiNyQ6y6z", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 75, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "X-Ray", "answer_id": "GR2EbBYa6f6SL293KnnzdF", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 76, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "oA8iAYA3mG3nDwrqZT424t", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 77, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "gAvgZHELHfkBWBjK7cvhKB", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 78, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "buV75WyxgbEPowVqYeKoUU", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 79, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Pneumothorax", "answer_id": "EaiM4HBcUNv6UhqFpPWGM3", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 80, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left Lung, Right Lung, Left Chest, Right Chest", "answer_id": "2jhJYKTPHEwXXKLdUBLuMq", "gt_response": "Center", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 81, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "Gd4DRp8bTgVDJiPWP3tRfw", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 82, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "X-Ray", "answer_id": "frHRx2rqyYAPSHjEMoYXa7", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 83, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "d22YjXEF8vssmgyrUf4wpK", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 84, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "dFJSLGkLngxDKvV5o2hokh", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 85, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "Wc3tfkzQbF2dZZgqJba5Np", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 86, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Pneumothorax", "answer_id": "jJHJZjwAszduvzewUrVUNK", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 87, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left Lung, Right Lung, Left Chest, Right Chest", "answer_id": "MrQuaKeMX3uYjBLnyR9VFG", "gt_response": "Center", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 88, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "5WG2AXwZGvZtdz8Kxqd8PV", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 89, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "X-Ray", "answer_id": "BYuEkEc7ybtmdUG4TA2Zmd", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 90, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "GxpbcTay96KHPqiQ7XF2FM", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 91, "prompt": "Does this image look normal?\n<image>", "pred_response": "No", "answer_id": "TgdUvrSv5Umr9WgCtp2eAb", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 92, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "cabh6jf9gB6apBcNPvDoEc", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 93, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "U2qMq2uTyhDhEMGUjxfBhn", "gt_response": "Nodule", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 94, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left Lung, Lower Left, Upper Right, Upper Left", "answer_id": "C5x7rTFgoJ2uuhFYoiENoo", "gt_response": "Left Lung, Right", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 95, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "mhUcvGS4ccXUHLGP8QJxdP", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 96, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "fFXU5Hqv6v7vFgr3exuMTU", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 97, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "X-Ray", "answer_id": "hMX5kBDZ4co7RpxfMa9pTu", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 98, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "fZgR4bKseEH4KGhjAsRcLk", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 99, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "ZeAngZcJjdUECUsgLUTmdq", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 100, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Pulmonary Edema", "answer_id": "ZZJZn4r4N3bVSZsGxNMdcR", "gt_response": "Nodule", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 101, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Right Lung", "answer_id": "Q6NXFKVD6zmSAPSB6NrVwz", "gt_response": "Right Lung, Left", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 102, "prompt": "Does this image look normal?\n<image>", "pred_response": "No", "answer_id": "KiEgeP7wmF6ixtwELeRD3T", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 103, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "PuFQnTRwnfLW4chb4VaWFw", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 104, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "USGCw6oMAKgbCVTk9nqnXr", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 105, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "X-Ray", "answer_id": "K6pcaXvWsrE4dKNweQP9F8", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 106, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "SDGr29mtgWQt3PnLknFYMk", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 107, "prompt": "Does this image look normal?\n<image>", "pred_response": "No", "answer_id": "QC9u8BbLnmJHotvLe7n6eX", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 108, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "SxoQX45H3v2VnbocNHDy2k", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 109, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Pulmonary Edema", "answer_id": "NvfMQHnNEQ8BGihAAAUEpC", "gt_response": "Atelectasis", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 110, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left Lung", "answer_id": "i2VYwfkjcUutVPmn9cqz6d", "gt_response": "Right Lung, Lower Left", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 111, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "Zq32bAuBA2NbCdoHcvpVTz", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 112, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "EgRxHuLYtWxSej2K3gnQi6", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 113, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "X-Ray", "answer_id": "iMn6oaZL295Y3QgiEVM5bg", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 114, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "3YpAqsDh8LhEUJ3JPWdprH", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 115, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "6QSpp3YFAYN9PHFCAucP4j", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 116, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Pneumothorax", "answer_id": "UZe2H8E5vZmb74vwpRPJJ6", "gt_response": "Pneumonia", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 117, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left Lung", "answer_id": "dzj7C5hNkVgQLPcqZamCaJ", "gt_response": "Left Lung, Right", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 118, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "c24dFe6NvngxBdGVt9p5hS", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 119, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "3khMmYn6jye7XYPRfgG7yY", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 120, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "NG2tbx9YYA2AjHzLmQXbxi", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 121, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "KFrtqp2MFFszoeCGt8Cy97", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 122, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "U8HrwnwjNCG6eCSCijsjuo", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 123, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "eyQp5tcCDwk28hdXZFYDkh", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 124, "prompt": "How many kidneys are there in this image?\n<image>", "pred_response": "2", "answer_id": "ANxFfLzvJqLJkBkaqr484z", "gt_response": "1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 125, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "76VXZpCvtDvwdX6727eZEL", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 126, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "kGbfSWRWN5gSsb6RU8fTwG", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 127, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "2CyefYqUGgR9aYoSLwfUj9", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 128, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "KM3G2UPHTiFnsibx33SRTg", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 129, "prompt": "Does the liver look normal ?\n<image>", "pred_response": "No", "answer_id": "Hk8PJAUfrUHhRAS9ngE9Ko", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 130, "prompt": "Does the kidney look abnormal?\n<image>", "pred_response": "No", "answer_id": "Yiesq84w3XMGmfMdg4gxcZ", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 131, "prompt": "Do the spleen look abnormal?\n<image>", "pred_response": "No", "answer_id": "5LKVtZbnnnifHkFw2BXDjm", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 132, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "X-Ray", "answer_id": "DGy4u6hYmURVfTdU3FQKJr", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 133, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "GhcFZ5arraL4YNBEWATTNC", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 134, "prompt": "Does this image look abnormal?\n<image>", "pred_response": "Yes", "answer_id": "YgauAi5XAfkaEyoQG2BD4N", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 135, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "Zy5XPu79ZPiNaQxH22ytSC", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 136, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Pneumothorax", "answer_id": "mZQ936S6d2eVAYe4gf2kbV", "gt_response": "Pneumothorax", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 137, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left Lung", "answer_id": "Shji4f75fUsTAwXzWETw22", "gt_response": "Right Lung, Left", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 138, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "jRqzT9NBCBmUQtEne3LoVu", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 139, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "Qy2Nf8bzy48bDeBkmbFCyv", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 140, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Lung", "answer_id": "oQaBs55NniuzCpfySswNPU", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 141, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "MztePBHi3b4tu8yVK6E5iG", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 142, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "9PqtxpKw2azSQKPyzC2PcL", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 143, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "mxr3MpF8gvrHy5bS5m8ZZf", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 144, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "4SgQiyJggrnUZ6msVxKTVL", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 145, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "mE6AhPnnmBsisjzHj6vTmd", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 146, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "Rw9nj6cKB58KQTmYtDikyW", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 147, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "UY8AQkPw9Xz6NVnQSH457H", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 148, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "bNfm2cv3ZFvUeBwVutqqav", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 149, "prompt": "Where is the liver?\n<image>", "pred_response": "Upper Left", "answer_id": "UZtXY3dnrKKv6utBvLi8pm", "gt_response": "Left", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 150, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "LqgrwiFnET8R8ZaYfmEZE4", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 151, "prompt": "Is the liver healthy?\n<image>", "pred_response": "No", "answer_id": "iwZnHHv2Nnn2iaJX94LT4g", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 152, "prompt": "Which is bigger in this image, kidney or liver?\n<image>", "pred_response": "Liver", "answer_id": "9tV9r97ToqNptWoiajeYkm", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 153, "prompt": "What is the organ located in the lower right corner of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "Tv2S2qcF9JMms9z54uRqwf", "gt_response": "Left Kidney", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 154, "prompt": "What is the organ on the left side of this image?\n<image>", "pred_response": "Liver", "answer_id": "hxvr8iK4fm9FBcSv8n4j4t", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 155, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "V76EM8MuUbLVzMXpzwBx6G", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 156, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "XvgkL2j4rfCvcAn3qNcx3Z", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 157, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "bsr3qC9uQLqh4eCkAZpPEf", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 158, "prompt": "Does this image look normal?\n<image>", "pred_response": "No", "answer_id": "Mt4DkZLw34hW3HbGnWE9mZ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 159, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "2", "answer_id": "NGXb7QppoSb9hQquF2Wi6W", "gt_response": "4", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 160, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "jUocgxETT6HMeRhV2MnVwY", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 161, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "GpsHMSzUKgNRce3dYZkjfn", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 162, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "7tJzd5r5Zhum2xc2H7k795", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 163, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "JpFPDqCzcGqjmQ5BEwcWQr", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 164, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "j7M5TaRdRijcnQN8K7V3X4", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 165, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "Yes", "answer_id": "F69xPL3a2R9dUkYptXfhQB", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 166, "prompt": "Where is the liver?\n<image>", "pred_response": "Lower Left", "answer_id": "WckF5YuRwrtMaDskM6977N", "gt_response": "Top", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 167, "prompt": "Is the liver healthy?\n<image>", "pred_response": "No", "answer_id": "nLP4tdghUzWvNvdVLHtkNN", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 168, "prompt": "Which is the biggest in this image, lung,heart or liver?\n<image>", "pred_response": "Lung", "answer_id": "4AEand3hWMeqiqJvh4cFRV", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 169, "prompt": "What is the rightmost organ in this image?\n<image>", "pred_response": "Right Lung", "answer_id": "S9iP9uNYKHWE8gGnw4Ujmv", "gt_response": "Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 170, "prompt": "What is the organ on the left side of this image?\n<image>", "pred_response": "Lung", "answer_id": "CpgLauXn9dhDYbnhWPWLGW", "gt_response": "Right Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 171, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "8WhH5ceEmMipzsB8QbyAvR", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 172, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "MzSg9bK7HcBnHTbBprcdhR", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 173, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "GnYB3bBDbekFQDwDzJWe3z", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 174, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "QFLxWkpib3H8NAeSgdZNd8", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 175, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "BXrg3xStLBS9BpZTrjqD3w", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 176, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "bMdTWA75VL5rAPqrVFk3Gi", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 177, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "R8J5kzAESR3K33SAiqkfq7", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 178, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "gXwmAThbLTbPLbtKm43TuQ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 179, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "o2zYiNNz6PjkpUmpu2mFXN", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 180, "prompt": "Where is the liver?\n<image>", "pred_response": "Lower Left Abdomen", "answer_id": "c99yosvKtrHGun5StGYfTL", "gt_response": "Left", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 181, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "RZiDbxE4a2LXx3yhJ2u72L", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 182, "prompt": "Which is the biggest in this image, spleen,lung, or liver?\n<image>", "pred_response": "Liver", "answer_id": "DHDnqLvLhTMTa8bnVP9pEU", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 183, "prompt": "What is the organ on the left side of this image?\n<image>", "pred_response": "Liver", "answer_id": "YCj8cRWDibSc35mQbriPWs", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 184, "prompt": "What is the organ on the lower right corner of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "5DnWUe4PcwoFqkrroHXFpp", "gt_response": "Spleen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 185, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "3VBZ4hrxQgdH7ZLYCLqetu", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 186, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "95PgB2V8SwjszRgJdvzcyQ", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 187, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "GdeFNtDDNGuSyf2JNDHdQb", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 188, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "d385XaPRX3q6hxGSBw42Zg", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 189, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "MVj5qqozsgfgGqnB9wNp23", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 190, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "KwWhbqsSqwKBnmWSH7mnNb", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 191, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "VQhJDwZzaAdrBQmrZuXcC9", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 192, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Liver Cancer, Liver Tumor, Liver Abscess, Liver Cyst, Liver Inflammation, Liver Infection, Liver Cirrhosis, Liver Cancer, Liver Tumor, Liver Abscess, Liver Cyst, Liver Inflammation, Liver Infection, Liver Cirrhosis", "answer_id": "gWyggcS68D5fsBgrb8zDiq", "gt_response": "Liver Cancer", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 193, "prompt": "Where is the liver?\n<image>", "pred_response": "Lower Left", "answer_id": "V4KpXRns3fXqTmmcsGUFSh", "gt_response": "Left and top", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 194, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left Lung, Right Lung, Left Liver, Right Liver, Left Kidney, Right Kidney", "answer_id": "knJ33UHSssxjJm9fhoi96s", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 195, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "JRjCSDifCbrDdMfTgyxVNi", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 196, "prompt": "Which is bigger in this image, kidney or spleen?\n<image>", "pred_response": "Spleen", "answer_id": "BcXmpprfLm2DQUitnzqTKb", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 197, "prompt": "What is the organ on the left side of this image?\n<image>", "pred_response": "Liver", "answer_id": "XckJPqGn6vHfBNPioU8KqL", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 198, "prompt": "Does this image look normal?\n<image>", "pred_response": "No", "answer_id": "iu85hixfA2jX27hcYFr6e4", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 199, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "PaMq3icHkXgWyqpGJtGSQz", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 200, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "iuH4ga5FUaPbkvamjG7qwU", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 201, "prompt": "What is the rightmost organ in this image?\n<image>", "pred_response": "Liver", "answer_id": "Qdo7PFGHXUHDgx6AEijpmV", "gt_response": "Spleen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 202, "prompt": "What is the organ on the top of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "YCHVoGWp47sXuu2BaEoadu", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 203, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "4sAQ8mwxAbAcbjhRD5qJCQ", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 204, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "La4xUJPHJwcdWoxVUDsBu3", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 205, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "XV9FkaVyA5ar3R3EWvRdfF", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 206, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "4ZxA9TN9hGPQ6zHdQseSjz", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 207, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "ejZdAfepqX67rNjxecmtXP", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 208, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "GY4UpBmdMjow3kYACzA45c", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 209, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "6DPiwCMHK5hBi3zCjdDP6A", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 210, "prompt": "Where is the liver?\n<image>", "pred_response": "Lower Left", "answer_id": "KTwqGFA5ugFZzywuhLgpZR", "gt_response": "Left and top", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 211, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Right Lung, Lower Right Lung, Upper Right Lung, Lower Left Lung, Upper Left Lung", "answer_id": "ZDPuy39QMyVwLQ3wdKS6wa", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 212, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "MRErYHHLreF83mjQbbxpf8", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 213, "prompt": "Does this image look normal?\n<image>", "pred_response": "No", "answer_id": "Z9rMvvqCuWuCpPUn3QKHAX", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 214, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "Ki4HnjRjXX2TXZ6DpfYBtV", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 215, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "HkoWJtixZZbXM2DEjNPdt6", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 216, "prompt": "Which is bigger in this image, liver or spleen?\n<image>", "pred_response": "Liver", "answer_id": "c69iJeyiyFWrpAzJcqQWn3", "gt_response": "Spleen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 217, "prompt": "What is the organ on the lower right corner of this image?\n<image>", "pred_response": "Liver", "answer_id": "brRh35jwAG567xvkyzMBck", "gt_response": "Spleen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 218, "prompt": "What is the organ on the top of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "3dzVVhJcVvySzTsyLBGrcP", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 219, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "6kbHV8wVTEyHP3xrfBjup8", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 220, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "6Awj2QUXptH7awx2UCPffz", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 221, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "BMNEiXdTMb52oH2kdCAH6t", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 222, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "FAx4sqhzsd3wdYyZ2Sa2JB", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 223, "prompt": "How many kidneys are there in this image?\n<image>", "pred_response": "2", "answer_id": "6pfte6BK2agvGbV2SAfVL9", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 224, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "aQboj5aJTS5QZJumiMstBk", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 225, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "5F9h8voyMkB8WXsQ3u5uEL", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 226, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "YhAxTptKnMXnQwhhitAYjb", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 227, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "X-Ray", "answer_id": "g7yUX83BQcx4mjQmx6HSaM", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 228, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "nmpGWapT76XeUfjdXwfX5n", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 229, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "UAMUXYHLRYZQaSrJxRpQhw", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 230, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "PvCeMyR2WDYVV5gz8Sn8M5", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 231, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "5bmdcsr3KvadBLsf7mNCmo", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 232, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "MsdKTtAhaAVoXgzMYbTuUQ", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 233, "prompt": "Where is the liver?\n<image>", "pred_response": "Left", "answer_id": "BPuKw9SLcsvLFVwsqJ8adA", "gt_response": "Not seen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 234, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "K26TcESoxNe4jKQv7uaonV", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 235, "prompt": "What is the organ on the left side of this image?\n<image>", "pred_response": "Lung", "answer_id": "euocArww3Wxydr3hxsARCa", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 236, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "RCebk4tQ9kGSm5CfK5MRNs", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 237, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "PkSxhfdGpgvUX7UpSDbxhe", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 238, "prompt": "Does this image look normal?\n<image>", "pred_response": "No", "answer_id": "iuePevenE45BTMXuQp4iyt", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 239, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "HqesqZzv3LfFNcsiuiRAKS", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 240, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "8WdB57oFtBcF2ZHFkRSFNn", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 241, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "ZWTsTmS2W2KAPNApYnpmjw", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 242, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "bHN6LDMcsfPzEuyB6SnjA3", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 243, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "m6DhsFkZC8ERFQJSp9m7wm", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 244, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "mLiBqLM9eu8VDeBsFbh9BP", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 245, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Liver, Kidney, Heart", "answer_id": "demGvxM644a27CSemAf9S9", "gt_response": "Liver Cancer", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 246, "prompt": "Where is the liver?\n<image>", "pred_response": "Lower Left", "answer_id": "cPT5v8z4qsBaASL9Tqk2wZ", "gt_response": "Upper Left", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 247, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Right Lung, Left Lung, Right Liver, Left Liver", "answer_id": "WzwczNbxZY8YyZsSvjJs4G", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 248, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "MRSvEut5sHVSTqwN7Ck9wP", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 249, "prompt": "Which is bigger in this image, liver or heart?\n<image>", "pred_response": "Heart", "answer_id": "GLRo9wdZMUqqhB6b6xK848", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 250, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "czL2iK6cwpYvR5hYAK7pFJ", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 251, "prompt": "What organ is the black part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "idrMLx4NjCKQ6nUGoQKDBk", "gt_response": "Right Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 252, "prompt": "What is the organ on the top of the body in this image?\n<image>", "pred_response": "Lung", "answer_id": "3U492AdRBmVMpgZPhUcqom", "gt_response": "Heart and Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 253, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "Wq7iz4CohVY3qRd472RFFz", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 254, "prompt": "What are the white points in the black part of the body?\n<image>", "pred_response": "Liver", "answer_id": "f85zAKjDa5utVXQ2yQxHBn", "gt_response": "Pulmonary bronchus", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 255, "prompt": "Which is the kidney?\n<image>", "pred_response": "Right", "answer_id": "3PjvgTtDJuENjsidLgpySN", "gt_response": "Symmetrical to the bone marrow", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 256, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "JdukZXCwequPqP9kZqXwU8", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 257, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "8rSJaxGMESYBnFt4hHUgS6", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 258, "prompt": "Does this image look normal?\n<image>", "pred_response": "No", "answer_id": "GtbKiBGNDdDS3J76hm2SZL", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 259, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "Y4hUKD5oosgqFPDrhvWjaf", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 260, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "XpWiwFEseZ5ZbV6YF7CsgG", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 261, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "XtTUxsbkC2juRSbSVgKL3H", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 262, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "24hnK6H9HCWamhqN8BU6x8", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 263, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "QbKjc3m7fDYVCxDKWHqBuJ", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 264, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "c2nbKRGgYREnf8yTimmXTe", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 265, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Liver Cancer", "answer_id": "huz5vYsoJkBbRJHteW9zWk", "gt_response": "Liver Cancer", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 266, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left, Right, Upper, Lower, Upper Left, Lower Left, Upper Right, Lower Right, Upper Left, Lower Left, Upper Right, Lower Right, Upper Left, Lower Left, Upper Right, Lower Right, Upper Left, Lower Left, Upper Right, Lower Right, Upper Left, Lower Left, Upper Right, Lower Right, Upper Left, Lower Left, Upper Right, Lower Right, Upper Left, Lower Left, Upper Right, Lower Right, Upper Left, Lower Left, Upper Right, Lower Right, Upper Left, Lower Left, Upper Right, Lower Right, Upper Left, Lower Left, Upper Right, Lower Right,", "answer_id": "YjNtjjfqjttgtAwsG2iyEu", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 267, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "5U8LvZH7XKYjGSUQvxs2yt", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 268, "prompt": "Which is bigger in this image, kidney or spleen?\n<image>", "pred_response": "Kidney", "answer_id": "VwjP9TToouBWDADiy3gzTm", "gt_response": "Spleen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 269, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "F3v5kwcX2qZVXeBNah3JiQ", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 270, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "4dgszqYrkYkeqzVVgbjuTb", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 271, "prompt": "What is the shape of the kidney in the picture?\n<image>", "pred_response": "Oval", "answer_id": "Nu33FV9tPAPWAzSNbubRtU", "gt_response": "Irregular", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 272, "prompt": "Which is the kidney?\n<image>", "pred_response": "Left", "answer_id": "99YFv2XEcQfcJ5nW7tvcpy", "gt_response": "Symmetrical to the bone marrow", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 273, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "gb4UtoXxu52XWTSUnD7mQt", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 274, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "axRM87RiPFkrtub3i8d5V3", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 275, "prompt": "Does this image look normal?\n<image>", "pred_response": "No", "answer_id": "jZbEqjqSaGPigPyozMMZ5L", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 276, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "doSGdycZLMbbstpop48aNj", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 277, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "GVCt8AK9umsZsQit8qBGnF", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 278, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "79BQXJXdqESTvSQjqDhniA", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 279, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "eUcHcegqjYjUs8kU23NADY", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 280, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "7MCqknvxHxnw88m4dMZoFx", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 281, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "7S9pgxN4DEe2upiogcnDRL", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 282, "prompt": "Where is the liver?\n<image>", "pred_response": "Lower Left", "answer_id": "PcipkkkecRYDoRAMWoCX68", "gt_response": "Left", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 283, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "DVAc9bSzQCfmYbJF7aBWtq", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 284, "prompt": "What organ is the black part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "N7WLarR6Q99NRbwu9EXpr7", "gt_response": "Right Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 285, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "XrksUNTy9Rza6hvnxRTjgB", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 286, "prompt": "What are the white spots in the black part of the lower part of body?\n<image>", "pred_response": "Liver", "answer_id": "dt4aq2TWP6VTYbtJauQqrC", "gt_response": "Pulmonary bronchus", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 287, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "YBTBYmKVstjHqqsuM8ta9n", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 288, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "MgnUBtegmiLEF3oyyGjQxu", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 289, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "dJ5QhLgq7Ebn8oX8RBFycF", "gt_response": "Small Bowel", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 290, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "3UDMW5EvjAGEMsppP6MAcy", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 291, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "Yes", "answer_id": "2Dtkjp3SfMbM88tZ72jeNG", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 292, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "Gt7eZxGgTLEXMzoSbMgMfR", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 293, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "6At9UBnt9KmvzRx9BYmGEu", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 294, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "Jzuv4Pxm7bMuXQaUFz4JrQ", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 295, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "bNE92scWFMxq6i4z6YTJ8J", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 296, "prompt": "What is the rightmost organ in this image?\n<image>", "pred_response": "Liver", "answer_id": "hEsZt2NqCvDAztEEE5tkEr", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 297, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "L255SUJrHpP73utCDZxTUk", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 298, "prompt": "What is the organ on the top of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "ZeY4t2wGpncpGcqPVmieCf", "gt_response": "Small Bowel", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 299, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "VTFjNQdwKaUb59NWo7ymHu", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 300, "prompt": "Which is the kidney?\n<image>", "pred_response": "Left", "answer_id": "eHqT9VHUz6NKBd34Gyb688", "gt_response": "Symmetrical to the bone marrow", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 301, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "bzKKhB828JsfAJ3Gjb5GaJ", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 302, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "ET6ukr8vCVnqMCYCVznDEZ", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 303, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "EnZuqbd3vWDFxikZxkBzZD", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 304, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "Zek4DVK88Ez7KicHRDywnB", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 305, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "Pb5opuYjuWxr4WLTrArWL5", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 306, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "fhkAAa46DmKaYYpLkhRZmV", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 307, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "aKmyckLF8M3KvjrLvzDGnk", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 308, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "cHDprVLD4dHj5TzjkGUCVK", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 309, "prompt": "What is the rightmost organ in this image?\n<image>", "pred_response": "Liver", "answer_id": "Xmnx4kxEYaWqkbf6ufvYsm", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 310, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "DQrzc8gELV8m7U9QS2asST", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 311, "prompt": "What organ is the black part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "QNyvwLKRX88xPDGN2KymBW", "gt_response": "Right Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 312, "prompt": "What is the organ on the top of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "Zpiy9nhUDqpShqcaiqkWDA", "gt_response": "Stomach", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 313, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "jR9NWccW35qUrwHoygZqsD", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 314, "prompt": "What organ is the black hole in the upper right corner?\n<image>", "pred_response": "Liver", "answer_id": "YG3wHRhGET9fkuY6STtC6e", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 315, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "ZnW8zC8y55vZbcbVDnFW6J", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 316, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "GiDUxhkCUfXH5NsG8uuBfJ", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 317, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "EwC7XkpuBSrWyeuHbaeokN", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 318, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "4WUZp4L3bUvJBfKDKKUSot", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 319, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "8cXeWjjXdAftJg3ZAdNa3q", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 320, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "GuXHHLwCD8BA8pfRrW9A8H", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 321, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "6qzmTu8p7VXyAX628TJWDy", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 322, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "a6YKDmdFruscSPWNCqCfUW", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 323, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "8P7WNfPovmhZ5uw96UUe3g", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 324, "prompt": "Which is bigger in this image, liver or spleen?\n<image>", "pred_response": "Liver", "answer_id": "fnGrmQtyf95P8wjZJxcutn", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 325, "prompt": "What is the rightmost organ in this image?\n<image>", "pred_response": "Liver", "answer_id": "2oWCGKc9isaM57GHTjLtHg", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 326, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "U37zLguZPH4xwmiaxJjTMS", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 327, "prompt": "What organ is the black part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "L7kvXA5Gg2Nu5dPtB8fysV", "gt_response": "Right Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 328, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "XDDQZbsCpNCRdZS9zd5hca", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 329, "prompt": "What organ is the black hole in the upper right corner?\n<image>", "pred_response": "Liver", "answer_id": "ksV6XgHQaPtnvuDX8mZY5t", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 330, "prompt": "What type of medical image is this?\n<image>", "pred_response": "X-Ray", "answer_id": "CsQiUZJ9n4RjSzQyNrgr5a", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 331, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Plane 2", "answer_id": "SbJMHXeNRjR9RhVWiwZaGw", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 332, "prompt": "Which part do the organs in this image locate from the aspect of the whole body?\n<image>", "pred_response": "Chest", "answer_id": "NXHzrWSAofbD9KdogzSMo8", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 333, "prompt": "Which side of lung is abnormal in this image,left or right?\n<image>", "pred_response": "Left", "answer_id": "2uJ9voAi88mBR8sCRBx7U2", "gt_response": "Right", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 334, "prompt": "What part of the lung is the pneumothorax located in?\n<image>", "pred_response": "Right", "answer_id": "56e92fT8aZjqJmK6KvPkk9", "gt_response": "Upper Right Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 335, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Chest", "answer_id": "fJjGyuWBRoYiRWX7tVShN9", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 336, "prompt": "Which organs appear in pairs?\n<image>", "pred_response": "Lung", "answer_id": "mNzrNPkyvHpBvwSXiRRKNJ", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 337, "prompt": "Is/Are there abnormalities in the patient's right lung?\n<image>", "pred_response": "Yes", "answer_id": "iGS6cB4VqeWm5ymPN2YBTv", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 338, "prompt": "Is this image normal?\n<image>", "pred_response": "No", "answer_id": "FthXkoPcx7ADvC6hWvU9Em", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 339, "prompt": "Can pneumothorax be observed on the lower left lung in this picture?\n<image>", "pred_response": "No", "answer_id": "bjYveBpXog2MLaMHMCTFDL", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 340, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "Black", "answer_id": "dLRAYTRdJdZtkpYiUwxrCS", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 341, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "3txqEQR9ue3DdYfC32BBkj", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 342, "prompt": "What is the effect of the main organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "3eTKJ5tL2mDjrBxxQSNt2B", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 343, "prompt": "What kind of symptoms will the disease on the upper right of lung bring to the patient?\n<image>", "pred_response": "Chest pain, cough, shortness of breath", "answer_id": "6uCVaWjv8gn2a9uYTLqQgf", "gt_response": "Chest pain, dyspnea", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 344, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "GVEddzNSAoqADA9TtktELj", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 345, "prompt": "What imaging modality was used?\n<image>", "pred_response": "X-Ray", "answer_id": "9EfPLyjshotE8iMfXqkBD9", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 346, "prompt": "Is this a transverse plane?\n<image>", "pred_response": "No", "answer_id": "TsyBDyjiukoh39uJZ68fsk", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 347, "prompt": "Which part of the human body is the organ located in the image?\n<image>", "pred_response": "Chest", "answer_id": "dHDhh5CTqQPwKEcLyXFwdB", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 348, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Left Lung", "answer_id": "22gwUHr3F2rnqUEKN2QMjs", "gt_response": "Lower Right Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 349, "prompt": "Where is the atelectasis in this image?\n<image>", "pred_response": "Right Lung", "answer_id": "mvXwgo8QpPUoWibYJsJwMY", "gt_response": "Lower Right Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 350, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Chest", "answer_id": "ArXjdWYJfSptJ5gBX4Jpow", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 351, "prompt": "Is this a study of the chest?\n<image>", "pred_response": "Yes", "answer_id": "Btsx8jnoYj6HXWXiUUJvok", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 352, "prompt": "What disease is/are shown on the right of lung?\n<image>", "pred_response": "Atelectasis", "answer_id": "DMiDDFJXsSpEBTa7rkrhSb", "gt_response": "Atelectasis", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 353, "prompt": "Is/Are there abnormalities in the patient's right lung?\n<image>", "pred_response": "Yes", "answer_id": "44zrwMSReX5DgzTtHyuK3C", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 354, "prompt": "Can atelectasis be observed on the upper right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "3zgfdGiCDEPkeUsRjrndts", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 355, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "Black", "answer_id": "6iHHoWQ6eo4QYk6vBvB5Zc", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 356, "prompt": "How many existing lungs in this image?\n<image>", "pred_response": "2", "answer_id": "HrMshieNSMM5MYuNzPDpEZ", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 357, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "7mF3KCPHoEkExoqnV5LojE", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 358, "prompt": "What is the main cause of the disease on the lower right of the lung in the picture?\n<image>", "pred_response": "Atelectasis", "answer_id": "8ZmanxedC5q7Xki6bAoef5", "gt_response": "Bronchial obstruction", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 359, "prompt": "Which organs/organ in the picture belong to the circulatory system?\n<image>", "pred_response": "Heart", "answer_id": "gkKAbBb58vrd3U2RBpmnX8", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 360, "prompt": "Is this image taken via X-Ray?\n<image>", "pred_response": "Yes", "answer_id": "QrQLh436NM9fDoEpoVCVpi", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 361, "prompt": "Is this a transverse plane?\n<image>", "pred_response": "No", "answer_id": "YpWHYdNbDNAzYVoSEPjkMv", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 362, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "dP4PnjzwH7FBYL9fMaeex6", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 363, "prompt": "Which side of lung is abnormal in this image,left or right?\n<image>", "pred_response": "Right", "answer_id": "5hTkbvgFDo5hCeXCBBkKRr", "gt_response": "Left", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 364, "prompt": "Where is the mass in this image?\n<image>", "pred_response": "Left Lung", "answer_id": "Y5onP4EzvftbAgqndKe6Zn", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 365, "prompt": "Is this a study of the chest?\n<image>", "pred_response": "Yes", "answer_id": "ZQuSi4waekoEdMnLXViRmG", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 366, "prompt": "Is this a study of the pelvic cavity?\n<image>", "pred_response": "No", "answer_id": "dKLKpJwpWUo85yUguq9RYF", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 367, "prompt": "What disease is/are shown on the left of lung?\n<image>", "pred_response": "Lung Cancer", "answer_id": "Jjxn2Uarrgxu6LgVEmJYBq", "gt_response": "Pulmonary Mass", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 368, "prompt": "Is the lung abnormal?\n<image>", "pred_response": "Yes", "answer_id": "9EuUQj7qn7eZZHeJG3ngz6", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 369, "prompt": "Can mass be observed on the lower left lung in this picture?\n<image>", "pred_response": "Yes", "answer_id": "QGgVJJcUjUjazjwHrLKSgK", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 370, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "Black", "answer_id": "PJ9SWWJpdcjsXJKnDKUGop", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 371, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "1", "answer_id": "ZzhNKksbST8uLroJE42KSv", "gt_response": "1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 372, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "RYAc5nuh4LzTWFNgzKjwYZ", "gt_response": "A Little", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 373, "prompt": "How to prevent the occurrence of the disease on the lower left lobe of lung in this image?\n<image>", "pred_response": "Avoid smoking, avoid exposure to pollutants, maintain physical fitness", "answer_id": "mNUeoDbBVeM9cQu7gxKKKw", "gt_response": "Keep healthy", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 374, "prompt": "Which organs/organ in the picture belong to the circulatory system?\n<image>", "pred_response": "Heart", "answer_id": "9iRocaH6i6HXAYQafaE5Yw", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 375, "prompt": "What type of medical image is this?\n<image>", "pred_response": "X-Ray", "answer_id": "42kyMjShmMkH9DjJwBCzF6", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 376, "prompt": "Is this a coronal section?\n<image>", "pred_response": "No", "answer_id": "c9dfUjVnY3TNymNUCzif9c", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 377, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "Gc3RPa7io3gBB3D5SJi5eu", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 378, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Left Lung", "answer_id": "NBkR6zApxbxrSaZsbaz6fJ", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 379, "prompt": "Where is the atelectasis in this image?\n<image>", "pred_response": "Right Lung", "answer_id": "fXfc8o5QLjWuXdWt2wqudn", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 380, "prompt": "Is this a study of the neck?\n<image>", "pred_response": "No", "answer_id": "99jmoePscebbzihBekdAWS", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 381, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "Aopuktn3zYzszSderP9pGx", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 382, "prompt": "Does the patient have any abnormalities?\n<image>", "pred_response": "Yes", "answer_id": "kCXi2yL8JTmDjcK3Gh29bN", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 383, "prompt": "Is this image normal?\n<image>", "pred_response": "No", "answer_id": "2J4RH2G5WcBcHRcB2rp8D6", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 384, "prompt": "Can atelectasis be observed on the lower left lung in this picture?\n<image>", "pred_response": "No", "answer_id": "dAGRADEysjG4skDdEWPqqL", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 385, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "Black", "answer_id": "XxJPgJpSLVmPuv79BSrQZ9", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 386, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "dwku7ZGCJeAM2wSfJQbUmF", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 387, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "faLnyPbqHdfVXKwkEqpGPY", "gt_response": "A Little", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 388, "prompt": "How to treat the disease located on the left of lung in this image?\n<image>", "pred_response": "Medical treatment", "answer_id": "gW4xgPp7cYZsv2Qm3vasHf", "gt_response": "Physical therapy, surgical treatment", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 389, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "K9ujTazhcjk5vpezdcGXjL", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 390, "prompt": "Is this image taken via X-Ray?\n<image>", "pred_response": "Yes", "answer_id": "cAGKWWZfyqWkXHo9fWm2gu", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 391, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Coronal Plane", "answer_id": "krcxhWu6a9eMCAE5YgqFbW", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 392, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "7b5zRSzLP3D9i5uf3tYN5z", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 393, "prompt": "Which side of lung is abnormal in this image,left or right?\n<image>", "pred_response": "Right", "answer_id": "VnFvNqpaMQUrHjebQ6fPnY", "gt_response": "Right", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 394, "prompt": "Where is the pneumonia in the lung?\n<image>", "pred_response": "Right Lung", "answer_id": "Zy4jvmCerFyrPke4xdZ2kX", "gt_response": "Right Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 395, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Chest", "answer_id": "9Nm5rmrxm9WCXzgPLby3kR", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 396, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Chest", "answer_id": "i7h7quKTNM2K9jbB8GFBvk", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 397, "prompt": "What disease is/are shown on the right of lung?\n<image>", "pred_response": "Pulmonary Edema", "answer_id": "Dw7dmeXLEC3gKLtvwppN64", "gt_response": "Pneumonia", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 398, "prompt": "Does the lung look abnormal?\n<image>", "pred_response": "Yes", "answer_id": "NreffRvmztYAEPb9vkhqsj", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 399, "prompt": "Can pneumonia be observed on the right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "HoTmDepRom7FKGXKbpC2QC", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 400, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "Black", "answer_id": "X9RFDWbKbXhtpVGKRSZZPf", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 401, "prompt": "How many existing lungs in this image?\n<image>", "pred_response": "2", "answer_id": "WJAbdAzQTfsgYJCVqHMa6o", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 402, "prompt": "What can the organ with black color in this image be used for?\n<image>", "pred_response": "Breathe", "answer_id": "fij5ubxvJAwG2jFsg6DwxN", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 403, "prompt": "How to treat the disease located on the right of lung in this image?\n<image>", "pred_response": "Medical treatment", "answer_id": "aoWzCnKGNSLMDUvVuxDtiC", "gt_response": "Medical therapy, supportive therapy", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 404, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Heart", "answer_id": "BJKVEzkbG6P2GnL2ETDMQd", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 405, "prompt": "What modality is shown in this image?\n<image>", "pred_response": "X-Ray", "answer_id": "5KsUiuSVDMQMGFZubwbvsT", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 406, "prompt": "Which plane is the image scanned, transverse plane or coronal plane?\n<image>", "pred_response": "Transverse Plane", "answer_id": "3sKtPPqtwAzeeKu4djZbju", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 407, "prompt": "Do the organs in the image exist in the abdomen?\n<image>", "pred_response": "No", "answer_id": "iHnyCL8WNDHSWKE6VvU3JB", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 408, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Left Lung", "answer_id": "QzHyKuE4q93HEMYrDnHgN4", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 409, "prompt": "Where is the cardiomegaly in this image?\n<image>", "pred_response": "Left Lower Lung", "answer_id": "V7Aj2bdAwcn4wbUPX6Dut8", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 410, "prompt": "Which organs appear in pairs?\n<image>", "pred_response": "Lung", "answer_id": "F72PAyiJLtkCBFJqpvLcvQ", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 411, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Chest", "answer_id": "hfdwZZFc2FZJx9dEx3Y4Qr", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 412, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Pulmonary Edema", "answer_id": "8nt7UY3fgeZoD3uFaR6ssk", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 413, "prompt": "Is this image normal?\n<image>", "pred_response": "No", "answer_id": "nSqR9RewXKfa3ECyL7Qsre", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 414, "prompt": "Can cardiomegaly be observed on the heart in this picture?\n<image>", "pred_response": "No", "answer_id": "8zNERz2Fz6P4h9LuK5ifzF", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 415, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "Black", "answer_id": "kW5Sq5iRCM7WKgKVeTjbg7", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 416, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "2", "answer_id": "ayrNZoo3oQ7EncBZrDAiHD", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 417, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "StNgH2RkYmrCcRHiS2gyJq", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 418, "prompt": "What kind of symptoms will the disease on the heart in this image bring to the patient?\n<image>", "pred_response": "Chest pain, shortness of breath, fatigue", "answer_id": "YpadUVfvyhvnqNuhhZoGha", "gt_response": "Chest tightness, arrhythmia", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 419, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Heart", "answer_id": "BtjRhXn9n5Q7SUX6qtX22g", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 420, "prompt": "What type of medical image is this?\n<image>", "pred_response": "X-Ray", "answer_id": "cctshtKqkPipiHJ5VEJHMv", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 421, "prompt": "Is this sagittal plane?\n<image>", "pred_response": "No", "answer_id": "ThtkSNKPRZ9qj2DL3uNAfK", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 422, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "cr2F3BBzxDjKbbrLzVsfgc", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 423, "prompt": "Which lobe is abnormal in this image?\n<image>", "pred_response": "Right", "answer_id": "8Zy4zaiV4fQNbPhszQXMaG", "gt_response": "Left", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 424, "prompt": "Where is the pulmonary infiltration in this image?\n<image>", "pred_response": "Right Lung", "answer_id": "MDZEbEZMufCpad9yhxLtFk", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 425, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Chest", "answer_id": "BFCQAKTsRSucSQRQoA2AB4", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 426, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Chest", "answer_id": "Gp56n3cFGVrLgSzMhitF64", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 427, "prompt": "Does the lung look abnormal?\n<image>", "pred_response": "Yes", "answer_id": "eZ26V2SRn7t7PFEMEgwaAr", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 428, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Pneumothorax", "answer_id": "c3FoQ5rNtkx8zmT5SbTAR2", "gt_response": "Pulmonary Infiltration", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 429, "prompt": "Can pulmonary infiltration be observed on the upper right lung in this picture?\n<image>", "pred_response": "Yes", "answer_id": "m8SVMWobwRmf5hx8ZaHH3a", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 430, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "Black", "answer_id": "i5W9452sWjm4f2HqbpBUfu", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 431, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "2", "answer_id": "8qKBmTHE68sPwkcpqseSTn", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 432, "prompt": "What is the effect of the main organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "BkrLT8pYr3i33G8dARgkDd", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 433, "prompt": "How to prevent the occurrence of the disease on the lower left lobe of lung in this image?\n<image>", "pred_response": "Avoid smoking, avoid exposure to pollutants, avoid exposure to infections, maintain physical fitness", "answer_id": "Xdn5hPcdLQKcio2TFYrmaS", "gt_response": "Live healthy, enhance physical fitness", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 434, "prompt": "Which organs/organ is part of the respiratory system in this image?\n<image>", "pred_response": "Lung", "answer_id": "iab2qBFgCWthFhK8gwbdKs", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 435, "prompt": "What imaging modality was used?\n<image>", "pred_response": "X-Ray", "answer_id": "WvWrg3iD72HyaupqoAzkK5", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 436, "prompt": "Which plane is the image scanned, transverse plane or coronal plane?\n<image>", "pred_response": "Transverse Plane", "answer_id": "QuwaxVLkeSGq5EJzzUZtA9", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 437, "prompt": "Which part do the organs in this image locate from the aspect of the whole body?\n<image>", "pred_response": "Chest", "answer_id": "9CAr85Y72u5jACifAoHUjP", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 438, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Left Lung", "answer_id": "jS9qAu4t9XMLShGPv6WNBQ", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 439, "prompt": "Where is the pulmonary nodule located in this image?\n<image>", "pred_response": "Right Lung, Lower Right", "answer_id": "YqFjaNw9tt8HvDKRBodzbz", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 440, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Chest", "answer_id": "CBvN4vkz5Zdni8Chz2RVEL", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 441, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Chest", "answer_id": "MaVSASu8UR7WVqp5Kr4Eab", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 442, "prompt": "Is the lung normal appearing?\n<image>", "pred_response": "No", "answer_id": "JZP2RTgfe3yHaDtHopqCku", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 443, "prompt": "Is this image normal?\n<image>", "pred_response": "No", "answer_id": "iPSrDssbBamSg52hk4jdfw", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 444, "prompt": "Can nodule be observed on the lower left lung in this picture?\n<image>", "pred_response": "No", "answer_id": "YiPsYbcsyUSRavQz29jTwT", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 445, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "Black", "answer_id": "3TvbPmoju5yxF5SyZefQtV", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 446, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "nuGzA3MpvztzrZizg2omjU", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 447, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "KDwV3njjZWHVYcXCypqtUF", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 448, "prompt": "How to treat the disease located on the lower left of lung in this image?\n<image>", "pred_response": "Medical treatment", "answer_id": "59kTjZgRzSsdUHUpi3yU5L", "gt_response": "Medical treatment, surgical treatment", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 449, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Heart", "answer_id": "Eq52QJB4q5iHviVoSdffp9", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 450, "prompt": "What imaging modality was used?\n<image>", "pred_response": "X-Ray", "answer_id": "E6Js6aboey585BLTVoAHnm", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 451, "prompt": "Which plane is the image scanned, transverse plane or coronal plane?\n<image>", "pred_response": "Transverse Plane", "answer_id": "jVrsNMvv8oGMwjkdUBzZrb", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 452, "prompt": "Which part of the human body is the organ located in the image?\n<image>", "pred_response": "Chest", "answer_id": "fdR5AvaqyngactkAFAeZm8", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 453, "prompt": "Where is/are the abnormality located in this picture?\n<image>", "pred_response": "Left Lung", "answer_id": "UXvcKXxtrgMARWq8zQTptW", "gt_response": "Upper Right Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 454, "prompt": "Where is the nodule in this image?\n<image>", "pred_response": "Right Lung", "answer_id": "6cZX3kskzJtKoAaJ8v3dSo", "gt_response": "Upper Right Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 455, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Chest", "answer_id": "3UUBCa2mkK8TA2up5jVq2i", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 456, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Chest", "answer_id": "3rrTndopDUZuSMP9sgiaVX", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 457, "prompt": "Does the patient have any abnormalities?\n<image>", "pred_response": "Yes", "answer_id": "M3sHex6hgTUmp4whqa48jx", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 458, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "St43AKAhnXZsjCvY8HngoJ", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 459, "prompt": "Can nodule be observed on the upper right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "BRUmDXf5pfbsxbUfUoc6Qi", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 460, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "Black", "answer_id": "YhzBxAuTmEZpVwK7ssz7NS", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 461, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "9syKw4uE2fWzC7ujMewNtK", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 462, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "Qnb2ihLVpFzrXGkVU4VzBB", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 463, "prompt": "What kind of symptoms will the disease on the upper right lung bring to the patient?\n<image>", "pred_response": "Chest pain", "answer_id": "EiDNLH2AVed57AWN8WEZmj", "gt_response": "Chest tightness, fatigue", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 464, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "Ks6nC2xcTLATFcRyvto8Fg", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 465, "prompt": "Is this an MRI scan?\n<image>", "pred_response": "No", "answer_id": "UxJk95MdA2saFbAyFyrXQn", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 466, "prompt": "Is this a coronal plane?\n<image>", "pred_response": "No", "answer_id": "geprK7oPXbMfxPyuBcannZ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 467, "prompt": "Do the organs in the image exist in the chest?\n<image>", "pred_response": "Yes", "answer_id": "koEgQpNpiCkHRQasPpxGDx", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 468, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Left Lung", "answer_id": "RZfSTUqXghf98DiJNPvkoY", "gt_response": "Upper Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 469, "prompt": "What part of the lung is the pneumothorax located in?\n<image>", "pred_response": "Right", "answer_id": "CNwKvvrkNMXKtcVRtGdzsm", "gt_response": "Upper Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 470, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Chest", "answer_id": "fcoVE3sgPTunQns2cscAi7", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 471, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Chest", "answer_id": "msSiBhchuBoNaPmtoUyw62", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 472, "prompt": "Is the lung normal appearing?\n<image>", "pred_response": "No", "answer_id": "JqnjbiMwaT74ApwvCnhmqZ", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 473, "prompt": "Is/Are there abnormalities in the patient's left lung?\n<image>", "pred_response": "Yes", "answer_id": "CXEsiFvz2pU86gXQgjPwGe", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 474, "prompt": "Can pneumothorax be observed on the lower right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "g3SAvzmQRgGStLjKNymc3K", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 475, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "Black", "answer_id": "5eM4khhsJwSEadj8xnbhoa", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 476, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "2", "answer_id": "hTPsR5igzZpQSYTHmMftEo", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 477, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "6LXJxudfV4gRkHCyCPiiqi", "gt_response": "A Little", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 478, "prompt": "How to treat the disease located on the upper left of lung in this image?\n<image>", "pred_response": "Medical treatment", "answer_id": "SeQfeY3jjHs3MSWWbe27bs", "gt_response": "Medical therapy, supportive therapy", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 479, "prompt": "Which organs/organ in the picture belong to the circulatory system?\n<image>", "pred_response": "Heart", "answer_id": "UYQew4QZzdKwKe5MrTRePh", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 480, "prompt": "Is this a X-Ray?\n<image>", "pred_response": "Yes", "answer_id": "aJrVqH74z6LNziB3GnSEdM", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 481, "prompt": "What scanning plane does this image belong to?\n<image>", "pred_response": "Plane 2", "answer_id": "3sFihw7nhTfEwtGBhGmyun", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 482, "prompt": "Do the organs in the image exist in the abdomen?\n<image>", "pred_response": "No", "answer_id": "2egj76gM2THfP4tuFaSt5B", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 483, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Left Lung, Right Lung, Left Heart, Right Heart", "answer_id": "98Wew58Y5pwhxyd2CyZ2XP", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 484, "prompt": "Where is the pneumothorax in this image?\n<image>", "pred_response": "Right Lung", "answer_id": "7e8Gzy8uKGCfPtmuu8gTnD", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 485, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Chest", "answer_id": "gkcivXqMM9yrQgLSgNeNbN", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 486, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Chest", "answer_id": "moduUHVtikwGpbpyZ3GRzn", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 487, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "fSoExXRzoJx6Mju2vH3nNb", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 488, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "L28vA3JrxMM7pEgFLmKD3m", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 489, "prompt": "Can pneumothorax be observed on the lower left lung in this picture?\n<image>", "pred_response": "No", "answer_id": "FMM3fhX9zFTHaBM4sVA7g8", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 490, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "Black", "answer_id": "oQcRPLiYnB5yyn5tGWfUTb", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 491, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "1", "answer_id": "7tApLrR5Yjr5oWpU8BoJqu", "gt_response": "1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 492, "prompt": "What is the effect of the center organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "dntcmmWaZ3jYfiFx9pTNdu", "gt_response": "Promote blood flow", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 493, "prompt": "What is the main cause of the disease on the lower left of the lung in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "ayxvTwuDoWvckZADZDLKMn", "gt_response": "Chest injury, lung disease, bullae", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 494, "prompt": "Which organs/organ in the picture belong to the circulatory system?\n<image>", "pred_response": "Heart", "answer_id": "ghLvi9agZ2oR4ooWFnf3Po", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 495, "prompt": "Is this image taken via X-Ray?\n<image>", "pred_response": "Yes", "answer_id": "Wtidd2hocsjMJqh4t5aos7", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 496, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse Plane", "answer_id": "mPaPBgLkHn5K2Sioptktnn", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 497, "prompt": "Which part of the human body is the organ located in the image?\n<image>", "pred_response": "Chest", "answer_id": "ByGGBFvBZTeZ6ZL29jQfXj", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 498, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Lower Left Lung", "answer_id": "aP5dJamRupRLtWkHAzcsNf", "gt_response": "Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 499, "prompt": "Where is/are the nodule located in this image?\n<image>", "pred_response": "Right Lung, Lower Right, Upper Right, Lower Left", "answer_id": "2GisJBXsrmqjzTFSTynC5y", "gt_response": "Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 500, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "4KqKfhsQbYZfej2RgkyiSx", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 501, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Chest", "answer_id": "CtRehjS4xTUnMvgKBSfsmM", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 502, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "9tpd3CEA5dCmvEFpjRpM2w", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 503, "prompt": "Is the lung abnormal?\n<image>", "pred_response": "Yes", "answer_id": "iXAmd2eVevP32dGiKjdfon", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 504, "prompt": "Can nodule be observed on the lower right lung in this picture?\n<image>", "pred_response": "Yes", "answer_id": "5cCXEJVgyLJqBFtMnTjwTX", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 505, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "Black", "answer_id": "9GoB9PSWtk7LR4i4taiqKL", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 506, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "2", "answer_id": "EhC3ghNFk7kVPQBDvNinTK", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 507, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "eiApuoPVu9SiPewhFp6egm", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 508, "prompt": "How to prevent the occurrence of the disease on the left lobe of lung in this image?\n<image>", "pred_response": "Avoid smoking, avoid infections, avoid exposure to pollutants", "answer_id": "2MGZNhhTjSGu6j8Q36egTJ", "gt_response": "Quit smoking, enhance physical fitness", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 509, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "JgoCsh7oPNBgV8sxAUHbh2", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 510, "prompt": "Is this image taken via X-Ray?\n<image>", "pred_response": "Yes", "answer_id": "jyydhKys3tA6XaBPCFgdRQ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 511, "prompt": "What scanning plane does this image belong to?\n<image>", "pred_response": "Plane 2", "answer_id": "F6VKaGRgC3aNaJnQWhekLi", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 512, "prompt": "Where does the picture show?\n<image>", "pred_response": "Chest", "answer_id": "8L8eppKDggrQMrgpQuq9r6", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 513, "prompt": "Which side of lung is abnormal in this image,left or right?\n<image>", "pred_response": "Right", "answer_id": "NqcAJDcydBhrv4kkjZf85G", "gt_response": "Left", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 514, "prompt": "What part of the lung is the pneumonia located in?\n<image>", "pred_response": "Right Lung", "answer_id": "PUidUW8WkVuj69emaiPMvr", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 515, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Chest", "answer_id": "FqW2LcbSFhgUaacqWKkGLB", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 516, "prompt": "Which organs appear in pairs?\n<image>", "pred_response": "Lung", "answer_id": "i2cu498NHYBF8b9NVBdqY2", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 517, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "ih65MYAsXcaAWAwDnD2nKm", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 518, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "jToTxzqdznLsegaeRssQfc", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 519, "prompt": "Can pneumonia be observed on the upper right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "Xjiq65RNtqvB62xicMjdvv", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 520, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "Black", "answer_id": "Cfet2Sb3H7maFeU6p8ASJ2", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 521, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "NvLWBEv55ENvddada3tXuc", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 522, "prompt": "What is the effect of the center organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "SYh9EjN49PrNs8JRo3ssfh", "gt_response": "Promote blood flow", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 523, "prompt": "How to treat the disease located on the lower left of lung in this image?\n<image>", "pred_response": "Medical treatment", "answer_id": "ZNyxYecDh2pD5RnaFjcHCG", "gt_response": "Medical therapy, supportive therapy", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 524, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "WFfVxsy6eK53TxSmQGCmpE", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 525, "prompt": "What type of medical image is this?\n<image>", "pred_response": "X-Ray", "answer_id": "VKeacpBiwdPJofhkcbhzd5", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 526, "prompt": "Is this a coronal plane?\n<image>", "pred_response": "No", "answer_id": "RN6tipi4U9PuwRSBwqjLUb", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 527, "prompt": "Which part do the organs in this image locate from the aspect of the whole body?\n<image>", "pred_response": "Chest", "answer_id": "QLhZe435knrmqEoEwKgjMQ", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 528, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Lower Left Lung", "answer_id": "ZF7LqUUV6Gz4NmZgkgz5wG", "gt_response": "Upper Right Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 529, "prompt": "What part of the lung is the pneumonia located in?\n<image>", "pred_response": "Right Lung", "answer_id": "MBHJysWJsfNm3fpirXASfg", "gt_response": "Upper Right Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 530, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Chest", "answer_id": "SKfFihHHmJm3YjXXWfBxxF", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 531, "prompt": "Is this a study of the neck?\n<image>", "pred_response": "No", "answer_id": "c8sEisgyLFfR3hEznBzMCm", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 532, "prompt": "Does the lung look abnormal?\n<image>", "pred_response": "Yes", "answer_id": "hXut2u3AG7go2CWjWWBk78", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 533, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "K3ttFue6VPHRWzbsLLkhUN", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 534, "prompt": "Can pneumonia be observed on the upper right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "CJsWhZtfd2nqMZi4oExDJA", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 535, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "Black", "answer_id": "ZvS6V5XZMNU4DsgFK4iYJZ", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 536, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "1", "answer_id": "SJoW25ZueUrTR2b7sA2yEg", "gt_response": "1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 537, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "ckBWduinm9fM2Ax97sjTwV", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 538, "prompt": "How to prevent the occurrence of the disease on the upper right lobe of lung in this image?\n<image>", "pred_response": "Avoid smoking, avoid exposure to air pollution, avoid exposure to chemicals, avoid exposure to infections, maintain physical fitness, maintain healthy diet, maintain healthy weight", "answer_id": "CcsHP3x6MzWHQLg9TX8Yrz", "gt_response": "Pay attention to prevent cold and keep warm, enhance physical fitness", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 539, "prompt": "Which organs/organ is part of the respiratory system in this image?\n<image>", "pred_response": "Lung", "answer_id": "NiEPn8Ssp6Hzyn4ygpGXzK", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 540, "prompt": "How was this image taken?\n<image>", "pred_response": "X-Ray", "answer_id": "5BNxiyF23safoweWLobNQo", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 541, "prompt": "Is this a transverse section?\n<image>", "pred_response": "No", "answer_id": "jYxRvPAbK36VLoFzjHtn8k", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 542, "prompt": "Where does the picture show?\n<image>", "pred_response": "Chest", "answer_id": "jJvMhZmCwEaCPVsTWcYTBZ", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 543, "prompt": "Which side of lung is abnormal in this image?\n<image>", "pred_response": "Left", "answer_id": "D3KCPYjQz4vpqoYhdNyuNR", "gt_response": "Right", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 544, "prompt": "Where is the pneumonia in the lung?\n<image>", "pred_response": "Right Lung", "answer_id": "2FJ62fdxEQioTtQg3U7phv", "gt_response": "Lower Right Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 545, "prompt": "Is this a study of the chest?\n<image>", "pred_response": "Yes", "answer_id": "EqubrftJM9VXALMsdrs7KU", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 546, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Chest", "answer_id": "h5DAd4NaqgoWabpYFyE2b5", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 547, "prompt": "What disease is/are shown on the right of lung?\n<image>", "pred_response": "Pulmonary Edema", "answer_id": "Xwf4HSTaUmeyu7qk6GxrTr", "gt_response": "Pneumonia", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 548, "prompt": "Is the lung normal appearing?\n<image>", "pred_response": "No", "answer_id": "Fp2xjVzxyCw7MsV43iRfWF", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 549, "prompt": "Can pneumonia be observed on the lower right lung in this picture?\n<image>", "pred_response": "Yes", "answer_id": "b97RV6XLa637TwfzXtrHsG", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 550, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "Black", "answer_id": "iuXbMtpbR6PsFLSK8TP9ve", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 551, "prompt": "How many existing lungs in this image?\n<image>", "pred_response": "2", "answer_id": "dbiBEAvTEU9sTeKRxjJp7e", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 552, "prompt": "What is the effect of the center organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "muqpqwFBViDaygtWwyjeW9", "gt_response": "Promote blood flow", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 553, "prompt": "How to prevent the occurrence of the disease on the lower right lobe of lung in this image?\n<image>", "pred_response": "Avoid smoking, maintain physical fitness, and avoid exposure to air pollutants", "answer_id": "Kw2stdpT3pdJ6mVNjzDtoJ", "gt_response": "Pay attention to prevent cold and keep warm, enhance physical fitness", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 554, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Heart", "answer_id": "5jtrjKTaD6XVZuBJV8qwiw", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 555, "prompt": "Is this image taken via CT?\n<image>", "pred_response": "No", "answer_id": "XsyLEEJuqGU5wKHPDoqqaA", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 556, "prompt": "Is this a transverse plane?\n<image>", "pred_response": "No", "answer_id": "WSapbZMroLYFzonH4S5Pfb", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 557, "prompt": "Where does the picture show?\n<image>", "pred_response": "Chest", "answer_id": "FxbVmw5xSBvo9AeBqKQMnH", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 558, "prompt": "Which side of lung is abnormal in this image?\n<image>", "pred_response": "Right", "answer_id": "Cm3P6uM2q9EryHJguVEKRd", "gt_response": "Left", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 559, "prompt": "What part of the lung is the infiltration located in?\n<image>", "pred_response": "Right", "answer_id": "nGH9JtwjmJ7rojgd745AjD", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 560, "prompt": "Which organs appear in pairs?\n<image>", "pred_response": "Lung", "answer_id": "GQdNgPgPdDoznRodx4D3mz", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 561, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "VXzZArF5fPj89XDiPyC5jz", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 562, "prompt": "Is/Are there abnormalities in the patient's left lung?\n<image>", "pred_response": "Yes", "answer_id": "XysMFWFydC268Cz227MaQc", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 563, "prompt": "What disease is/are shown on the lower left of lung?\n<image>", "pred_response": "Lung Cancer", "answer_id": "Y6d9LoeHu3Zxam74VAfgCn", "gt_response": "Pulmonary Infiltration", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 564, "prompt": "Can infiltration be observed on the lower left lung in this picture?\n<image>", "pred_response": "No", "answer_id": "S4FbTXrXiQSSqBPHA3TcUp", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 565, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "Black", "answer_id": "CduG4qRWCsggGQ7D7qzKSK", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 566, "prompt": "How many existing lungs in this image?\n<image>", "pred_response": "2", "answer_id": "9oxsM5hxjzEUoVv4u6BGiy", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 567, "prompt": "What can the organ with black color in this image be used for?\n<image>", "pred_response": "Breathe", "answer_id": "mZYUC3no6fmwzEEcmPpDbP", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 568, "prompt": "What kind of symptoms will the disease on the lower left of lung bring to the patient?\n<image>", "pred_response": "Chest pain, cough, shortness of breath", "answer_id": "jg2r9apTkYqvM54u9H8LkM", "gt_response": "Chest pain, cough, expectoration", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 569, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Heart", "answer_id": "DXDPgGjKeiwDpZC6ARrXsV", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 570, "prompt": "Is this image taken via X-Ray?\n<image>", "pred_response": "Yes", "answer_id": "HkSbAeFjN3iyUXvL6ab4ev", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 571, "prompt": "Is this a coronal section?\n<image>", "pred_response": "No", "answer_id": "4YFpod5UTXYR8XtiwzLj9T", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 572, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Chest", "answer_id": "7wyUJJjeJAVdYqM8eurfHH", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 573, "prompt": "Which part of chest is abnormal in this image?\n<image>", "pred_response": "Right", "answer_id": "fKxkfU9LPv9J5YjrkLfsKu", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 574, "prompt": "Where is/are the effusion located?\n<image>", "pred_response": "Right Lung", "answer_id": "5ZCuFeX5i4TkiGZoQPJuz5", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 575, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Chest", "answer_id": "bguPu7aUESnuKqrNmsA3kG", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 576, "prompt": "Is this a study of the head?\n<image>", "pred_response": "No", "answer_id": "AfmKtgf54cJrvTtFmhbPB2", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 577, "prompt": "Does the patient have any abnormalities?\n<image>", "pred_response": "Yes", "answer_id": "jpWeHS9PyPdAVqKQfUcKdJ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 578, "prompt": "Does the lung look abnormal?\n<image>", "pred_response": "Yes", "answer_id": "gjd5nGUKciKsijEfQ7TUfH", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 579, "prompt": "Can effusion be observed on the lower left lung in this picture?\n<image>", "pred_response": "No", "answer_id": "TgKh4Eumci9pNaWFdT4T83", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 580, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "Black", "answer_id": "DsND3dspmRqQwFWjSUz5vP", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 581, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "AmS66bayTLmUgmv8XgtX6b", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 582, "prompt": "What can the organ with black color in this image be used for?\n<image>", "pred_response": "Breathe", "answer_id": "JUykxJbeDpm3WdiNabhRVz", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 583, "prompt": "What is the main cause of the disease on the lower left of the lung in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "icRtCqkRu2HxkLPXmnfAKs", "gt_response": "Inflammation, malignant tumor, trauma, etc", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 584, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Heart", "answer_id": "Xo3m69NwuyoEajPALgaBrM", "gt_response": "Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 585, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "X-Ray", "answer_id": "4CQy4W4m3B3MrkPerhUpwB", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 586, "prompt": "What scanning plane does this image belong to?\n<image>", "pred_response": "Plane 2", "answer_id": "GWt9Y6PNZwVipNVFfsx9pd", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 587, "prompt": "Do the organs in the image exist in the chest?\n<image>", "pred_response": "Yes", "answer_id": "FoEhjUCzvffvRsqmtK64Ai", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 588, "prompt": "Where is the heart in this image?\n<image>", "pred_response": "Left", "answer_id": "3yx6CKGt2QHfgkqjwrztUE", "gt_response": "Center", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 589, "prompt": "Where is the pleural effusion?\n<image>", "pred_response": "Left", "answer_id": "PMBvmwaGVV2f6ezW55Zb6D", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 590, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "UwQ2vx5ds6PaLJrjcenapA", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 591, "prompt": "Is this a study of the pelvic cavity?\n<image>", "pred_response": "No", "answer_id": "8EBComsq7oTzoBNpGPyWmp", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 592, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Pulmonary Edema", "answer_id": "WfkK2pxEBzkMSZBQ9HP8h8", "gt_response": "Pleural Effusion", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 593, "prompt": "Is this image normal?\n<image>", "pred_response": "No", "answer_id": "gQyZx6ZYZYffnDPPHg59pC", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 594, "prompt": "Can effusion be observed on the lower left lung in this picture?\n<image>", "pred_response": "No", "answer_id": "Gq2hjiYutwigDgoLzYfSpo", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 595, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "Black", "answer_id": "iz2F6ihvVmXx382EyoTLbQ", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 596, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "2", "answer_id": "VG3srpPtJeh7wsNPgoed6E", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 597, "prompt": "What is the effect of the main organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "nDj35AM8r5Xuv5iUjj4ymK", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 598, "prompt": "What kind of symptoms will the disease on the lower left of lung bring to the patient?\n<image>", "pred_response": "Chest pain", "answer_id": "Caqy9hFgvmrWyYWGmDSj5y", "gt_response": "Chest tightness, dyspnea, chest pain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 599, "prompt": "Which organs/organ is part of the respiratory system in this image?\n<image>", "pred_response": "Lung", "answer_id": "BkAMaaQnYmErhzKW3K78TL", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 600, "prompt": "Which type of modality is shown about this image, MRI, CT or X-Ray?\n<image>", "pred_response": "MRI", "answer_id": "DxdeM8X3QuSA2V7jmXjpyd", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 601, "prompt": "Is this sagittal plane?\n<image>", "pred_response": "No", "answer_id": "TZGQ96uwtmA2azQtVpFDNm", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 602, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "R6KeHkuPqJxvAi9fx3hmCi", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 603, "prompt": "Where is the brain stem in this image?\n<image>", "pred_response": "Left", "answer_id": "drwfvfYcMmeXopbTZAcDqK", "gt_response": "Center", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 604, "prompt": "Which organs appear in pairs?\n<image>", "pred_response": "2", "answer_id": "fs9Euqysroa2PuJu6S4SV6", "gt_response": "Temporal Lobe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 605, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "W6caeyP4Cq76yZMbZmBHJ4", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 606, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "KjJDKrRJ9kZMeyXvrQa9rV", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 607, "prompt": "Does the temporal lobe appear in this image?\n<image>", "pred_response": "No", "answer_id": "fdfwWAzUgtokkG5kkRHQpq", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 608, "prompt": "What color is the right temporal lobe in the picture?\n<image>", "pred_response": "Black", "answer_id": "NZg8fTtphX5U7hHZH7g2VJ", "gt_response": "Gray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 609, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "D2toZsg9QrGrfKf6tUezjg", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 610, "prompt": "How many ears are shown in this image?\n<image>", "pred_response": "1", "answer_id": "LziTbkSNBCx8bvzrhgqxgJ", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 611, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "ZVhRQjcrMhCwHststbp2iQ", "gt_response": "Much", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 612, "prompt": "Which organs/organ in the picture belong to the nervous system?\n<image>", "pred_response": "Brain", "answer_id": "PpQ4UFRRJ2psa9oJ88c5Vp", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 613, "prompt": "Are there some organs controlling heartbeat and breathing in this image?\n<image>", "pred_response": "No", "answer_id": "CSQYENkhCW6vpxJHCPLdNA", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 614, "prompt": "Do any of the organs in the picture belong to the respiratory system?\n<image>", "pred_response": "No", "answer_id": "59S6NkNEydmE8AfWsTubtd", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 615, "prompt": "Is this a CT?\n<image>", "pred_response": "No", "answer_id": "MEUsYqeJkfh239XKeoEYtT", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 616, "prompt": "Is this sagittal plane?\n<image>", "pred_response": "No", "answer_id": "9VNuAw4EhF3Gs4YFDDoj7X", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 617, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Head", "answer_id": "hRH5M3LRMbTfnM8bsvaUNr", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 618, "prompt": "Where is the brain stem in this image?\n<image>", "pred_response": "Lower Left", "answer_id": "9ebV9v6tvRQPoxNv3ytMQu", "gt_response": "Center", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 619, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Head", "answer_id": "7YipNBUZZParADa3LE6cQM", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 620, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Head", "answer_id": "c9rsXydWJovE7chDa9YyJ3", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 621, "prompt": "Does the ears exist in this picture?\n<image>", "pred_response": "No", "answer_id": "JSsQcuubUrfCENQmeuiZFe", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 622, "prompt": "Does the picture contain bladder?\n<image>", "pred_response": "No", "answer_id": "Ej2XgU35JquGvsQALswS3m", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 623, "prompt": "What color is the ears in the picture?\n<image>", "pred_response": "Black", "answer_id": "Dazv4Va8D548PPAxwM5K84", "gt_response": "White", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 624, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "HZxCTdWQpbaN5VPUqgvezU", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 625, "prompt": "How many temporal lobes in this image?\n<image>", "pred_response": "2", "answer_id": "A3a7JYDkgRF4DmNmzLBo9r", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 626, "prompt": "What is the function of the tissue on the center of this image?\n<image>", "pred_response": "Hearing", "answer_id": "Z9zP3FbTRNKHcRn4zTi8bg", "gt_response": "Control heartbeat and breathing", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 627, "prompt": "Does the picture contain the organ which has the effect of storing urine?\n<image>", "pred_response": "No", "answer_id": "TniA8z7JuqLTpmUDt7YZBH", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 628, "prompt": "Do any of the organs in the picture belong to the digestive system?\n<image>", "pred_response": "No", "answer_id": "8CARJMqYPHbTUEdi2CZXtt", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 629, "prompt": "What modality is shown in this image?\n<image>", "pred_response": "MRI", "answer_id": "aDcxBYh2FoJ5xrho5nCMPf", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 630, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse Plane", "answer_id": "Vrq7a4g8oMUcqtPqqPYFaf", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 631, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "7j6uKQsLcv35P2ixNJuMRe", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 632, "prompt": "Where is the tooth in this image?\n<image>", "pred_response": "Upper Right", "answer_id": "oMpKewXDqP7dzXNpbdtFER", "gt_response": "Top", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 633, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "nAGUfond3C3QGVYWaVdKGv", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 634, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Head", "answer_id": "DmAvJKtBbDsq539BGBjXt5", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 635, "prompt": "Does the picture contain mandible?\n<image>", "pred_response": "No", "answer_id": "RZk752QfLMFzsbK9TpnfV4", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 636, "prompt": "Does the parotid appear in the image?\n<image>", "pred_response": "No", "answer_id": "dhviWLhGMRaN8ddKQnaEU2", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 637, "prompt": "What color does the brain stem show in the picture?\n<image>", "pred_response": "Black", "answer_id": "PHtGsLbqKjYcAhseAijsTi", "gt_response": "Gray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 638, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "Rg4QDykeR54wyFRDZnFRFY", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 639, "prompt": "How many mandibles in this image?\n<image>", "pred_response": "2", "answer_id": "VtMyvScrwp6Fpv4yVk76MB", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 640, "prompt": "What is the function of the organ on the top of this image?\n<image>", "pred_response": "Hearing, smelling, tasting, and maintaining balance", "answer_id": "GANXLgwkJaFxWxMTqEVcaZ", "gt_response": "Cutting, chewing, maintaining facial contour and assisting pronunciation", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 641, "prompt": "Which organs/organ in the picture belong to nervous system?\n<image>", "pred_response": "Brain", "answer_id": "mtrwbEMETrtSA3QtqJsYZ5", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 642, "prompt": "Are there organs in the picture that can digest food?\n<image>", "pred_response": "No", "answer_id": "moMQmiUchSw4fMaNBtjjyh", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 643, "prompt": "Do any of the organs in the picture belong to the circulatory system?\n<image>", "pred_response": "No", "answer_id": "5quPw6pJZBFAd2wPQiwmsU", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 644, "prompt": "What modality is shown in this image?\n<image>", "pred_response": "CT", "answer_id": "RMRXUytrMQCfhgd9qDr96r", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 645, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse Plane", "answer_id": "Fb9XcQ7JNY2tMh5vSTrfxP", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 646, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "fFi2eGhhpkXAynseEmUuDa", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 647, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Head", "answer_id": "fH7xFLJL3X5VwtrFubV5gc", "gt_response": "Neck", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 648, "prompt": "Is there an esophagus in this image?\n<image>", "pred_response": "No", "answer_id": "3XxJukXdAzAj6VXRVcuXSJ", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 649, "prompt": "Does the trachea exist in this picture?\n<image>", "pred_response": "No", "answer_id": "VBwmF3AitZorHJSztGf22G", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 650, "prompt": "What is the shape of larynx in this image?\n<image>", "pred_response": "Oval", "answer_id": "hBywLZr8BBC4pKVPHvnDCv", "gt_response": "Oval", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 651, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "4KZnovbvHAHhtB5dvE2kXt", "gt_response": "Larynx", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 652, "prompt": "Are there organs in the picture that can deliver food?\n<image>", "pred_response": "No", "answer_id": "LaCWAmfiDDYUvr57K6msj2", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 653, "prompt": "Does the picture contain the organ which has the effect of gas delivery?\n<image>", "pred_response": "No", "answer_id": "TNjnm9SxBMbfsDNZcKKbSm", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 654, "prompt": "Is this a CT?\n<image>", "pred_response": "No", "answer_id": "X9fUQiyZ5fXuFgAobAVCq6", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 655, "prompt": "What scanning plane does this image belong to?\n<image>", "pred_response": "Transverse Plane", "answer_id": "fWKpLXVecStzEBWbnrxnJX", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 656, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Head", "answer_id": "ScgfaMFy5nmUUzaJoAirbY", "gt_response": "Neck", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 657, "prompt": "Where is the esophagus in this image?\n<image>", "pred_response": "Left", "answer_id": "CCVuTE4KGJhcsgibNKwyLi", "gt_response": "Under the trachea", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 658, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Head", "answer_id": "D8VxbrdHWXWRAbrWS5QUvt", "gt_response": "Neck", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 659, "prompt": "Does the ears exist in this picture?\n<image>", "pred_response": "No", "answer_id": "MSAaemumMe5LhhNntjQ78B", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 660, "prompt": "Is there small bowel in this image?\n<image>", "pred_response": "No", "answer_id": "QgA2Dpr53DdnqUkTW7FjDw", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 661, "prompt": "What color do the trachea show in the picture?\n<image>", "pred_response": "Black", "answer_id": "2JrhB98Q9spjyxScJtSgqv", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 662, "prompt": "What is the shape of trachea about this patient?\n<image>", "pred_response": "Oval", "answer_id": "fERNvTR7bGeQH7fwwLz834", "gt_response": "Irregular", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 663, "prompt": "What role does the black hollow tissue play in the human body?\n<image>", "pred_response": "No role", "answer_id": "DbrSbr7kuGBqYNbBTNo6J2", "gt_response": "Gas delivery", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 664, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "AehdyNXrgGsAxr5cbxqcw8", "gt_response": "Trachea", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 665, "prompt": "Do any of the organs in the picture belong to the nervous system?\n<image>", "pred_response": "No", "answer_id": "e95PkwTA4vULen6FnXiC8E", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 666, "prompt": "Does the picture contain the organ that could be used for detoxicating?\n<image>", "pred_response": "No", "answer_id": "2jbVy2AHQr8ZqAeXfxywFB", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 667, "prompt": "Is this an MRI scan?\n<image>", "pred_response": "No", "answer_id": "GwJwg8bnL7w5gCdnfMvfKB", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 668, "prompt": "Is this a transverse section?\n<image>", "pred_response": "No", "answer_id": "2f7RcexWxAuY7tVKnrdQEo", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 669, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "CwPaaTZrrvSJPjeYzz8ugu", "gt_response": "Neck", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 670, "prompt": "Which is trachea in this image?\n<image>", "pred_response": "Left", "answer_id": "mLzQ3xjyyvaak75B3JPNdo", "gt_response": "Black Hollow", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 671, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Abdomen", "answer_id": "N29Vkq7dWfsf6AGxkuD55n", "gt_response": "Neck", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 672, "prompt": "Does the picture contain spinal cord?\n<image>", "pred_response": "No", "answer_id": "CgomPUPRyBLxsyMDNMtmoc", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 673, "prompt": "Are/Is there kidneys in the picture?\n<image>", "pred_response": "No", "answer_id": "6Q5pb8r6zQzyF8UeZK7gVo", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 674, "prompt": "What is the shape of trachea about this patient?\n<image>", "pred_response": "Oval", "answer_id": "fZGfQ8DZ73UQWtu6etLM7K", "gt_response": "Irregular", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 675, "prompt": "What role does the black hollow tissue play in the human body?\n<image>", "pred_response": "No role", "answer_id": "jx9mWNXFe4a7QeA22tGSAG", "gt_response": "Gas delivery", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 676, "prompt": "Which organs/organ in the picture belong to the digestive system?\n<image>", "pred_response": "None", "answer_id": "23DW6DsQ8unt63m6jxJuCj", "gt_response": "Esophagus", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 677, "prompt": "Does the picture contain the organ which has the effect of discharging waste?\n<image>", "pred_response": "No", "answer_id": "E9XovxMAbYksR2Bijsg6Ha", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 678, "prompt": "Do any of the organs in the picture belong to the nervous system?\n<image>", "pred_response": "No", "answer_id": "iZQh6D2fw7m2qZPqEqmxR7", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 679, "prompt": "Is this a CT scan?\n<image>", "pred_response": "No", "answer_id": "Jo58iM6guP6NeCsABFteKa", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 680, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T2", "answer_id": "fcg8UPy5qraXWJNdsKaF3z", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 681, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "CGUKprXwKBAVSekLAbaAxd", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 682, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "AY4GcdjsTPBtC5Wdg4uL3J", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 683, "prompt": "Which hemisphere is abnormal,left or right?\n<image>", "pred_response": "Left", "answer_id": "QJK7tKfxNFMSesasBxBjx7", "gt_response": "Right", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 684, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Head", "answer_id": "GJx8U4B9FRX37qBFUC2hx6", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 685, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain Tumor, Brain Aneurysm, Brain AVM, Brain Edema, Brain Infection, Brain Stroke, Brain Tumor, Brain Aneurysm, Brain AVM, Brain Edema, Brain Infection, Brain Stroke, Brain Tumor, Brain Aneurysm, Brain AVM, Brain Edema, Brain Infection, Brain Stroke, Brain Tumor, Brain Aneurysm, Brain AVM, Brain Edema, Brain", "answer_id": "iu5ait2iBMwbAQ4AfZtcGT", "gt_response": "Brain Edema, Brain Enhancing Tumor", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 686, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "2", "answer_id": "H8yj84bVmmEfEq6UQrSZHN", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 687, "prompt": "What is the main cause of the disease on the right lobe of brain in this picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "echwvCfkPs8jNE8wiK5KN4", "gt_response": "Brain embryonic tissue dysplasia, genetic factors, chemical factors", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 688, "prompt": "How was this image taken?\n<image>", "pred_response": "MRI", "answer_id": "dtAXnPgUE8yGTWGG4pWsqG", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 689, "prompt": "Is this an MRI image with T1 weighted?\n<image>", "pred_response": "No", "answer_id": "d3xu77S7RnLdhzHkQX7Zs8", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 690, "prompt": "Is this a coronal section?\n<image>", "pred_response": "No", "answer_id": "edpaXZksytbiejLAVpvqhx", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 691, "prompt": "Where does the picture show?\n<image>", "pred_response": "Head", "answer_id": "VLkYv5LSK4Fi6EEFW9eXMS", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 692, "prompt": "Where is the brain non-enhancing tumor?\n<image>", "pred_response": "Right", "answer_id": "dMjDdwrEiwwh6QvuTEF3vb", "gt_response": "Upper Left Lobe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 693, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Head", "answer_id": "driE6gk3v7nzdxZoRNfJbi", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 694, "prompt": "Is the brain healthy?\n<image>", "pred_response": "No", "answer_id": "7eroJSruQA2Fq9CMs6vx5i", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 695, "prompt": "What is the color of abnormality in this image?\n<image>", "pred_response": "Black", "answer_id": "iihSCKvzryQqKW9wLKV5at", "gt_response": "White", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 696, "prompt": "What is the main cause of the disease on the upper left lobe of brain in this picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "2VHH9K4TYKbPVcYrgd4XTV", "gt_response": "Brain embryonic tissue dysplasia, genetic factors, chemical factors", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 697, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "BCszjpf9YCtiGDLdxEozEn", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 698, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T2", "answer_id": "Vdg3w9HBnB7yGtMokRMw2s", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 699, "prompt": "Is this sagittal plane?\n<image>", "pred_response": "No", "answer_id": "M9upbSzbAjTEyVZQK2FFfC", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 700, "prompt": "Where does the picture show?\n<image>", "pred_response": "Head", "answer_id": "hV4GD3UeC8gbVgFX68VKav", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 701, "prompt": "Which hemisphere is abnormal,left or right?\n<image>", "pred_response": "Left", "answer_id": "PKSAafNEAS6Qcq4hPqChjP", "gt_response": "Left", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 702, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Head", "answer_id": "ArYz4dbWiRvBY2GFfKmhVS", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 703, "prompt": "What disease is/are shown on the left of brain?\n<image>", "pred_response": "Brain Tumor", "answer_id": "hRKX8ag4NPK22k6Ci2P65D", "gt_response": "Brain Edema, Brain Enhancing Tumor, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 704, "prompt": "Is the tumor enhancing?\n<image>", "pred_response": "No", "answer_id": "XD4P4vG23LLWZCr6XnZdcc", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 705, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "2", "answer_id": "EDUyRmBG5QCgoxSVEgCksT", "gt_response": "3", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 706, "prompt": "What kind of symptoms will the disease on the left lobe of brain in the picture bring to the patient?\n<image>", "pred_response": "Headache, dizziness, nausea, vomiting, confusion, difficulty in speaking, difficulty in understanding speech, difficulty in writing, difficulty in reading, difficulty in performing mental calculations, difficulty in learning, difficulty in remembering, difficulty in recognizing faces, difficulty in recognizing objects, difficulty in recognizing letters, difficulty in recognizing numbers, difficulty in recognizing colors, difficulty in recognizing shapes, difficulty in recognizing sounds, difficulty in recognizing tastes, difficulty in recognizing smells, difficulty in recognizing body movements, difficulty in recognizing body positions, difficulty in recognizing body parts, difficulty in", "answer_id": "oG7NcPY57NU7KmbK9XMxdS", "gt_response": "Visual impairment, vomiting, tinnitus, increased intracranial pressure", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 707, "prompt": "What modality is shown in this image?\n<image>", "pred_response": "MRI", "answer_id": "dSjCGcXk6ffXZ2pyAcrvUb", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 708, "prompt": "Is this an MRI image with T1 weighted?\n<image>", "pred_response": "No", "answer_id": "Aa6hDahjWzwSNvmugN39fd", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 709, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "UhNg2v88zgRf2gxMF8cE2d", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 710, "prompt": "Do the organs in the image exist in the chest?\n<image>", "pred_response": "No", "answer_id": "fNdLVu23ysrVRwxeGW4BMX", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 711, "prompt": "Where is the brain edema?\n<image>", "pred_response": "Right", "answer_id": "TWaybPdUW8zh6nzjWmCWzm", "gt_response": "Left Lobe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 712, "prompt": "Is this a study of the head?\n<image>", "pred_response": "Yes", "answer_id": "8QMFtRTzQaxP3QJ8WFSyfA", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 713, "prompt": "Is the brain abnormal?\n<image>", "pred_response": "Yes", "answer_id": "KToDeJvVTsuxvuhAFLnS5r", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 714, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "2", "answer_id": "gxhPpm7yAD7vp9ohuboM3R", "gt_response": "1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 715, "prompt": "How to treat the most severe disease in this image?\n<image>", "pred_response": "Medical treatment", "answer_id": "ShioaaWWVy9SqohuuXZTzH", "gt_response": "Pharmacotherapy, rehabilitation", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 716, "prompt": "What modality is shown in this image?\n<image>", "pred_response": "MRI", "answer_id": "FvGSNpapukixXTwocmzpTk", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 717, "prompt": "Is this a T1 weighted or T2 weighted MRI image?\n<image>", "pred_response": "T2", "answer_id": "bSgoVFTiPfhsifF5hcPLgh", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 718, "prompt": "Is this a coronal section?\n<image>", "pred_response": "No", "answer_id": "cUSP6d5BxebdaQwrPiPKpf", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 719, "prompt": "Do the organs in the image exist in the abdomen?\n<image>", "pred_response": "No", "answer_id": "697PMnh4UpSFD6nTKskyJS", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 720, "prompt": "Where are the abnormalities in this image?\n<image>", "pred_response": "Left, Right, Top, Bottom", "answer_id": "KEFDuH9rChUXck3kUA2Li7", "gt_response": "Upper Left Lobe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 721, "prompt": "Is this a study of the head?\n<image>", "pred_response": "Yes", "answer_id": "JSBnhkwGPX7muJas5vTEqe", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 722, "prompt": "Is there evidence of a brain edema?\n<image>", "pred_response": "No", "answer_id": "NY5RiikeRfWWJBT32JepWK", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 723, "prompt": "Is the abnormality hyperdense or hypodense?\n<image>", "pred_response": "Hypodense", "answer_id": "cZZvo42cW4uqdBXe6ccKbS", "gt_response": "Hyperdense", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 724, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "2", "answer_id": "E29soxZ5fxwQi38toRGAdj", "gt_response": "1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 725, "prompt": "How to treat the most severe disease in this image?\n<image>", "pred_response": "Medical treatment", "answer_id": "45yxvBAMC2zabjTR8EoxLH", "gt_response": "Pharmacotherapy, rehabilitation", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 726, "prompt": "What type of medical image is this?\n<image>", "pred_response": "MRI", "answer_id": "7Czyk4Ef4mRqp5uJbuctu8", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 727, "prompt": "Is this a T1 weighted or T2 weighted MRI image?\n<image>", "pred_response": "T2", "answer_id": "ifB7jXf6gtgkDYP2UZKHQ7", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 728, "prompt": "Is this a coronal section?\n<image>", "pred_response": "No", "answer_id": "JnNqFw9GXmWeETYLTN9Ssc", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 729, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "bhVxcuq24kjEULCgq3wTYj", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 730, "prompt": "Where is the brain tumor?\n<image>", "pred_response": "Right", "answer_id": "MgDt2FTtVtCzUjSn9L8tFY", "gt_response": "Upper Left Lobe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 731, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Head", "answer_id": "nDugJzefj7ytXnphWPbYiM", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 732, "prompt": "Is this image normal?\n<image>", "pred_response": "No", "answer_id": "7tRxz2DBgb7ri68iKFj4ij", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 733, "prompt": "How to prevent the occurrence of the disease on the upper left lobe of brain in this image?\n<image>", "pred_response": "1. Avoid physical trauma, 2. Maintain healthy diet, 3. Avoid alcohol, 4. Avoid smoking, 5. Maintain physical fitness, 6. Avoid excessive exposure to sunlight, 7. Maintain healthy sleep, 8. Avoid stress, 9. Maintain healthy weight, 10. Avoid excessive caffeine intake", "answer_id": "NMNguad2jwBXgJWNYMJpe8", "gt_response": "Pay attention to dietary hygiene, strengthen physical fitness and avoid brain trauma", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 734, "prompt": "What imaging modality was used?\n<image>", "pred_response": "MRI", "answer_id": "Fio2QFLm3n8FmBx85WsZSY", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 735, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T2", "answer_id": "mDZisR8vDQvUMQeA9E8bzB", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 736, "prompt": "Is this a transverse section?\n<image>", "pred_response": "No", "answer_id": "6WgCQARz5JiaWhWsDtLtuy", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 737, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "Q3oUf2uRQjcFHgJGVkBNbV", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 738, "prompt": "Where is the brain tumor?\n<image>", "pred_response": "Right", "answer_id": "kdZREw6JQZdNVM8dkhxqz8", "gt_response": "Upper Right Lobe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 739, "prompt": "Is this a study of the neck?\n<image>", "pred_response": "No", "answer_id": "6jBuDtHLuboqZp89eyNfzU", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 740, "prompt": "What disease is/are shown on the upper right of brain?\n<image>", "pred_response": "Brain Tumor", "answer_id": "a7QHDEhVeNLhb9p9Pm2U6k", "gt_response": "Brain Edema, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 741, "prompt": "Is the abnormality hyperdense or hypodense?\n<image>", "pred_response": "Hyperdense", "answer_id": "JHGjGtobuoyQfedZDcwqGo", "gt_response": "Hyperdense", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 742, "prompt": "What kind of symptoms will the disease on the upper right lobe of brain in the picture bring to the patient?\n<image>", "pred_response": "Headache, dizziness, nausea, vomiting, difficulty in speaking, difficulty in understanding speech, difficulty in reading, difficulty in writing, difficulty in performing fine motor tasks, difficulty in performing gross motor tasks, difficulty in maintaining balance, difficulty in coordinating movements, difficulty in controlling body temperature, difficulty in controlling body humidity, difficulty in controlling body salivation, difficulty in controlling body sweating, difficulty in controlling body digestion, difficulty in controlling body urination, difficulty in controlling body defecation, difficulty in controlling body menstruation, difficulty in controlling", "answer_id": "MS5NFWoiQoKWsqJoeXoibJ", "gt_response": "Visual impairment, vomiting, tinnitus, increased intracranial pressure", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 743, "prompt": "How was this image taken?\n<image>", "pred_response": "MRI", "answer_id": "nDFYVVqJpRduG6v4sxeAGR", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 744, "prompt": "Is this a T1 weighted or T2 weighted MRI image?\n<image>", "pred_response": "T2", "answer_id": "ZXj3TtCsFqvrvTuwVCbnht", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 745, "prompt": "Is this a coronal section?\n<image>", "pred_response": "No", "answer_id": "SZFQRJvHLtteTHymZUETHL", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 746, "prompt": "Do the organs in the image exist in the abdomen?\n<image>", "pred_response": "No", "answer_id": "6v4F5gkWKA6hpDGRJtVE2g", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 747, "prompt": "Where is the brain tumor?\n<image>", "pred_response": "Right", "answer_id": "EnkS35Rra2imHAoHABeNyK", "gt_response": "Upper Right Lobe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 748, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Head", "answer_id": "Ktd4tPhBk3su5U45iiom8a", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 749, "prompt": "Is the brain abnormal?\n<image>", "pred_response": "Yes", "answer_id": "j2FvWSq8hnM3hijX2SpMUE", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 750, "prompt": "How to prevent the occurrence of the disease on the upper right lobe of brain in this image?\n<image>", "pred_response": "Avoid the use of cell phones, avoid the use of electrical appliances, avoid the use of electrical devices, avoid the use of electrical equipment, avoid the use of electrical gadgets, avoid the use of electrical devices, avoid the use of electrical gadgets, avoid the use of electrical devices, avoid the use of electrical gadgets, avoid the use of electrical devices, avoid the use of electrical devices, avoid the use of electrical devices, avoid the use of electrical devices, avoid the use of electrical devices, avoid the use of electrical devices", "answer_id": "mtL3SpBGfy9nQFUrGStYfi", "gt_response": "Pay attention to dietary hygiene, strengthen physical fitness and avoid brain trauma", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 751, "prompt": "Is this image taken via MRI?\n<image>", "pred_response": "No", "answer_id": "2fhv5AFapjqzTSydWFP6zn", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 752, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "PSXAPNgGwQDyTfFCeB8Zxv", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 753, "prompt": "What scanning plane does this image belong to?\n<image>", "pred_response": "Transverse Plane", "answer_id": "fkTqu4Q642SzPbQtND2f3W", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 754, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Head", "answer_id": "YBqX6WF8pujmPmGtBkRVNj", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 755, "prompt": "Which lobe is abnormal,left or right?\n<image>", "pred_response": "Right", "answer_id": "g6trdH73iFQNNRxkqCHpL5", "gt_response": "Right", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 756, "prompt": "Is this a study of the head?\n<image>", "pred_response": "Yes", "answer_id": "eXwo2rHeDqoook4qDmX6xs", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 757, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain", "answer_id": "DHDfpZFZzNt8eLHqDhA3BN", "gt_response": "Brain Edema, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 758, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "0", "answer_id": "QVa7KS7T6c9j4jBXcAjbgb", "gt_response": "None", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 759, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "UisM2kiHm6YLsJu4cgJJgr", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 760, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "J2gVqcui4WpP4GqfSHrpm7", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 761, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "cq8hjmTvkWKM4xbkAyqp8x", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 762, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Brain", "answer_id": "iWTJURYFQjV5MC73erneFK", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 763, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "FJpHbrWeJRWnZ5xPQY56wX", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 764, "prompt": "Does the picture contain brian?\n<image>", "pred_response": "Yes", "answer_id": "8Xwqm3fK2ecjDhSsp4Xjdn", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 765, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "GC3ctptbXJCUYovz4Mrd9t", "gt_response": "Brain Edema", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 766, "prompt": "Does the brain look abnormal?\n<image>", "pred_response": "Yes", "answer_id": "Rr7JY5NqrLVqBZX7o3P45b", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 767, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Right", "answer_id": "aCgHdyCdS5My3Qw3PqwCct", "gt_response": "Lower Right", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 768, "prompt": "Is this a X-Ray?\n<image>", "pred_response": "No", "answer_id": "dfNersTSyyBqsQnyJ9nGgt", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 769, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T2", "answer_id": "CnbtAJPQgiqBdJM4TNkqxj", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 770, "prompt": "Is this a coronal plane?\n<image>", "pred_response": "No", "answer_id": "MguyTde5c2UKFFcKQLfSjz", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 771, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "ZAYK7CNkUTHUWRTGVeut9T", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 772, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left", "answer_id": "ifU3NHkoXRQ9MjBmuau2UQ", "gt_response": "Upper Right Lobe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 773, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Head", "answer_id": "YUUScHDsm9jBXmWaVj6TdF", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 774, "prompt": "Is/Are there abnormalities in the patient's left brain lobe?\n<image>", "pred_response": "Yes", "answer_id": "hnPrMBWz3sMbbgguEeJajq", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 775, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "2", "answer_id": "RGSAfzAmi4MMqpgWdpwsvg", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 776, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "0", "answer_id": "LicuEhfEWZ6r6jBFCMDYDU", "gt_response": "None", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 777, "prompt": "Is this a CT scan?\n<image>", "pred_response": "No", "answer_id": "JKTPTVCqB99sLBmtFVzQpt", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 778, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "grXCBznNqKGNiywsj2qN8R", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 779, "prompt": "Is this sagittal plane?\n<image>", "pred_response": "No", "answer_id": "YyNEA9rbW2cxjD5zZnKWLy", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 780, "prompt": "Do the organs in the image exist in the pelvic cavity?\n<image>", "pred_response": "No", "answer_id": "RAtmYvjq2nmiJygxwyXBFY", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 781, "prompt": "Where are the abnormalities in this image?\n<image>", "pred_response": "Right", "answer_id": "f2NYLDc4tyU8cm22CQMP2j", "gt_response": "Left Lobe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 782, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Head", "answer_id": "Z2LmUTYmEJpYpXh4FJ2eVz", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 783, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "j836Uqw69rBMJTKVQkRVZt", "gt_response": "Brain Edema, Brain Enhancing Tumor, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 784, "prompt": "Is the brain enhancing tumor hyperdense or hypodense?\n<image>", "pred_response": "Hyperdense", "answer_id": "VS8WrZV8mUQrCnNd2XzZL6", "gt_response": "Hyperdense", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 785, "prompt": "What kind of symptoms will the disease on the left lobe of brain in the picture bring to the patient?\n<image>", "pred_response": "Headache, dizziness, nausea, vomiting, and sometimes seizures", "answer_id": "EsSP4juXqZm4afKub7H84e", "gt_response": "Visual impairment, vomiting, tinnitus, increased intracranial pressure", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 786, "prompt": "Is this image taken via CT?\n<image>", "pred_response": "No", "answer_id": "TdiVuktbLKepiG6PCMrpTL", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 787, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T2", "answer_id": "7Dc32ucCp7AEbgsBk7FiLq", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 788, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse Plane", "answer_id": "Wqoa6S3r3CmE8i4Xu7AZVH", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 789, "prompt": "Which part of the human body is the organ located in the image?\n<image>", "pred_response": "Head", "answer_id": "8LHJGmFrM6TjSfctVnvMgX", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 790, "prompt": "Where is the brain tumor located?\n<image>", "pred_response": "Right", "answer_id": "JQAxC8jFXVBLEVFb6s9Xiq", "gt_response": "Lower Left Lobe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 791, "prompt": "Is this a study of the chest?\n<image>", "pred_response": "No", "answer_id": "MMuTkTfUHMTN2KdK67FGo5", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 792, "prompt": "Does the patient have brain tumor?\n<image>", "pred_response": "Yes", "answer_id": "J6oSfh7rBoJ45sc7wNwZQq", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 793, "prompt": "What is the main cause of the disease on the left lobe of brain in this picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "hDB8CLHAGaxtsyzbGKa7U8", "gt_response": "Brain embryonic tissue dysplasia, genetic factors, chemical factors", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 794, "prompt": "How was this image taken?\n<image>", "pred_response": "MRI", "answer_id": "gspTogSfRVavr8KgCzic26", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 795, "prompt": "What type of MRI is shown in this image?\n<image>", "pred_response": "MRI", "answer_id": "Q9HnsfT993kxwiFb6UK6oy", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 796, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse Plane", "answer_id": "F4eM3uuoeLAqmJF3or7nzd", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 797, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Head", "answer_id": "gV2zeUxU5UVthErYG4RctC", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 798, "prompt": "Where is the brain tumor?\n<image>", "pred_response": "Right", "answer_id": "nqJAbhEUYH8bzr4dLgzoqc", "gt_response": "Left Lobe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 799, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "6uTvCWoir2L68rb4XbbjsL", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 800, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "66CCUrg278BMmq5u3NR4kK", "gt_response": "Brain Edema, Brain Enhancing Tumor, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 801, "prompt": "What density is the brain enhancing tumor ?\n<image>", "pred_response": "2", "answer_id": "et7XqksunPEs8sHk5PiG3t", "gt_response": "Hyperdense", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 802, "prompt": "What kind of symptoms will the disease on the left lobe of brain in the picture bring to the patient?\n<image>", "pred_response": "Headache, dizziness, nausea, vomiting, confusion, difficulty in speaking, difficulty in understanding speech, difficulty in reading, difficulty in writing, difficulty in performing mental calculations, difficulty in maintaining balance, difficulty in coordinating movements, difficulty in controlling emotions, difficulty in controlling facial expressions, difficulty in controlling body temperature, difficulty in controlling blood pressure, difficulty in controlling heartbeat, difficulty in controlling breathing, difficulty in controlling bladder, difficulty in controlling bowel movements, difficulty in controlling sexual function, difficulty in controlling sleep, difficulty in controlling", "answer_id": "7UuDyWCU8vgDavJoX8b2vA", "gt_response": "Visual impairment, vomiting, tinnitus, increased intracranial pressure", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 803, "prompt": "Is this a CT?\n<image>", "pred_response": "No", "answer_id": "dm8RMH8AK2haQsriBFTLrY", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 804, "prompt": "Is this an MRI image with T2 weighted?\n<image>", "pred_response": "No", "answer_id": "Smi2oXjxTKryaZepkyJpc3", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 805, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "BwDLyUrpcoZQKcLncgUvQz", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 806, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "bzJDeUHMLvFya3E9Xxc7LK", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 807, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Right", "answer_id": "kEvD9ZpCcW2os6cTrAbZtJ", "gt_response": "Lower Left Lobe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 808, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Head", "answer_id": "TH4a6BRW9HmPrnwvKqAzp2", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 809, "prompt": "What disease is/are shown on the lower left of brain?\n<image>", "pred_response": "Brain Tumor", "answer_id": "Ufwgq5V3AB49ngFy7wzMD8", "gt_response": "Brain Edema, Brain Enhancing Tumor", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 810, "prompt": "What is the color of brain enhancing tumor in this image?\n<image>", "pred_response": "Black", "answer_id": "X8gCp24DdXY6jGvdHjxuFm", "gt_response": "White", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 811, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "2", "answer_id": "YpsgsGKSMv6nk29rCfBNZe", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 812, "prompt": "How to prevent the occurrence of the disease on the lower left lobe of brain in this image?\n<image>", "pred_response": "Avoid physical and mental stress, maintain a healthy diet, and avoid alcohol", "answer_id": "EJeWAxYkvThGxJHNHT2jzq", "gt_response": "Pay attention to dietary hygiene, strengthen physical fitness and avoid brain trauma", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 813, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "gRL7uudrpa9zeMuAD66WYR", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 814, "prompt": "Is this an MRI image with T2 weighted?\n<image>", "pred_response": "No", "answer_id": "UBprBfmdXU97Jpo8v8JqGJ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 815, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse Plane", "answer_id": "Du5cXawkhT2BMHHfMKm6mR", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 816, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "95pVbewfiZKN6nhdF4yTuu", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 817, "prompt": "Where is the brain non-enhancing tumor?\n<image>", "pred_response": "Right", "answer_id": "W8bgxiQuuZLR72Wiu69eyw", "gt_response": "Left Lobe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 818, "prompt": "Is this a study of the head?\n<image>", "pred_response": "Yes", "answer_id": "XbSK5PNEXwtwH42meiWTvm", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 819, "prompt": "What disease is/are shown on the left of brain?\n<image>", "pred_response": "Brain Tumor", "answer_id": "hqySdQriHurNHbbLLxHtoV", "gt_response": "Brain Edema, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 820, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "2", "answer_id": "C8MVXJkY8WpTDdk9emgK4e", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 821, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "0", "answer_id": "B5DhZwPNcj7N7Fb57PL9FJ", "gt_response": "None", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 822, "prompt": "Is this an MRI scan?\n<image>", "pred_response": "Yes", "answer_id": "JSPqsZYZ7Pf6NTHjCnDpEA", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 823, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "EDD4Xy3KkG6Z2ZbhFW46bm", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 824, "prompt": "Is this a coronal section?\n<image>", "pred_response": "No", "answer_id": "X9wXndQjpPbNGWP8vnxG2N", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 825, "prompt": "Do the organs in the image exist in the neck?\n<image>", "pred_response": "No", "answer_id": "PTfqsqBVsNiUm9RA7RstjN", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 826, "prompt": "Where is the brain edema located?\n<image>", "pred_response": "Right", "answer_id": "jY2zvYZ3wzUMFtVSWxEo4J", "gt_response": "Left Lobe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 827, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Head", "answer_id": "k5ymhXyJMATKRvSJ4eUFPN", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 828, "prompt": "Is the brain normal appearing?\n<image>", "pred_response": "No", "answer_id": "J7kK4mYfydn2P4aCrCXPdw", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 829, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "0", "answer_id": "DFjUGL5vbBPda9JtubUnZB", "gt_response": "None", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 830, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "envm9LW8VsHLDNnuNiQUUJ", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 831, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "bKHKK3GwcSQGunBY9iAPUh", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 832, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "An3zp2nRpfZqVj4Mgww3wp", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 833, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Brain", "answer_id": "4m2AYuw9UJMjHWwz8G8LSf", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 834, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "c2yJn92GBkuHtdiAgLSok6", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 835, "prompt": "Does the picture contain brian?\n<image>", "pred_response": "Yes", "answer_id": "VJMinyn3WxL442kh6Ae6HG", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 836, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "Zc7D2cqMLq3VXVKCMeWdhU", "gt_response": "None", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 837, "prompt": "Does the brain look abnormal?\n<image>", "pred_response": "Yes", "answer_id": "Bw9pzaHZtzvkJGgtRqQEo8", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 838, "prompt": "Is this image taken via MRI?\n<image>", "pred_response": "Yes", "answer_id": "GvbiEiKqVXzZbJobSrJN47", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 839, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T2", "answer_id": "LNecrsZd4CktGrZB3txoJZ", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 840, "prompt": "Is this a transverse section?\n<image>", "pred_response": "No", "answer_id": "nXtjaacYYvR9qc2AXu3xNC", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 841, "prompt": "Which part do the organs in this image locate from the aspect of the whole body?\n<image>", "pred_response": "Head", "answer_id": "Bzi9GPszrHBaMxzojyA43H", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 842, "prompt": "Where are the abnormalities in this image?\n<image>", "pred_response": "Right, Left, and Center", "answer_id": "WeNXKekRwEq3dkjGba2MT7", "gt_response": "Lower Left Lobe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 843, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Head", "answer_id": "JAxoHccnyWj9urHVccd9kY", "gt_response": "Head", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 844, "prompt": "What disease is/are shown on the left of brain?\n<image>", "pred_response": "Brain Tumor", "answer_id": "JZMaNE6acQr2RojKRrC9jQ", "gt_response": "Brain Edema", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 845, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "2", "answer_id": "Tf5brKUnD5MMzkToivFhbh", "gt_response": "1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 846, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "0", "answer_id": "BZyKy4yMEBFPU2kov8g9Gt", "gt_response": "None", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 847, "prompt": "Which is the kidney in this image?\n<image>", "pred_response": "Right", "answer_id": "AQQuFyRX69dmbQNL7xMNpE", "gt_response": "Symmetrical to the bottom spine", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 848, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "9fJnVvPcF2Y7ZgoUB8d73g", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 849, "prompt": "Does the stomach appear in the image?\n<image>", "pred_response": "No", "answer_id": "BotHixspQKwWck9KYKGqH2", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 850, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "QXjQiDygR74Yx9d2uTwSJQ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 851, "prompt": "What color is the spleen in the picture?\n<image>", "pred_response": "Black", "answer_id": "XDkM5F9ytFV6z3sK8tDERp", "gt_response": "Gray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 852, "prompt": "Which is smaller in this image,liver or right kidney?\n<image>", "pred_response": "Right kidney", "answer_id": "b2yJHMDi7hWcGUS5X8LXaY", "gt_response": "Right Kidney", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 853, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "2", "answer_id": "49wEu3ZxcaETw9eCDDta22", "gt_response": "6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 854, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "kh5t2KxXV84Va4UzixzmgA", "gt_response": "Much", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 855, "prompt": "Which organs/organ is part of the lymphatic system in this image?\n<image>", "pred_response": "Liver", "answer_id": "UvudZNss5dPDejToLJB958", "gt_response": "Spleen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 856, "prompt": "Which organs/organ is part of the urinary system in this image?\n<image>", "pred_response": "Kidney", "answer_id": "jNVDPjRJ5RKpG2EkcKw6he", "gt_response": "Kidney", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 857, "prompt": "Does the picture contain the organ which has the effect of discharging waste?\n<image>", "pred_response": "Yes", "answer_id": "3wYhXyGkAvywvrRfaa6Ggj", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 858, "prompt": "Do any of the organs in the picture belong to the respiratory system?\n<image>", "pred_response": "No", "answer_id": "Fh698UsdYLmRVEdRBQaZPq", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 859, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Plane 2", "answer_id": "E2XizTwEihmSvPpiNB9GLc", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 860, "prompt": "Where is the colon in this image?\n<image>", "pred_response": "Left", "answer_id": "C86TeCJgSGsid27tD8uuJY", "gt_response": "Upper Right", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 861, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "QbxBbkBVd5PE9Lo5KqY9cq", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 862, "prompt": "Does the trachea exist in this picture?\n<image>", "pred_response": "No", "answer_id": "Ua9gzWDsbqKpPA4EVLhPtU", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 863, "prompt": "Does the picture contain bladder?\n<image>", "pred_response": "No", "answer_id": "cBoXodaFGfhfHxitmGWHgT", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 864, "prompt": "Which is smaller in this image,liver or lung?\n<image>", "pred_response": "Lung", "answer_id": "9zeprPdvBCP6HfkYz4MAn9", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 865, "prompt": "How many lungs have existed in this image?\n<image>", "pred_response": "2", "answer_id": "3BEMFijZNS7Ap5Ef7qCyJP", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 866, "prompt": "What is the effect of the organ on the upper left of this image?\n<image>", "pred_response": "Liver", "answer_id": "nnuMN9GDfVdYnKx2YZpvar", "gt_response": "Biotransformation, detoxification", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 867, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "FrPKeLpHJiE4yKYoaqPQba", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 868, "prompt": "Does the picture contain the organ that could be used for secreting enzymes?\n<image>", "pred_response": "No", "answer_id": "G9kmN2S338uJHMwr28SVFn", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 869, "prompt": "Do any of the organs in the picture belong to the digestive system?\n<image>", "pred_response": "No", "answer_id": "GaHJmZK2EXbnY2CUDym79n", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 870, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Abdomen", "answer_id": "H6qrqpriuDeaHxKPHMtbWa", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 871, "prompt": "Does the larynx appear in the image?\n<image>", "pred_response": "No", "answer_id": "QR3w5iCxrgo2uw6t3nFNPR", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 872, "prompt": "Does the lung exist in this picture?\n<image>", "pred_response": "No", "answer_id": "cuPnAosgrW2jd9UVQnTmuh", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 873, "prompt": "Which is smaller in this image,colon or liver?\n<image>", "pred_response": "Liver", "answer_id": "5Mg3WCrudwkGyxV7s7e6xE", "gt_response": "Colon", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 874, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "1", "answer_id": "cEVhPqnUDHAfonnaQjeqP3", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 875, "prompt": "What is the function of the organs appearing in pairs in the picture?\n<image>", "pred_response": "Liver, Kidney", "answer_id": "Z5wKYrhbC9AHg8Kv9rkEAD", "gt_response": "Breathe", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 876, "prompt": "Which organs/organ is part of the digestive system in this image?\n<image>", "pred_response": "Liver", "answer_id": "aZohLSwbUMsntrkHubRZKd", "gt_response": "Stomach, Colon", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 877, "prompt": "Which organs/organ in the picture belong to the urinary system?\n<image>", "pred_response": "Kidney", "answer_id": "BLU4Hnh2SAufWMmyGzFvAu", "gt_response": "Kidney", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 878, "prompt": "Does the picture contain the organ which has the effect of detoxicating?\n<image>", "pred_response": "Yes", "answer_id": "EqN7dQeSxPvSES4fWneWMG", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 879, "prompt": "Does the picture contain the organ which has the effect of adjusting water and osmotic pressure balance?\n<image>", "pred_response": "Yes", "answer_id": "czJTEUhE5tzSaCmYC3R83E", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 880, "prompt": "What organ is the gray part on the right of the image?\n<image>", "pred_response": "Liver", "answer_id": "dypdHPJ29SaFcMLGkHmrqy", "gt_response": "Small Bowel", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 881, "prompt": "Does the picture contain colon?\n<image>", "pred_response": "No", "answer_id": "McDZctjTGAHQxHueGsW4Go", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 882, "prompt": "Does the picture contain brain stem?\n<image>", "pred_response": "No", "answer_id": "LvC9Zwa52vJ3VniKahBYax", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 883, "prompt": "Which is smaller in this image,kidney or small bowel?\n<image>", "pred_response": "Small Bowel", "answer_id": "VCX8Y69fwNbU8c8GDX4K7F", "gt_response": "Kidney", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 884, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "1", "answer_id": "da5CFMiPMYanxkWveqLWVM", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 885, "prompt": "What is the effect of the organ on the top of this image?\n<image>", "pred_response": "No effect", "answer_id": "KHMAHqTPRbNYWVQFoPtg3k", "gt_response": "Absorb nutrients, secrete enzymes, digest food", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 886, "prompt": "Are there some organs controlling heartbeat and breathing in this image?\n<image>", "pred_response": "No", "answer_id": "dzLevUUdNkfjBuitcYbcDV", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 887, "prompt": "Does the picture contain the organ which has the effect of digesting food?\n<image>", "pred_response": "No", "answer_id": "8vCTXLLatjoUzxTwuvjFJG", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 888, "prompt": "Which is the kidney in this image?\n<image>", "pred_response": "Right", "answer_id": "4GRGXg5LV8f3UMA35EPA4N", "gt_response": "Symmetrical to the bottom spine", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 889, "prompt": "What organ is the black part on the upper left of the image?\n<image>", "pred_response": "Liver", "answer_id": "neoFU6MYJjCyPHABY2Lozx", "gt_response": "Small Bowel", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 890, "prompt": "Does the rectum appear in this picture?\n<image>", "pred_response": "No", "answer_id": "oTVWnRuvvEyCJZSZPjPzrr", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 891, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "DhEroGHhjj69SA36f68qho", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 892, "prompt": "What color does the left kidney show in the picture?\n<image>", "pred_response": "Black", "answer_id": "9j9QDiThJxK7dNKaSDCWBW", "gt_response": "Gray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 893, "prompt": "Which is smaller in this image, liver or right kidney?\n<image>", "pred_response": "Right kidney", "answer_id": "XXHJGRCDDoWebiu5Byc59p", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 894, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "2", "answer_id": "QdeqLrCDYygtoXkAX6wfrM", "gt_response": "5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 895, "prompt": "What is the shape of spinal cord in this image?\n<image>", "pred_response": "No spinal cord", "answer_id": "kTkkgWZbp7pQk6u6GGgRVH", "gt_response": "Circular", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 896, "prompt": "What is the effect of the organ on the top of this image?\n<image>", "pred_response": "No effect", "answer_id": "CmKxCksRWTbMWMLWQ2yFLE", "gt_response": "Absorb nutrients, secrete enzymes, digest food", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 897, "prompt": "Which organs/organ is part of the nervous system in this image?\n<image>", "pred_response": "Brain", "answer_id": "RoWmSFeryQwTfTehvosPtD", "gt_response": "Spinal cord", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 898, "prompt": "Does the picture contain the organ which has the effect of digesting food?\n<image>", "pred_response": "No", "answer_id": "ZkakMJPdAsAFjBJCTtBPR5", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 899, "prompt": "Does the picture contain the organ that could be used for secreting enzymes?\n<image>", "pred_response": "No", "answer_id": "XLSkiQepHT6dW7NPE24WMs", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 900, "prompt": "Where is the liver in this image?\n<image>", "pred_response": "Lower Left", "answer_id": "UoVkmS3fPrPAN9KZRohE9t", "gt_response": "Left", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 901, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "MkDxaK6qQBkCstfWAdZwUj", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 902, "prompt": "Does the picture contain colon?\n<image>", "pred_response": "No", "answer_id": "NHcSV9teBpwBpHMxhpjjmZ", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 903, "prompt": "Does the humerus head exist in this picture?\n<image>", "pred_response": "No", "answer_id": "WFmCMJjoh763gpTpFrRHJL", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 904, "prompt": "What color does the duodenum show in the picture?\n<image>", "pred_response": "Black", "answer_id": "gWEaw7RrbPGKPmErxxpFPN", "gt_response": "Gray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 905, "prompt": "Which is smaller in this image, spleen or kidney?\n<image>", "pred_response": "Kidney", "answer_id": "JQaKg6BSkztXE9pWALACkU", "gt_response": "Spleen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 906, "prompt": "How many lungs have existed in this image?\n<image>", "pred_response": "2", "answer_id": "aucM3xiwdxy7D5yGkFe2YM", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 907, "prompt": "What is the shape of spleen in the picture?\n<image>", "pred_response": "Lung", "answer_id": "86bc69ujxm8xZF4eSVwf3u", "gt_response": "Oval", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 908, "prompt": "What is the function of the organ on the center of this image?\n<image>", "pred_response": "Store urine", "answer_id": "efG3Ki8JQzQX33Bz7d8YMj", "gt_response": "Absorb nutrients, secrete enzymes, digest food", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 909, "prompt": "Which organs/organ is part of the nervous system in this image?\n<image>", "pred_response": "Brain", "answer_id": "43Dw5vcrBNaYYxmdJn6ZCH", "gt_response": "Spinal cord", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 910, "prompt": "Are there organs in the picture that can adjust water and osmotic pressure balance?\n<image>", "pred_response": "No", "answer_id": "SANWG25ezzEkdf2UsGfp6n", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 911, "prompt": "Are there any organs used for breathing in this image?\n<image>", "pred_response": "No", "answer_id": "7s8ugUeYKHERzecERGnv7K", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 912, "prompt": "Is this a transverse section?\n<image>", "pred_response": "No", "answer_id": "hT29fnLPYXS8hDi7LZK5eq", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 913, "prompt": "Where is the stomach in this image?\n<image>", "pred_response": "Left", "answer_id": "grgMZDf3GKz2fUh3UmXhVe", "gt_response": "Upper Right", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 914, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Abdomen", "answer_id": "FPqiG9ck6ZeJpu9sWCpepo", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 915, "prompt": "Does the picture contain eyes?\n<image>", "pred_response": "No", "answer_id": "672nnzCZcYVjQMqFKSNEnA", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 916, "prompt": "Is there femoral head in this picture?\n<image>", "pred_response": "No", "answer_id": "EuJs3Ksvc97tKeucr5i94J", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 917, "prompt": "Which is bigger in this image,liver or stomach?\n<image>", "pred_response": "Liver", "answer_id": "DP5UKR8kVJmjs4XVCWu3MK", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 918, "prompt": "How many kidneys have existed in this image?\n<image>", "pred_response": "2", "answer_id": "Yga992ymMbqyGixhiK2AMi", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 919, "prompt": "What is the shape of spinal cord in this image?\n<image>", "pred_response": "No spinal cord", "answer_id": "a68qzV9f2fk4eMe8MiGMHL", "gt_response": "Circular", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 920, "prompt": "What is the function of the rightmost organ in this picture?\n<image>", "pred_response": "Liver", "answer_id": "HBamNybRYZ2rc94KU9aJYs", "gt_response": "Improve the body's immunity", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 921, "prompt": "Which organs/organ in the picture belong to the lymphatic system?\n<image>", "pred_response": "Liver", "answer_id": "kJghC89EakU8Wr9rxV9GsJ", "gt_response": "Spleen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 922, "prompt": "Does the picture contain the organ that could receive sound?\n<image>", "pred_response": "Yes", "answer_id": "kpSymz3PTNYFMPqk3RLrYL", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 923, "prompt": "Does the picture contain the organ which has the effect of gas delivery?\n<image>", "pred_response": "Yes", "answer_id": "QUD3rj9LFiTkPt523y23GJ", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 924, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse Plane", "answer_id": "J8AkcrTEVWx6XQTu5j7zFy", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 925, "prompt": "Where is the esophagus in this image?\n<image>", "pred_response": "Left", "answer_id": "V6AzoezvzL94TmnCyNojpL", "gt_response": "Center", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 926, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Abdomen", "answer_id": "dwpnVmZhUneU8FSt2TSjd8", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 927, "prompt": "Does the esophagus exist in this picture?\n<image>", "pred_response": "No", "answer_id": "BbkLtQgZKKcuSGkQTY9dGK", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 928, "prompt": "Does the stomach appear in the image?\n<image>", "pred_response": "No", "answer_id": "GXfAXPSkcFbhUWtReSPDKq", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 929, "prompt": "What color is the spleen in this image?\n<image>", "pred_response": "Black", "answer_id": "P3WgwirQMEnxfPpT5r6SUu", "gt_response": "Gray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 930, "prompt": "Which is the biggest in this image, spleen, heart or liver?\n<image>", "pred_response": "Liver", "answer_id": "MZi4jToEkdH2UtKFftY74z", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 931, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "1", "answer_id": "8LbfD9vCehEWDWvHsoQYnP", "gt_response": "1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 932, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "4vNJX4Psc3BQRSdfy6VvHc", "gt_response": "Much", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 933, "prompt": "Are there organs in the picture that can digest food?\n<image>", "pred_response": "No", "answer_id": "6nXjBfA2wzA55hYdNXrgtt", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 934, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "UfaxtjcaF7MfMWzXpuZSJn", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 935, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "knCRYe2jgbJkyuaaqbrdCb", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 936, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "KAQLgCGXnRUzD342jPA4Xc", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 937, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Brain", "answer_id": "76acntQDZdme5Y9cwqVvN6", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 938, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "2prkMHFsLJfa7fFPAevBHH", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 939, "prompt": "Does the picture contain brian?\n<image>", "pred_response": "Yes", "answer_id": "NPx9Go3cXNVJyBkAzZLQoT", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 940, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "T33gRMc6QxChGw7uPyKQuy", "gt_response": "Brain Tumor, Brain Edema", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 941, "prompt": "Is the brain healthy?\n<image>", "pred_response": "No", "answer_id": "9SzEq9HyNTKAeHDfZm3pvm", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 942, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left, Right, Left,", "answer_id": "bgvQ6EHKEZXSjAZHsoxapc", "gt_response": "Lower Left", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 943, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "NpZeWaX3d65n4J2ChA6RiD", "gt_response": "Abdomen", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 944, "prompt": "Where is the heart in this image,left,top or upper left?\n<image>", "pred_response": "Upper Left", "answer_id": "L65nAgz8SMYabeD8trqQ4h", "gt_response": "Top", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 945, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "jH7vAZxDhJzcUFRN8da6tL", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 946, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "Qh85Kf5g5c2iR7FbeUJ874", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 947, "prompt": "Does the picture contain stomach?\n<image>", "pred_response": "No", "answer_id": "NwJgGRwPXmfYNwXzszxGQw", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 948, "prompt": "Which is smaller in this image, esophagus or liver?\n<image>", "pred_response": "Esophagus", "answer_id": "9NLczKCS29UcBF8FCmnJPG", "gt_response": "Esophagus", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 949, "prompt": "How many lungs have existed in this image?\n<image>", "pred_response": "2", "answer_id": "25sMJusUkh6pE9JEYpPY3o", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 950, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "None", "answer_id": "dGyPs7fWu65udFUfJMdwEY", "gt_response": "Much", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 951, "prompt": "Which organs/organ in the picture belong to the digestive system?\n<image>", "pred_response": "None", "answer_id": "JyRNScfbJbapnXnaSsRPJt", "gt_response": "Esophagus", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 952, "prompt": "Does the picture contain the organ which has the effect of gas delivery?\n<image>", "pred_response": "Yes", "answer_id": "28nm56S8kzjiRyJiiNpaVC", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 953, "prompt": "Does the picture contain the organ that could promote blood flow?\n<image>", "pred_response": "Yes", "answer_id": "PrumJa4qTp2MEknGnHxduq", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 954, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse Plane", "answer_id": "3zkFyFQibSyhA2sJNt4qnz", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 955, "prompt": "What is the leftmost organ in this image?\n<image>", "pred_response": "Lung", "answer_id": "S2RGdPwyNz2cpwkmZijgaB", "gt_response": "Right Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 956, "prompt": "Does the picture contain spinal cord?\n<image>", "pred_response": "No", "answer_id": "mTmCDnejARAvcLWCiCKRFR", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 957, "prompt": "Does the rectum appear in this picture?\n<image>", "pred_response": "No", "answer_id": "M6Zjpy8MSPQ8LniMmCPMaw", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 958, "prompt": "What color is the lung in this image?\n<image>", "pred_response": "Black", "answer_id": "ZxE6L4fAuWwp6LjENp9yW7", "gt_response": "Black", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 959, "prompt": "Which is smaller in this image,heart or esophagus?\n<image>", "pred_response": "Heart", "answer_id": "UztfwvFmQcPavScBZZWHtU", "gt_response": "Esophagus", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 960, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "2", "answer_id": "JTNsHvYzdj6FUdg8UJRBBC", "gt_response": "4", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 961, "prompt": "What is the function of the organ on the top of this image?\n<image>", "pred_response": "Breathe", "answer_id": "jXNZMqAqEqopzVNodv78jg", "gt_response": "Promote blood flow", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 962, "prompt": "Does the picture contain the organ that could receive sound?\n<image>", "pred_response": "Yes", "answer_id": "V8ZQhHfJD3tBv6Jo5ez5YB", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 963, "prompt": "Does the picture contain the organ which has the effect of sensing light?\n<image>", "pred_response": "Yes", "answer_id": "3k3aUsuFDpddjWfV2ip2rE", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 964, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "J9qzEzRgCzhm64hX4WrMCS", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 965, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Lower Abdomen", "answer_id": "4Z7kPwVxYncsQc3E4Txf65", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 966, "prompt": "Where is the rectum existing in this image?\n<image>", "pred_response": "Lower Left", "answer_id": "8Qh4g4kLqRiX62HQbpbREy", "gt_response": "Bottom", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 967, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Lower Abdomen", "answer_id": "KHraF7uF2K4ZXmrPBjBaAR", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 968, "prompt": "Does the picture contain bladder?\n<image>", "pred_response": "No", "answer_id": "EfFAgD8pYimd9pyNswuB99", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 969, "prompt": "Does the parotid appear in the image?\n<image>", "pred_response": "No", "answer_id": "QXqj33cWDHS4Bi97exGwLt", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 970, "prompt": "Which is smaller in this image, bladder or rectum?\n<image>", "pred_response": "Rectum", "answer_id": "cVRHgFRWtjKbiGnwUjjf4S", "gt_response": "Rectum", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 971, "prompt": "How many femoral heads are shown in this image?\n<image>", "pred_response": "2", "answer_id": "8LwbJESYiuavrkrjrX3nqM", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 972, "prompt": "What is the function of the organ on the bottom of this picture?\n<image>", "pred_response": "Store urine", "answer_id": "LCSx5SHLfWopmFiNP2jjjr", "gt_response": "Store feces, excrete feces", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 973, "prompt": "Which organs/organ in the picture belong to the digestive system?\n<image>", "pred_response": "None", "answer_id": "67rRPRTHq66mcVmqSphu4X", "gt_response": "Rectum", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 974, "prompt": "Does the picture contain the organ which has the effect of discharging waste?\n<image>", "pred_response": "Yes", "answer_id": "kLYQmJam6UMQ6w3nV4g842", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 975, "prompt": "Does the picture contain the organ which has the effect of excreting feces?\n<image>", "pred_response": "No", "answer_id": "DZh2j5s2Eqy2JP2JKT4Nfj", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 976, "prompt": "Is this a coronal section?\n<image>", "pred_response": "No", "answer_id": "6z7mmogFtgwRG7PWCkcSXt", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 977, "prompt": "Which part do the organs in this image locate from the aspect of the whole body?\n<image>", "pred_response": "Lower Abdomen", "answer_id": "c7hHd6CFfY8EDgyuUxRWGW", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 978, "prompt": "Where is rectum in this image?\n<image>", "pred_response": "Left", "answer_id": "JftNZyydDJDVy59FpPvNWS", "gt_response": "Bottom", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 979, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Pelvic", "answer_id": "XLJbJmny2zfXyC9p76Kjcf", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 980, "prompt": "Does the picture contain rectum?\n<image>", "pred_response": "No", "answer_id": "nvSzFvK8SBYjPxgU4ikHpk", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 981, "prompt": "Does the picture contain bladder?\n<image>", "pred_response": "No", "answer_id": "MYuYe3rUfyLDBckegTv4eP", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 982, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "7zNevHYFkN9iWFm3XtZLgn", "gt_response": "Bladder", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 983, "prompt": "What is the effect of the gray organ on the center of this picture?\n<image>", "pred_response": "Lower the spine", "answer_id": "7NEQU6wUYp7Bquqtc3s3TZ", "gt_response": "Store urine", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 984, "prompt": "Which organs/organ is part of the digestive system in this image?\n<image>", "pred_response": "None", "answer_id": "crZhnHJ4yXB5FGHwrpKYA5", "gt_response": "Rectum", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 985, "prompt": "Does the picture contain the organ which has the effect of excreting feces?\n<image>", "pred_response": "No", "answer_id": "iQYtJQrYtuqFHs6fgjYn6c", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 986, "prompt": "Does the picture contain the organ which has the effect of detoxicating?\n<image>", "pred_response": "No", "answer_id": "ghGWgBu93XMFCbUs4GUXLu", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 987, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "e6G6goTw9oZJFvJUSB4QGL", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 988, "prompt": "Is this a study of the pelvic cavity?\n<image>", "pred_response": "Yes", "answer_id": "GDFRmhNCMZ7yruKowsC25E", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 989, "prompt": "Does the picture contain colon?\n<image>", "pred_response": "No", "answer_id": "kLGgB8Bfh8wzBKW3dmft4Q", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 990, "prompt": "Which organs/organ in the picture belong to the urinary system?\n<image>", "pred_response": "Kidney", "answer_id": "6zSGQ5BkPnWAsMLHUpCwjZ", "gt_response": "Bladder", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 991, "prompt": "Are there organs in the picture that can deliver food?\n<image>", "pred_response": "No", "answer_id": "iTc4deU2fZ2Yb8nLYNsjHL", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 992, "prompt": "How was this image taken?\n<image>", "pred_response": "MRI", "answer_id": "bxxkjAFzck3cyPgxH7ipcZ", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 993, "prompt": "Where does the picture show?\n<image>", "pred_response": "Abdomen", "answer_id": "GWvjjMcSEgrBB55ekJ9Xth", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 994, "prompt": "Does the rectum exist in this picture?\n<image>", "pred_response": "No", "answer_id": "ZkTZiDp7gF6bVDc8kBiqD7", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 995, "prompt": "Which is bigger in this image, small bowel or colon?\n<image>", "pred_response": "Small Bowel", "answer_id": "PLaczeMHAavNk7euQyMAQa", "gt_response": "Small Bowel", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 996, "prompt": "What is the function of the organ on the bottom of this image?\n<image>", "pred_response": "Store urine", "answer_id": "5DDqHzpGZ6gj4mAEFrezUv", "gt_response": "Digest food, absorb water, excrete body waste", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 997, "prompt": "Do any of the organs in the picture belong to the urinary system?\n<image>", "pred_response": "No", "answer_id": "n2zTkAMfb2nrocegfzRBrF", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 998, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Abdomen", "answer_id": "33JuW7sVu5yEMKxswqDYqU", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 999, "prompt": "Is there colon in the image?\n<image>", "pred_response": "No", "answer_id": "TokSXe55SnuLSGe6s53X5P", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1000, "prompt": "Which is smaller in this image, colon or small bowel?\n<image>", "pred_response": "Small Bowel", "answer_id": "jfBdxFvXZtjtGGfTMe2DcP", "gt_response": "Colon", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1001, "prompt": "What is the function of the organ on the top of this image?\n<image>", "pred_response": "Store urine, excrete waste", "answer_id": "i4CwrZAbba4dr6CTn3XnFY", "gt_response": "Absorb nutrients, secrete enzymes, digest food", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1002, "prompt": "Do any of the organs in the picture belong to the digestive system?\n<image>", "pred_response": "No", "answer_id": "cNqsgrZawPUpRAEGk87wCk", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1003, "prompt": "Is this a coronal plane?\n<image>", "pred_response": "No", "answer_id": "HfbXzX44DAPa8iFX5suE43", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1004, "prompt": "Which part of the human body is the organ located in the image?\n<image>", "pred_response": "Abdomen", "answer_id": "7HkDHu3EHuKGaH3a3gQTcB", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1005, "prompt": "Where is colon in this image?\n<image>", "pred_response": "Left", "answer_id": "U6kS7eGBuFrgjpTkgGNfv6", "gt_response": "Upper Left", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1006, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Abdomen", "answer_id": "TZMYjcv3zZQTnMWtFbgdzW", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1007, "prompt": "Is there spinal cord in this picture?\n<image>", "pred_response": "No", "answer_id": "gqD8tUotYuJa4NQP9fw72h", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1008, "prompt": "What is the effect of the organ on the left of this picture?\n<image>", "pred_response": "No effect", "answer_id": "fCyAi53GXxrWXG6hfdu8gL", "gt_response": "Digest food, absorb water, excrete body waste", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1009, "prompt": "Which organs/organ is part of the digestive system in this image?\n<image>", "pred_response": "Liver", "answer_id": "SJJxEe2MnDHDtWjaedFFJW", "gt_response": "Colon, Small Bowel", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1010, "prompt": "Are there organs in the picture that can promote blood flow?\n<image>", "pred_response": "Yes", "answer_id": "Rbke6awZBo6rfhzn47HfNu", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1011, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "oEoosyUnXRyDNmJvufV48a", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1012, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "Ueh89MFd3DnwHYxff584N7", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1013, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "fTcWVHBbfVFdaUTNjq5Wth", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1014, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Brain", "answer_id": "UkFoWsDhvXy44kj4qRbUoA", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1015, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "dbddQBxDtD7w5LK9oKhiXc", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1016, "prompt": "Does the picture contain brian?\n<image>", "pred_response": "Yes", "answer_id": "CDgGbs49rWNEMJEGyYvpxm", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1017, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain Tumor", "answer_id": "ee4UqFEkBDCrgQ7kFytEN3", "gt_response": "None", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1018, "prompt": "Is the brain healthy?\n<image>", "pred_response": "No", "answer_id": "oNyjNJnjFwKmLXiaHAiKDT", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1019, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "CiqkfaMZvDXjHP9GHm9Pg8", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1020, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "nQnAb9MJEqF8age4zZLQAr", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1021, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "T2", "answer_id": "Sn4ySmkR8th5qM4wE2QN7F", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1022, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Brain", "answer_id": "XMhq4CMUg3xGKWripexN3u", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1023, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "oKdgakUtgg8HLMHzWe6Ax2", "gt_response": "Brain", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1024, "prompt": "Does the picture contain brian?\n<image>", "pred_response": "Yes", "answer_id": "cFzsGJyUpk9mHLzDyC5hPa", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1025, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain", "answer_id": "YQk9EYAjZts6dC3GsK27Vu", "gt_response": "None", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1026, "prompt": "Is the brain healthy?\n<image>", "pred_response": "No", "answer_id": "3bHDjACREcxnriRadHztHJ", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1027, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "X-Ray", "answer_id": "ddbqBKEKjgXD92djsctobw", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1028, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "7xCutvfXGZhKT3KaMxZmbf", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1029, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "WfGH5MJBTt3SG3i3mH5b5C", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1030, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "FK8TZq3G7DYs2R5ui5j2GS", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1031, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "QhFZtGxYtektwDcCs33neR", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1032, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "52Jo42ArLENLmN9VkbFEAL", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1033, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "FDaoYnCd6Q8MmLJCoMRwPc", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1034, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "6rTCpiAGNFoaaCM38q7UZF", "gt_response": "None", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1035, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "VJGZAKEkXapfhAPDWm4tgE", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1036, "prompt": "Which is the biggest in this image,lung,liver or heart?\n<image>", "pred_response": "Lung", "answer_id": "FkNxi5Vu2J5vqbHL7HMFHm", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1037, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "e4VEivnRSiEUKFDPZiJTWq", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1038, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "fpZCbRC3LFzpyUT2dS6QsZ", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1039, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Liver", "answer_id": "WWNp9KV9uiVAJ3qCe3U53R", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1040, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "2NPjbYZj7Teokuc26cKEA7", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1041, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "8sGFYtVtqzae2wqFwHFrDC", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1042, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "cXNQykg7YPUedwHkH5KrkY", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1043, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "QmSWtnAW3s9kMormukpPw2", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1044, "prompt": "Does the lung look healthy?\n<image>", "pred_response": "No", "answer_id": "F4Gf558WGuUhhVmEXtFi9D", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1045, "prompt": "Which is the biggest in this image,lung,liver or heart?\n<image>", "pred_response": "Lung", "answer_id": "PLF3qDeiMg57A9uqom4dGj", "gt_response": "Liver", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1046, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "ZHzLMQfBCmnjVyxheVkGcd", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1047, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "o9LG5EXr4iGVMU8mzXoZsu", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1048, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "n8Tcr6MhC9c8a7v78mkaY7", "gt_response": "Lung, Heart", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1049, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "6nWamPC4BAZxibgzSQAAMp", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1050, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "jsXoCPjV498fwygZEsgCEs", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1051, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "2C4VK3n7V2QPf7yAzpZPLz", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1052, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "S2Ube934m4C2xGQus7scSj", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1053, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "e82prTWjPwzmX59uXJRz43", "gt_response": "Yes", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1054, "prompt": "Which is the biggest in this image,lung,liver or heart?\n<image>", "pred_response": "Lung", "answer_id": "Pp8DbTYEWgZXFCZQtWivxT", "gt_response": "Lung", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1055, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "MRI", "answer_id": "nJD3dkVujMBBhewpdkYsCf", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1056, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "EcqCNtA9eQ9ZdNskCURjV6", "gt_response": "Chest", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1057, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "HhNcAKvBceZZdDsTzkHqPp", "gt_response": "Spinal Cord", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1058, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "gbbeLCfuzr7DwomPaAYDpQ", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1059, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "5owt2DMKiLxw3WQtKvSHgS", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1060, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "6Tvt4nNs8VjmJX4mgMM7Mz", "gt_response": "No", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1061, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "NEp2YWz6DWjKoTGwGm2RZr", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1062, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "nMieHzXebFACCTjTTcxs5a", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1063, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "MNq6qtTpug3bB3qRSGLFFw", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1064, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "RFq3g7cvW7YfakMTUqs4pP", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1065, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "DGDuA4RLtMAYCPSTJrJBCD", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1066, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "hqYux5cHy6rNHPLpV39xh7", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1067, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b", "answer_id": "NwszsojGYrSZdm9XFUUBxP", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1068, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "EiczK6rkcM98sDiZTQmGFJ", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1069, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "SpYBDYyVsdDcxifBEF2VyW", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1070, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "BbgrRtV9fMgEuQWhakkKBN", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1071, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "9Riss6kVExx48zsRZ7DnKp", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1072, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "ZVmDnRerAmRH5o277JATXT", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1073, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "3DTWPuWQULaVPg6nzc9nSY", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1074, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "mr6h32AMiTfJmcahKXjnKP", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1075, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u708e", "answer_id": "9Azinfofr52bugZHejiKns", "gt_response": "\u80ba\u764c", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1076, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "fDiuuTLo8FfiRDoSR4mEMR", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1077, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u80ba, \u53f3\u4e0a\u80ba, \u5de6\u4e0a\u80ba, \u53f3\u4e0b\u80ba", "answer_id": "eGurevwFVVLEGPrg8dZXaq", "gt_response": "\u53f3\u80ba, \u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1078, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "Txb82UZmZTdbfQDf7Tw8UX", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1079, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "FzjQY7yWTMEvudWfcMF7VR", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1080, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "RWgApPsVDmynW7iYcAa223", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1081, "prompt": "\u813e\u810f\u662f\u5426\u5b58\u5728\u5f02\u5e38?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "cqXGMQ3BvdDfhGY2sdDvPg", "gt_response": "\u5426", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1082, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "5ttVabEKgGyNp9ocHq6pJ6", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1083, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "8zPZLVkv3S82ftbWRCrdwj", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1084, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "kHNwiE8Jp4jqP5LnVs3Gqs", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1085, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u809d", "answer_id": "5AtmaXj65pB2XnoxTk5yDz", "gt_response": "\u53f3\u4e0b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1086, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "GyPRTeJaYAC7D3vyLQsiC2", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1087, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "SiowYYk4fGb4UrCUumFkK6", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1088, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "QT8DLufqL2BX7A9MyBNVBe", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1089, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "nGfYeYSygKY7uPwWKZGKkB", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1090, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "SgyEBLK9sYkzX5iTpemgwH", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1091, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "GhvZdE2ws8LapqqBqzDvJ2", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1092, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u6c14\u80bf", "answer_id": "CYoqsGtrdfcCY25A2N6V5M", "gt_response": "\u80ba\u764c", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1093, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "SFQNbk45fYAtehgUqUbcGZ", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1094, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u80ba", "answer_id": "a5Lx9yQeBJGzfAiAbchffr", "gt_response": "\u53f3\u80ba, \u5de6\u4e0b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1095, "prompt": "\u56fe\u4e2d\u80ba\u3001\u5fc3\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u80ba", "answer_id": "joyxxo9VUBbVaYDLZZwbH3", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1096, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "6HAtqZpixncPRZY5nywDzZ", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1097, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "gzrF7jJeapbYKhyUpubWDZ", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1098, "prompt": "\u56fe\u4e2d\u5b58\u5728\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "L2Ve3nKqSRB3NNbVLvYGdM", "gt_response": "\u5b58\u5728", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1099, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "kxJz9MQzNpDyabGDcsCY5t", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1100, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u6c14\u80bf", "answer_id": "C5p9MGo2yoyQdyYJDBCXHK", "gt_response": "\u5fc3\u810f\u80a5\u5927", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1101, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u80ba", "answer_id": "bzBydHVfaBJz8hkKtY6SAs", "gt_response": "\u4e2d\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1102, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u662f\u4e0d\u6b63\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "kMfHgvNnL3r8Qn5ym5c4qh", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1103, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "8Terzj4Rw7yt728szD8ffy", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1104, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "jccbXKmUf9DExC6EVDou9R", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1105, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "67nWNKxANn7CR3Y8TgUY6J", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1106, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "MiZmBMGvRkgfpWes4owzSG", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1107, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u708e", "answer_id": "DTw26DXZULBEbpRNTgRrXk", "gt_response": "\u5fc3\u810f\u80a5\u5927", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1108, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u80ba", "answer_id": "YCAt3BNhfeLajJFoJJRT3k", "gt_response": "\u4e2d\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1109, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "jKDzeYdUgXsp5nmpE36u76", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1110, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "gaHnhRpppGpa5kiTPKFdJ4", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1111, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "XQXAhwz5M6acnauXmdCBAt", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1112, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "42D7NrDbmH54UBknaqZo78", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1113, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "cuEajQK686wJd2XUkgeT4a", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1114, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u6c14\u80bf", "answer_id": "cNYWrxTYSnkB2QRzTiiYPX", "gt_response": "\u80ba\u4e0d\u5f20", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1115, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u80ba", "answer_id": "NUF8H8mYtq3FBaw4A2qFnx", "gt_response": "\u5de6\u80ba,\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1116, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "grR4LYwr9ogcaJXbVPwn5E", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1117, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "PYpEg7z99G2j4rwqjQ6Xqe", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1118, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "FFkQX3si9twFo9MAufPg9G", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1119, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "KTRcbJMwg5qnBRrrW7UQZS", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1120, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "CVKz5tz3vBEQpF3AfKRA4g", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1121, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "KJ7nmZ6iTNbTyyCU6VVoTh", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1122, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u6c14\u80bf", "answer_id": "EfxFrcRZwGQXDNxQiCuJ5g", "gt_response": "\u80ba\u7ed3\u8282", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1123, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u80ba", "answer_id": "S36LDJPZm7SkisN2Ry2YEz", "gt_response": "\u53f3\u80ba, \u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1124, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "CRTZi9mHuUeAJ772MTMzNA", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1125, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "iHgxaUriWR4LNnALKZXQgK", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1126, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "RKQhAZYFUrm7Bdw3SjwjYs", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1127, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "fB6fLKnWHovVvkKNEQpNRK", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1128, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "TJbbUryrJgi7F5hTZboLzS", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1129, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "YppR97P6N3tjb2dDZ4wBSW", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1130, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u708e", "answer_id": "P88ecm89jryaPaEMhpBaNm", "gt_response": "\u80ba\u7ed3\u8282", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1131, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u80f8\u8154", "answer_id": "9wDXxHpGqA95nJ8zccDZMB", "gt_response": "\u53f3\u80ba, \u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1132, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "LyTiKUK6VFYEyy8DPpCCtF", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1133, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "4T2s8xAowsR4mQpthQLe4q", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1134, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "dEQdTBZXVDFjfQkdoRaFuy", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1135, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "JMfzvQE5uFLy7Kr9jZoyBT", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1136, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "gL36xJynxW7mQxn8qJtW7D", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1137, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "Bfq2xYkLfJZ5aMoFMehLpd", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1138, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u6c14\u80bf", "answer_id": "Y2CxGMrMwWLfo8jCUm6UDc", "gt_response": "\u80ba\u7ed3\u8282", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1139, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u80ba", "answer_id": "nXmrqJ9aEa9vVbDH6wB7oW", "gt_response": "\u5de6\u80ba,\u53f3\u4e0a", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1140, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "m5wNyifk6NqFFxYHtmAt9T", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1141, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "ZY2HkqJHeakkiGhR5jp5Ly", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1142, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "CbUv9EoMQ42WL9CF9kt6So", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1143, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "AyAn7Sp4zqBRHV8UqfARJ2", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1144, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "eGv7obPtPSBMtJ8nnDBPAG", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1145, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "KwHv2vsHtyYuaD9myMbuX9", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1146, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u708e", "answer_id": "2fFhVBKtPzG4AJCMRew3QY", "gt_response": "\u80ba\u708e", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1147, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0a\u80f8\u8154", "answer_id": "a9TR5mrG5LL8Jkmdf6vUBk", "gt_response": "\u5de6\u80ba, \u53f3\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1148, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "MMutZonYmzBiPkotjw2PNq", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1149, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "iXwUePGDALi2cgwUjGV9da", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1150, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "9BEcp77ZeZg6ESK7oH72wn", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1151, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "bxgLmyppmcJKTzmv7Lyiwi", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1152, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "KoZvS4b9cTCyAAhqb8wCGS", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1153, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "hiFdtfjxDihqB4vZXyebMP", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1154, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u6c14\u80bf", "answer_id": "Akf6nxJaj7TEr9WBpVUxk3", "gt_response": "\u80ba\u80bf\u5757", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1155, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u80ba", "answer_id": "ngbFMyEALFQvzjrMXwtsfV", "gt_response": "\u53f3\u80ba,\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1156, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "hciV4f2GuQLXtWHnFdiCps", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1157, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "Vvpv5mpgvH8JzzDCHZCX4K", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1158, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "JA2xaAQM9tktF2z5RavBsu", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1159, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "REk5mF4woCrUizdjnXE3vK", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1160, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "hrufgMxCUNDtVXbLW5zEPw", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1161, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "RzGQqPEfRJZFDpQYyEcVDv", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1162, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u708e", "answer_id": "dJFK6Hydo7JvYWAz2fzHL8", "gt_response": "\u80ba\u7ed3\u8282", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1163, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u53f3\u80ba", "answer_id": "4LAxd5avbx95qQh9Hd6M4z", "gt_response": "\u53f3\u80ba,\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1164, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "WKBuCzKfNV3YJErzpX9cVN", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1165, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "6j92yzMSND3rzDRJotkKdZ", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1166, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "cdWa8wj5MyDw9LvrZrQrwW", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1167, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "W6pk2nLy4QvJVUELAo34pk", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1168, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Qi5DHpKGyjViAkWh4bQXjX", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1169, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "oWxqEnVSYY5aNW4Wd6oMXs", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1170, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u6c14\u80bf", "answer_id": "URtpdx2LLhn7E3C35mJx3k", "gt_response": "\u6c14\u80f8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1171, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u80ba", "answer_id": "KP3Zcafd3BQf5FbXitowtd", "gt_response": "\u53f3\u80ba,\u5de6\u4e0a", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1172, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "9yYUvdT5JeBmWCQWUMctxq", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1173, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "drL8saU5uksBbAeu496Evk", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1174, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "PADFhGn6iqnx3aQLSnkbXF", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1175, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "ZrYmRY6fxqR2BuY6CZ9ujM", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1176, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "gPiaFqYQYrDGYmo8V2EaYu", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1177, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "QTcUubu5CmjqfdekkH4Lju", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1178, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u6c14\u80bf", "answer_id": "2pT2qh8P3VkKReYgG6sBMW", "gt_response": "\u6c14\u80f8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1179, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u80ba", "answer_id": "3tex5ESCTWghuem6i5Q7gY", "gt_response": "\u53f3\u80ba,\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1180, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "MvnTPXDxeUBtMJaADfLPvC", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1181, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "nWMs3JD4iDQpqWLKzgc6VN", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1182, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "ZoTHgrs9owRQuDpaBE5vMy", "gt_response": "X-Ray", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1183, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "nQmayh3iEpJzUDN3iifosR", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1184, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "XhwAFmNfz4YKMNGPwjgTbb", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1185, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "CHrQSNU999UGmpV2YXn7bq", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1186, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u708e", "answer_id": "8oAnSLpjNSwcWJ79n2CSLK", "gt_response": "\u80ba\u4e0d\u5f20", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1187, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u80f8\u8154", "answer_id": "6rXphkzUF6ncKunna9VtuE", "gt_response": "\u5de6\u80ba,\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1188, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "WW3QBQkbqboVnX5DNjrFFd", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1189, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "RrsBwY8TKsHURSKgsFfHER", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1190, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "cWVMtwqahfTcjkYeMZCKVK", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1191, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "7aGHUCJckLhRLdmoJ7qS9e", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1192, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "Ge6JGPSG5maXN7SXw4navo", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1193, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "PG76NvzWoD8gZbXtyRMXz8", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1194, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "3hBWis87pMDWkQoBmepPGa", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1195, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "LvyhJx8CYHhXCcmAHKtSxL", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1196, "prompt": "\u809d\u810f\u548c\u813e\u810f\u8c01\u7684\u4f53\u79ef\u5927?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "emPB5USZ2dTRQxjEHQcSRr", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1197, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "WqXE8ZDb4tygiTwNUZoBo6", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1198, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "cQQaohFmx3QFN4qT2gdb2X", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1199, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u662f\u5426\u6b63\u5e38?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "P8CktsbVZW8P3inptGj8Kk", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1200, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "e5AifWgSVUhnS9L8f2EX6A", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1201, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "JRfiQ326n3hRby73VqPboA", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1202, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "PqxLy8oC2BRzqPstGfSqtz", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1203, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "W6jMCnpTBdK8kExhzXmftz", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1204, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u809d\u810f\u708e", "answer_id": "ZfZHTTRpD7DVKrGFhLnrZG", "gt_response": "\u809d\u764c", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1205, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b", "answer_id": "bDtoDW9pDjEXyHV29tSFGq", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1206, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "4zvkpsrn6KbFNhwSxd7dJQ", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1207, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "L26p8hxHcPpoL6rMWR3iaS", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1208, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u809d", "answer_id": "kWkjMzPejujmM2ktLKm43n", "gt_response": "\u809d\u810f,\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1209, "prompt": "\u56fe\u4e2d\u80be\u810f\u548c\u813e\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u53f3\u4fa7", "answer_id": "k9369LnbtrRSmy8zXkUZY7", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1210, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "93HC4mZN9oiELzhPjxmo3U", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1211, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "iu3hgh4EnbDadk9TAgWNyX", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1212, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "X4DB9pVe5PfYkaryqiKCXz", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1213, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "QuK6DZLsoqJsYzDLPYkNZg", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1214, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "HJmyzuk6QQvW9MZEaPbAmt", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1215, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "oXkeXpnB37Pu8PzDhPA9bb", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1216, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "RupBg98otQ4yjB7zJfhmRi", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1217, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "TUEMfxfZUwtanxrTWwZm2f", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1218, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u809d", "answer_id": "W75MRd4sPsPEvUoET3V8Zw", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1219, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "NrFajtitLUsc28tHdwm8QE", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1220, "prompt": "\u56fe\u4e2d\u80be\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "L9wjSy7CcF7yhZ38FMDtQZ", "gt_response": "\u80be\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1221, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u80be\u810f", "answer_id": "nfnDkCgH3L7vDtcqwD3ZKS", "gt_response": "\u5de6\u80be", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1222, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "8GPEd56Q4FshHSqSVdeDaG", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1223, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "eDPaVKx6yYbkdiz7XpLTr8", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1224, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "67LbB9dmGCMY3e724LKHz2", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1225, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "FQiFTHvwiMNk6KFm22bmrj", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1226, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u662f\u5426\u6b63\u5e38?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "6vQBqhm6tsx8nS9FA9CD84", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1227, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "6srfhqVU2ZSaQDdPhHAiG8", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1228, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "AdgzXfcbsaEpcbmbHUPhTx", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1229, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "inR6922CGWVFRD5mBb73vg", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1230, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "UySFcJYQcG9Z75hD9SXArV", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1231, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "PR3LcX2xH86gYvwpcqrpMw", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1232, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "P2biSrEPDScssngBDas96R", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1233, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "GHpTJwUU9hKKws3XReTn9X", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1234, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "YdL5h4aQnR2pp4dnWtxuLC", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1235, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "GzTrDqrBvwrYSUXiG7kYQc", "gt_response": "\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1236, "prompt": "\u56fe\u4e2d\u80be\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "RAnzrdShuoyQagL2rsYoYt", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1237, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "3ULXMWTuQQxxtQQATkFhtp", "gt_response": "\u5de6\u80be", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1238, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "4bM9LMv8bcKoosmRZ74AqW", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1239, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "hJneqEruxF6R3rtmdrQf2m", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1240, "prompt": "\u8fd9\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "WxTrw587VUT3xAyeJqx8eH", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1241, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "8rYcDkH3NDJQpoFyg8i5Xt", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1242, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "fnGhWMshKemRFwHYAqxdy5", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1243, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "9ozogFQhLFJALHstNx2zZ9", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1244, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "AXTfXVwGS83cdnR68QiGRm", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1245, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "CSDVKLcJrA5nMEdiJsaRU9", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1246, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "bewvd6UWtFHzCwKi6P9CfK", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1247, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "krvfYMFLZei2ez2atb2ecL", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1248, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b", "answer_id": "9cooayfQsXFyMAjTYfBqHD", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1249, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "KNhyJpfiQykX9HiLrNPB8A", "gt_response": "1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1250, "prompt": "\u56fe\u4e2d\u80be\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "bexCUfYQ3qyVZg7ByyjC9H", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1251, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u6700\u53f3\u4fa7\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "h7TZCJhhaAe8HppBGfRCkZ", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1252, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "ED9BJ5XUWJscujSbHnKhyK", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1253, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "iaVg4TDkvBwRnYjdLnsFAu", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1254, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "EPNfzLkjy62bu6sGuy8Vyq", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1255, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "dKcMLZpNG4eXNx89oG34zS", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1256, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "2D", "answer_id": "jQPToBbTq9P4qpnW3CrzaT", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1257, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "6QTaeW3QY4DVwrJZuRy5bx", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1258, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "YfVT25vKjUARg5n2587Nxs", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1259, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "gwoZuFVuWbn2eBZU3i4rDb", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1260, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "GrCZceTwTPsCru4GuMEC7r", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1261, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "4o7sazd9qiMGK6AuxagCq5", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1262, "prompt": "\u813e\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "hSvjmAU6KMy7KggrfrY8Bg", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1263, "prompt": "\u56fe\u4e2d\u809d\u810f\u548c\u813e\u810f\u54ea\u4e2a\u4f53\u79ef\u66f4\u5927?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "9fnPKoPvDE3WzjiMkd6oEs", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1264, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "4YYwzALMSdvaRQvnFHSLw7", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1265, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "neCKFdBucaWmGndn88iykZ", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1266, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "QX5vieYYEzVX6LmW7Z3T7s", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1267, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "hxCy36YFHSH2SyzQJH9TgL", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1268, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "iHWpT5gww7VEAGH7bNTtQz", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1269, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "iFx2DqPtVkUCGrbK9ywoSX", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1270, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "FhyXFzJEyXcUg4Tdbza3j2", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1271, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b", "answer_id": "ZSRwrdD2rAUgyW8W9xSqek", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1272, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "oR8AkHJrcy9oUEcCyVSCrz", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1273, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "kGYZWigbyEjCDRKD4VQTKR", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1274, "prompt": "\u56fe\u4e2d\u5de6\u80be\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u5de6\u80be\u810f", "answer_id": "Gjpvvw9Z9UWNFTRk2SNyMJ", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1275, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u80be\u810f", "answer_id": "hGTur6SLv8HEGfytv9guB4", "gt_response": "\u5de6\u80be", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1276, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "XME4ZhLNKKzMWMvCZApbJg", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1277, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "KUTPGtYW5UdSJxRWPneuet", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1278, "prompt": "\u56fe\u4e2d\u80be\u810f\u7684\u5f62\u72b6\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u4e0d\u89c4\u5219", "answer_id": "MFW6Sz8VDQ54PgiYj6VNKR", "gt_response": "\u4e0d\u89c4\u5219", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1279, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "ekaYfL7ucCLxjznSg5c8k7", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1280, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "7cR9WtghK6TmRBogxRcvyx", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1281, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "Prd5mUnaLWt7cwVU68odYZ", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1282, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "fCgN76D7V34Mihd7Wjj7AB", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1283, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "hn9wFJ5hbWdqniNetG6zFb", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1284, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "DwdD9eXDLuMynVXGAYrjML", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1285, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "WUnzi3qZksZQs6zCYgzb9z", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1286, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "BWduyf2R2YTJy9MGi5hZCF", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1287, "prompt": "\u56fe\u4e2d\u809d\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4e0b\u809d", "answer_id": "Wfrca7riXYdT3QTkSv3ni7", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1288, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "V3bDX8PXyYNyMmgdsiz6Y7", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1289, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "7DSr6MdNQMiFWMk8TXqDAt", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1290, "prompt": "\u56fe\u50cf\u53f3\u4fa7\u7684\u9ed1\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "dJKocV3NHzQRFBnbtznhcG", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1291, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u7070\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "iN3F6hvDHmyYbZ8HpTwk9F", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1292, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u9ed1\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "3V2XcPXuL42PKKuJ8MFcVG", "gt_response": "\u53f3\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1293, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "CEuDzeAans4wWZKKvVNDQf", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1294, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "SkTUBvGfQWMRmfExDH2pCu", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1295, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "PCpqwueAB2H9Qh5kiAAgsY", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1296, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u662f\u5426\u6b63\u5e38?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "C5X72YroDDRjZu3ZfVsTdk", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1297, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "KLgzdQRZkMwis38yZtqWKK", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1298, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "AuJxJpyWFjJyrzGbQPZUxM", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1299, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "Ei9XV6VqaeyECF57rYvieL", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1300, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "itUjSG3bmVjj4JQ7uDbZSG", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1301, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "2VwVPbnnv2XgTxSbmKA3mm", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1302, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "N5dpbrDsmrDrkn4z9j73x9", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1303, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u809d\u810f\u80be\u810f", "answer_id": "j3qUehgGiHgSBaNG7qXvEy", "gt_response": "\u809d\u764c", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1304, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "8KfkuZQEaf7wBwtchrKeWD", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1305, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "EuGPjSVcYTQ2Ksm9uVU5Tx", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1306, "prompt": "\u56fe\u4e2d\u809d\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4e0b\u809d", "answer_id": "if65bBynGM3TV3YC6gG2rp", "gt_response": "\u5de6\u4e0b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1307, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3", "answer_id": "8deBhXEfZKQ4kZmTa3cfnS", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1308, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "b8rJigQoUwXJ2LjrjPWXMs", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1309, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u7070\u989c\u8272\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "iAzfT7M7rWgQ3hFqUdwGNJ", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1310, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u7070\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "K3FuRHKNtUv58zgdNT5MpE", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1311, "prompt": "\u56fe\u50cf\u4e2d\u8eab\u4f53\u9876\u90e8\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "jRiuHJWFJmJ9LxHjqqqPCC", "gt_response": "\u80a0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1312, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "CEybGUb8PExbrbKErrPehY", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1313, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "mbSSveRtfWVDTmveFatLP4", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1314, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "XAqQwupY6iWvRLVMeruSAN", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1315, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u662f\u5426\u6b63\u5e38?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "KMNRncxUjz46RP23DKUnDS", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1316, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "E22NbuujScAQiLYYg9RXNy", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1317, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "Mt3YvWsKAVYGkE8kjdpcfb", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1318, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "EkihF7j8GhyZMX4GJA5mtV", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1319, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "Y3nN9uiFxbovQS56bvdx8b", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1320, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "CWdJWiQ5i5PWSKGaAxoSED", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1321, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "AqwviWUmAcKAWdyrAP54Kk", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1322, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "juaqrpkYwArAMmvFhaAq2q", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1323, "prompt": "\u56fe\u4e2d\u809d\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4e0b\u809d", "answer_id": "CrSnjfTYUeUPpNKfGvmw2o", "gt_response": "\u5de6\u4e0b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1324, "prompt": "\u54ea\u4e00\u4e2a\u662f\u809d\u810f?\n<image>", "pred_response": "\u5de6\u4e0b", "answer_id": "Ymz3Ay9wRPvFBPP6K8gqBA", "gt_response": "\u5de6\u4fa7\u5448\u7070\u72b6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1325, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "7G5KQZ7NcXQ29VQgT2CB3R", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1326, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "XSBYRDsXBqcifa8LW745cX", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1327, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3", "answer_id": "NPjKrcDNV2HkpyBdGXpXAj", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1328, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "Y74ZZhYNBcYEh8gNqkZwbM", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1329, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "UVbJ6NLWMsbooncv2x4vEV", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1330, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "JJ35eubRUPhaehmaTiY662", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1331, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "L36nfE24Hf6wGXBKgEeBeW", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1332, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u662f\u5426\u6b63\u5e38?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "GosYsCxPSt2LSgdbCAEqQQ", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1333, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "U5xZZ3xKqqixcn2zesT4cY", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1334, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "QsvbGS35mEPRWQv2aicpr4", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1335, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "SP84o5FBak9wiT57ALu5qc", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1336, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "TTQs7Y7btW52bMiJTLVNDo", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1337, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "AUA9RWhMwgDC8AuN5XqywF", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1338, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "DhAUttdL27mPZVkCnT9Z7D", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1339, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "oLFgrF3yN4SF7SWTLyHakc", "gt_response": "\u809d\u764c", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1340, "prompt": "\u54ea\u4e00\u4e2a\u662f\u809d\u810f?\n<image>", "pred_response": "\u5de6\u4e0b", "answer_id": "nc2ucY7wbQK5PRnYwHETu9", "gt_response": "\u5de6\u4fa7\u5448\u7070\u8272\u72b6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1341, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "XGcYPxcAfxEUemzihuRrqD", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1342, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3\u4e0a\u809d,\u5de6\u4e0b\u809d,\u53f3", "answer_id": "7ZYVY769MFgEUMHKKaDJKs", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1343, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u80be\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u80be\u810f", "answer_id": "74atYmMBG5vSrz5nyhaUir", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1344, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "kxa2d9bcQCiDxWbj3Dbjjk", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1345, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u7070\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "6tRs7YZS6JPCRw5JA8kejC", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1346, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "G2YVuGVxHfi2g5jEUAxX7o", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1347, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "QSCUjx73U8domWidmJ7UfJ", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1348, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "SnvqodCQTxHL5ryyCrBL2f", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1349, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "hjjZc76Q6NvLmoMQPTXstt", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1350, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "NKuQQok7BKkaqLgrkFxjwD", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1351, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "MKynKKEupoSuNFjbDfVNFN", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1352, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Coqw84r4HY6zPjn6ivvx7b", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1353, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "aX547HEvSF7mK9gJLqahYC", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1354, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "JHhnt7c8VnoVEwxGZK5dAo", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1355, "prompt": "\u56fe\u4e2d\u5fc3\u810f\u548c\u80ba\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u80ba", "answer_id": "FNutaZCJL93MHuQVRfFDKv", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1356, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u6700\u53f3\u4fa7\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "QUZBjxnfSxZVt3D8qEskNd", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1357, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u9ed1\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u80ba", "answer_id": "ZPuAZ9PfM9RsoY4Xov6A7Z", "gt_response": "\u53f3\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1358, "prompt": "\u56fe\u50cf\u4e2d\u8eab\u4f53\u9876\u90e8\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "8onuQPZVaCoqyJa2nAaLem", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1359, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "5hFpYSMgsu96io9QpiQxeF", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1360, "prompt": "\u56fe\u4e2d\u8eab\u4f53\u9ed1\u8272\u90e8\u5206\u5185\u7684\u767d\u8272\u8109\u7edc\u72b6\u7269\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "WeHD2JiPgJkJJ3rGJSgLHZ", "gt_response": "\u80ba\u652f\u6c14\u7ba1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1361, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "hQuK9EgwcurK7NJV6fzZGT", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1362, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "hqX9KuWnaFYcyjE7So5t6S", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1363, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "RCTcKWAdPBKAKtVurvYccU", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1364, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "monjzrKrUtDtV99LiHJDVs", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1365, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "cr4wPoCyXLgk9wH9MwCAEa", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1366, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "7hcdHQys9hjBbhmNEKyUyo", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1367, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "o72nucfPM9kh5t8f6qPso7", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1368, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "NmY8rkF7TmyjX8iyhBKZZ9", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1369, "prompt": "\u80c3\u5728\u56fe\u7247\u4e2d\u51fa\u73b0\u4e86\u5417?\n<image>", "pred_response": "\u4e0d\u51fa\u73b0", "answer_id": "f22gUn3t4TD3ANKJmaPoem", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1370, "prompt": "\u56fe\u4e2d\u809d\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4e0b", "answer_id": "3KYeFxWDsmfhVPM7icSay3", "gt_response": "\u5de6\u4fa7\u548c\u4e0a\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1371, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "4WmoLeUwADfnyh7S9DGsz2", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1372, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "5s3xU6VJewxzMjowmmupgb", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1373, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u9ed1\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "kSUcQhbLnmzarFe9HJxR6K", "gt_response": "\u53f3\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1374, "prompt": "\u56fe\u50cf\u4e2d\u8eab\u4f53\u9876\u90e8\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "iN2LQnTa5DgGm9YQWbEqH7", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1375, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "SMkbrmmUrqdTuu3W9javck", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1376, "prompt": "\u56fe\u50cf\u53f3\u4e0a\u89d2\u7684\u9ed1\u8272\u7a7a\u6d1e\u4ee3\u8868\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "cykpXEsnRt2BrJLvyACvsh", "gt_response": "\u80c3", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1377, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "NqytVTTCSf7qiADs9WNGez", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1378, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "jj7M7nNWriqpdBio2asJjV", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1379, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "YiTbXXwbFreoZufB4XMq6q", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1380, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "oUYWDezmwyz684aP3xHRzp", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1381, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "PXZFanu49DZJiz7vuQejyv", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1382, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "dqrwX3R9MiM7AtvrKCUQNS", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1383, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "6G63Yekc7L6wpcdS33JVTs", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1384, "prompt": "\u56fe\u4e2d\u809d\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4e0b", "answer_id": "Zdb6dF4oMk3satkYBnHdRw", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1385, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "NekdUjxJyYjCsXBfKTvzQg", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1386, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "RLFKLzNXBmYWKbT6nwnc6F", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1387, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "TYC4TxJ7RQ5D6FJLrQLzqS", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1388, "prompt": "\u56fe\u50cf\u4e2d\u8eab\u4f53\u9876\u90e8\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "MUxMLMqJ5hKH6y45dR4gFK", "gt_response": "\u5927\u80a0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1389, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "438KgPYEysnSay8nuTGbcL", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1390, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "9jnRqoWjrC4DgVYZbnwCBL", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1391, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "m5U2gLJeW3foDGEhQRPoL7", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1392, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "Z9jLgwDkStTXYKkThHiSKz", "gt_response": "\u5c0f\u80a0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1393, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "JxBzbe7TDCZwXFua4voFVo", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1394, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Q4yaAooid9xhqwVpkj6bgP", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1395, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "8VDx888ps2HFMA3ovinuzB", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1396, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "ETPyTVZaZLdcMfxpABZpTx", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1397, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "SQ7vSqKrffd5Hwr9ZyJeKz", "gt_response": "\u5de6\u4fa7\u80be\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1398, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "g3MJbysgTLGHKtyWcJ5z6F", "gt_response": "\u5927\u80a0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1399, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "DcSHwjzCc7t2GHZg7BMrwk", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1400, "prompt": "\u8eab\u4f53\u5de6\u4e0b\u89d2\u7070\u8272\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80be\u810f", "answer_id": "LM5stQoFZDPtYvHNWScWXi", "gt_response": "\u53f3\u4fa7\u80be\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1401, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "k3zEwJ48GdJKJtWvZh2mrj", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1402, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "Xc5C8UMpEvZpSDsY8g6nW2", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1403, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "7Q9Vb6rLQZP9sYDytwjecs", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1404, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "hUmJZZefUZyC38rVoc84fQ", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1405, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "HZk9ug3wRS7mrpmgQnm4YJ", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1406, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "GrCEKNyPmWnZKdESRjn3jb", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1407, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "ejTWCnVUPvLsjBcEJBrRA8", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1408, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Snjs5Mofwr5rarm6wqbzaA", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1409, "prompt": "\u56fe\u4e2d\u5fc3\u810f\u3001\u80ba\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "ZQ3hFh4EyXK4UWZZMQQ6fD", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1410, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u6700\u53f3\u4fa7\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "Pw35q3PMifoRAsHkoiQLBn", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1411, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u7070\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u80ba", "answer_id": "mK7PRyM8u83RPHVbDy8sHM", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1412, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u9ed1\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "bibncYpGVporczKXBMqrTk", "gt_response": "\u53f3\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1413, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "bW3gHjvTk7QLASKFmgbo3S", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1414, "prompt": "\u56fe\u4e2d\u75c5\u4eba\u662f\u5426\u6709\u809d\u764c?\n<image>", "pred_response": "\u6709", "answer_id": "Rcneaj7eLz6WE5epkLjQaL", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1415, "prompt": "\u8fd9\u662fCT\u626b\u63cf\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "mQGPG7qLsa3cJ8HiuSizPh", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1416, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u51a0\u72b6\u9762", "answer_id": "fr6jyMv9aYfWLVmS2vVK2A", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1417, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "WtaDf4pSBiZ3nZ6KZJygyP", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1418, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "4Do4pnu6NJ7Mkv5fCefG7r", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1419, "prompt": "\u80bf\u5757\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u80ba", "answer_id": "DCRieAkSmnmT8c2SrKkudf", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1420, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "Z8JRVwswXx25RxFBjjvReQ", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1421, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u8179\u90e8\u4e48?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "drSZAnVHFDgmGBrzEQi9u4", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1422, "prompt": "\u75c5\u4eba\u7684\u5de6\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u6709", "answer_id": "HVLeDf7Ynnqd7MpjGHZTvV", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1423, "prompt": "\u80ba\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "RPfrXyyNF7BaSD6Y7XcHhX", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1424, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80bf\u5757\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "njBhkNYj8HNe2YuHyVThxU", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1425, "prompt": "\u56fe\u4e2d\u7684\u80ba\u662f\u4ec0\u4e48\u989c\u8272\u7684?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "b9C7zjk9875ujQwTUYH7Vz", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1426, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u5fc3\u810f?\n<image>", "pred_response": "2", "answer_id": "96QrcXyGnchsaBgYLcPaaX", "gt_response": "1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1427, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u662f\u5f88\u5927", "answer_id": "kzwJ7X48TotDNcNHhWVQrN", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1428, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u80ba\u90e8\u5de6\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u836f\u7269\u6cbb\u7597", "answer_id": "hWe7uRrw4Qwxd9RQaDxFhX", "gt_response": "\u836f\u7269\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1429, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "LB9aDyAXGuQNiVxoff7XzT", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1430, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "4hStKHnqUgzAMhuH3xWvzR", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1431, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "8Wy8qkHbZ2kBn983mRqBJA", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1432, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "2D", "answer_id": "Druqes8Pge5TqXbEid6mge", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1433, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "TiUzz8zHjnCXaEzJjHdzJK", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1434, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "aWdMVQvVRPKaxBDJKtiBmc", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1435, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "cUEcE3rK9RGpqPir6eYiFb", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1436, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "hSxBRuUScXZaVgAnFfwi7s", "gt_response": "\u5de6\u4e0a", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1437, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "LHnGVkM2xP4KQCLa2hH8uv", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1438, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "6aWbPxqtkfSHYzXF49CpSi", "gt_response": "X\u5149", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1439, "prompt": "\u8fd9\u662f\u6a2a\u65ad\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "ADWaoezv7zFhykuCMX5ARF", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1440, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "P34KXun7axQqZmB2MajdHQ", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1441, "prompt": "\u56fe\u4e2d\u5fc3\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4e0b\u80ba", "answer_id": "GgKbEYvYxCkWQNAV9qw2y6", "gt_response": "\u4e2d\u5fc3", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1442, "prompt": "\u80ba\u4e0d\u5f20\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4e0b\u80ba", "answer_id": "MpUSpogYxA8dgYUfjr3xip", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1443, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "4fwTtwrH6RVT8np2cEFo5H", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1444, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "BntMg2d6tNU3bDYXcxsV44", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1445, "prompt": "\u8fd9\u5f20\u56fe\u7247\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "NC6F8dbwJoD55pZGkBX4tf", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1446, "prompt": "\u75c5\u4eba\u7684\u5de6\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u6709", "answer_id": "9FtJqgBQ5vFptVyQyV2XBh", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1447, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u4e0d\u5f20\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "GycYMyuzXseyGwakvbQGQj", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1448, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "hHySVKXvE87epY47HHLP72", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1449, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "EcsdkrPSg9MeVa97YNhbcm", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1450, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u662f\u5f88\u5927", "answer_id": "GF2hQ2t6KJGEGX9bRnG38S", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1451, "prompt": "\u56fe\u4e2d\u53f3\u4e0b\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u547c\u5438\u56f0\u96be", "answer_id": "ivvqrxyY6G5DwCrcgrWHj3", "gt_response": "\u80f8\u95f7,\u6c14\u6025,\u547c\u5438\u56f0\u96be,\u5e72\u54b3", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1452, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "o9BswU3WqaJLeJ4fmUiLUw", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1453, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u663e\u793a\u7684\u662f\u54ea\u79cd\u7c7b\u578b\u7684\u6210\u50cf,\u6838\u78c1\u5171\u632f\u3001CT\u8fd8\u662fX\u5149?\n<image>", "pred_response": "X\u5149", "answer_id": "iggXxeuHfvU9QmA9trRqLP", "gt_response": "X\u5149", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1454, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "dfCJwy2mEeLTU9zEnkyxrv", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1455, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "XXGX6SoJHp9WBbvjWUcow8", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1456, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u6b63\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "7o7ibneVpemoeiZffCxqKY", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1457, "prompt": "\u80bf\u5757\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0b\u80ba", "answer_id": "XzpdAm2LVED39yLWiZHNPX", "gt_response": "\u53f3\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1458, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "8XQMBsLxhXUZaBkrpcwCKA", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1459, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "3br5FSK7JiCQUdTthWUmE4", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1460, "prompt": "\u56fe\u7247\u4e2d\u5305\u62ec\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u6c14\u80bf", "answer_id": "hbUdLcruPAY9prwisUwGGY", "gt_response": "\u80ba\u80bf\u5757", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1461, "prompt": "\u80ba\u7684\u53f3\u4fa7\u662f\u4ec0\u4e48\u75c5?\n<image>", "pred_response": "\u80ba\u708e", "answer_id": "oVkKD7Q4mzQ5yCoRBvnYZX", "gt_response": "\u80ba\u80bf\u5757", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1462, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80bf\u5757\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "KGesn4b6Y5YXzicZaD7XVF", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1463, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "kqCAv7Qn9goXdmbdsKSsUj", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1464, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "WheSWtHHZfJd4zz7HRVLVQ", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1465, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "FbMfCJUaGNpoaADUZCqy8P", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1466, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u547c\u5438\u56f0\u96be", "answer_id": "nrqyKTR9HgszRpJUtwoTQ6", "gt_response": "\u547c\u5438\u56f0\u96be,\u54b3\u8840,\u80f8\u75db", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1467, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "55nqZSMXQB4WxTjonvdJG9", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1468, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "X-Ray", "answer_id": "6TSKgGA62MagBTtKq2YpEH", "gt_response": "X\u5149", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1469, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "3iP9uo4hJsULdVKALLVTfk", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1470, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u80f8\u8154?\n<image>", "pred_response": "\u662f", "answer_id": "8iRi6WAujzAjbXKEUVqK46", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1471, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u53f3\u4fa7", "answer_id": "JF5QQFN8URHqtngjdWscAD", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1472, "prompt": "\u56fe\u4e2d\u80bf\u5757\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0b\u80ba", "answer_id": "m5NhPw7gHEavFVyGuKCycF", "gt_response": "\u53f3\u4e0a\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1473, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "3XyYW9DK2NrRAazfnBrjvr", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1474, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "RYUdYxM3SER46avgqUNCFP", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1475, "prompt": "\u80ba\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "KjFW4TAHQFz2DuZAJr9PhY", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1476, "prompt": "\u56fe\u4e2d\u80ba\u7684\u53f3\u4fa7\u662f\u4ec0\u4e48\u75c5?\n<image>", "pred_response": "\u80ba\u6c14\u80bf", "answer_id": "L6BexUsR7J2LzLJSU5rwPj", "gt_response": "\u80ba\u80bf\u5757", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1477, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80bf\u5757\u5417?\n<image>", "pred_response": "\u53ef\u4ee5", "answer_id": "dS38LaU3GapokRyrPFEYRw", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1478, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "D5p8brEeoDzBuL4Qjiuoj2", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1479, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "Wds7AwBaEmNRWrai6gUhKL", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1480, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "7jfc8sekJfwJMh5PXTtSeA", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1481, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u80ba\u90e8\u53f3\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u836f\u7269\u6cbb\u7597", "answer_id": "5tpE5F2XZ8o9mTYBJceRQ9", "gt_response": "\u836f\u7269\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1482, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "VFcTXtoYFEfbD2TWu7b9c6", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1483, "prompt": "\u8fd9\u662fCT\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "5saJueB6XuxTWLtQP3W49q", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1484, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "3RkfQ5qxvSrgDqH7VTM587", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1485, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u80f8\u8154?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "FDrqZMxmGoUEui84WpLms3", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1486, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "STGBYqsZrCBgVJCnsvrB57", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1487, "prompt": "\u56fe\u4e2d\u80ba\u7ed3\u8282\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0b\u80ba", "answer_id": "ZVauU7LmSmyeMJAvwEfcbn", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1488, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u9888\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "BxGNmMuE33o8zkAJo89Efu", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1489, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "i96nc9mrbCNJtLrq56r7vM", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1490, "prompt": "\u56fe\u4e2d\u80ba\u7684\u53f3\u4fa7\u662f\u4ec0\u4e48\u75c5?\n<image>", "pred_response": "\u80ba\u6c14\u80bf", "answer_id": "o3ecbHNN5g3wjTQudpwKfJ", "gt_response": "\u80ba\u7ed3\u8282", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1491, "prompt": "\u80ba\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "dxZHKVc87UJLLxyKZ2Q5W7", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1492, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u7ed3\u8282\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "Y8aimyytYPuo8czeJHQjjD", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1493, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "jJuQ55fJMK9sCL9QdrCiHb", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1494, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "ADEkUmXk9tpBpQmmqLWNpv", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1495, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u662f\u5f88\u5927", "answer_id": "PbV4UDpZ2F3cmscDMbqwmR", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1496, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u80ba\u90e8\u53f3\u4e0b\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u836f\u7269\u6cbb\u7597", "answer_id": "JobXSTVwHfcBbKDSZv84J2", "gt_response": "\u836f\u7269\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1497, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "fJuXz9WRmSetzf2FkLAGoM", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1498, "prompt": "\u8fd9\u662f\u6838\u78c1\u5171\u632f\u6210\u50cf\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "k7RTGxp2YizmsiUmySkZHs", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1499, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "4sFthCUji2TdLrWvVueLst", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1500, "prompt": "\u4ece\u6574\u4e2a\u8eab\u4f53\u7684\u89d2\u5ea6\u770b,\u8fd9\u5e45\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "RdbkvqukviLfSbYpsDDTCA", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1501, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u53f3\u4e0b\u80ba", "answer_id": "hXhkEDgT7jdjJbrDRYkDTN", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1502, "prompt": "\u56fe\u4e2d\u80ba\u4e0d\u5f20\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0b\u80ba", "answer_id": "6FeXUWNesHNqgHh9VaNmv2", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1503, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "9JfDWzUTADUuKCehxU9zWx", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1504, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "ACLgptAA9J692p5zDsqJto", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1505, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u6709", "answer_id": "ECf7GRk6UHyVidfnA6Yk2C", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1506, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "LqitqzEPGpPyiqv5zZETSt", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1507, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u4e0d\u5f20\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "ZX9epRHCQXNFdch2CxyUam", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1508, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "oA9w9oY8K5yMj2PrbzHMog", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1509, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "4iSxiEX3B99xW9FTNu4PTn", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1510, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u662f\u5f88\u5927", "answer_id": "BM6y8AjQ9Jq2XWnKrijZRq", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1511, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u80ba\u90e8\u5de6\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u836f\u7269\u6cbb\u7597", "answer_id": "f68aUNm3a8hCwkSzmjmgpL", "gt_response": "\u7269\u7406\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1512, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "VDMrehFeKwsNi3fybtMWuR", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1513, "prompt": "\u8fd9\u662fCT\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "VBpkgbpZRtkqGPCBvC3ojJ", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1514, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "goaRnFYUBiQwWokYsqvzz7", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1515, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "iEJz8GAhm2uTL47Emeenf2", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1516, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "XWTTKW7T5K67YRheL79D2g", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1517, "prompt": "\u6d78\u6da6\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0b\u80ba", "answer_id": "NX5spBh3dLPc8PQsLJFB35", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1518, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "E8H3rDYc7Pz4PixQmnaff9", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1519, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "8iq7ENfBaCEgpVr87VeRAQ", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1520, "prompt": "\u75c5\u4eba\u7684\u5de6\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u6709", "answer_id": "KN7SQkWH9m5cDV2jCTgKL9", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1521, "prompt": "\u56fe\u7247\u4e2d\u5305\u62ec\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u6c14\u80bf", "answer_id": "KZGf3XDw9mUYUi5C7W6gRs", "gt_response": "\u80ba\u6d78\u6da6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1522, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u6d78\u6da6\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "QXwx3AET53CvamazmDYCKD", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1523, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "gzMEcbY5jRYAqNRiPyxitL", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1524, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "9AtgTx3MjJvxdd7pQA2XMj", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1525, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u662f\u5f88\u5927", "answer_id": "LrSZJTQfLtQU2PATdgdw4V", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1526, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u80ba\u90e8\u5de6\u4e0b\u89d2\u7684\u75be\u75c5?\n<image>", "pred_response": "\u836f\u7269\u6cbb\u7597", "answer_id": "8tsZdYuBBCpTBbXBPywBnS", "gt_response": "\u836f\u7269\u6cbb\u7597", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1527, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "WMkLDLT8iWsEE4dzzaN9AJ", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1528, "prompt": "\u8fd9\u662fCT\u626b\u63cf\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "TL2a9uMW4tZrLMeShPrR2a", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1529, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "6dCWFc4KyQEqUYwdg6XZFT", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1530, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "P3yfNR564ZiAtmjc6iRAeh", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1531, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u4e0d\u6b63\u5e38\u7684?\n<image>", "pred_response": "\u53f3\u4e0b\u80ba", "answer_id": "JjnQ6aA5Jc727hGkASJ5JX", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1532, "prompt": "\u56fe\u4e2d\u7ed3\u8282\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0b\u80ba", "answer_id": "9WKXc6go9xbhyE7saaTjma", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1533, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "WCiToGAuxpSmLZ88rmZtYn", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1534, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "AhDDrFaZSyPkTxAqBHyyaF", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1535, "prompt": "\u75c5\u4eba\u7684\u53f3\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u6709", "answer_id": "UVauRJ2Ygxo34mUoExMvMB", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1536, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "oSUQLjBes2YfniJLP7dKf9", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1537, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u7ed3\u8282\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "WJbo53DVo4ku9GSgLpPzqz", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1538, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "jHaJLnHnw54QBcraBZjPok", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1539, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "UNWuAFd9zneucLWdTNCXrQ", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1540, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u9ed1\u8272\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "n5qLo6hztZ9xK2w8MMjQrx", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1541, "prompt": "\u56fe\u4e2d\u80ba\u90e8\u53f3\u4e0b\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba\u90e8\u80a5\u80d6", "answer_id": "eewtySH7kQvv4kNm8iEowH", "gt_response": "\u80ba\u90e8\u611f\u67d3,\u6162\u6027\u523a\u6fc0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1542, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "Ggz7Lx6648GzbyxcowFTrY", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1543, "prompt": "\u8fd9\u662fX\u5149\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "E7dvrTSCaz8TL6Wm5QdCvk", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1544, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "TJNB4MUbhDEeDmjCtd5YYp", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1545, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "mJVboyEW7FW9rgzFE33D6J", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1546, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "oXaakobjEoTRgoX4nfMWib", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1547, "prompt": "\u56fe\u4e2d\u6d78\u6da6\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0b\u80ba", "answer_id": "EUvBVJQhiaDuqqWfPS2hUR", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1548, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "JBTmgWHwbnuvmgmLKV3ELv", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1549, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "C78f5kdXxNjBHfT29bce4j", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1550, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "ESTBZwFbLJQf5xQvWRSobs", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1551, "prompt": "\u75c5\u4eba\u6709\u4efb\u4f55\u7684\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u6709", "answer_id": "oTHFxPeJjcNfCeoxt4Th9p", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1552, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u6d78\u6da6\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u89c1", "answer_id": "b3ueH2JLbx2kjzqvTw6jij", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1553, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "jkt8n9bES8DcwgCcBdsHLz", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1554, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "6ajcALbvzUD993ySSZ9Yn2", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1555, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "APaLrAS3yNSjPn5p7hUtaY", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1556, "prompt": "\u5982\u4f55\u9884\u9632\u80ba\u90e8\u5de6\u4fa7\u75c5\u53d8\u7684\u53d1\u751f?\n<image>", "pred_response": "\u907f\u514d\u5438\u70df,\u4fdd\u6301\u5065\u5eb7\u7684\u751f\u6d3b\u65b9\u5f0f", "answer_id": "ZYujvvTKra2RFTfefVsMPV", "gt_response": "\u5065\u5eb7\u751f\u6d3b,\u4fdd\u6301\u8eab\u4f53\u5065\u5eb7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1557, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "Tyye8d5nfM97Qs645PYmHM", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1558, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7\u6838\u78c1\u5171\u632f\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "ZYP66jmiDn4hoGShTL52Bo", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1559, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "HFXTnEn9NuNyYHqFX28oQY", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1560, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "9YrjKs5vqw4gPWbkERUAzR", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1561, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "7yKfDMNGFWfFjDR4ESdbiM", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1562, "prompt": "\u56fe\u4e2d\u7ed3\u8282\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0b\u80ba", "answer_id": "NTiMqWqjeBxV2n6wrvuMuc", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1563, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "SBhhbivg5mshZmYzTqwDuX", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1564, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "GuWyKvWjz63mvZv3SpTuUG", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1565, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "QQS4xzGJpG4CNpv58ELR9v", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1566, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "4dAkPtoTkBzLBdE6chMaze", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1567, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u7ed3\u8282\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "CwGrHfnxiLgnRxGe2Gsxsy", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1568, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "cfAt3LE9MvpcogvotHrzUJ", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1569, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "JnBP3nPU5HqfwEYaxSk4EZ", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1570, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u9ed1\u8272\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "DnWJhowYyKstbPQxZhftYp", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1571, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u547c\u5438\u56f0\u96be,\u80f8\u75db,\u54b3\u55fd,\u547c\u5438\u56f0\u96be", "answer_id": "AGFmzAWKPGKN7RWVasmRXs", "gt_response": "\u80f8\u95f7,\u4e4f\u529b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1572, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "3nHxEPU9objXZucgoYmuXM", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1573, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "Y7yrXo7NpTSiDtWPHXiE4u", "gt_response": "X\u5149", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1574, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "8nnfdakh53NErAf735Rj5Q", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1575, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "hd8AMZyhmFRNGqHT3Kdcir", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1576, "prompt": "\u56fe\u4e2d\u5fc3\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4e0b\u80f8\u8154", "answer_id": "VTwoQncNcxTrpE9Dvmt886", "gt_response": "\u4e2d\u5fc3", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1577, "prompt": "\u5fc3\u810f\u80a5\u5927\u4f4d\u4e8e\u80f8\u8154\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "WG2n5DPit6mN3qwVnW3X7E", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1578, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "JexXtvodMURLVG33ZApjDh", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1579, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "i6z3T26Na3bdwwEJBi74Ux", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1580, "prompt": "\u80ba\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "m77BquMt6TcSWLE46LAF8d", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1581, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "PsXgWLmWWj9Zn3CJLjEWqF", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1582, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80bf\u5757\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "aX6zZZ7AnH7uZBCGyHYCTF", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1583, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "ZAiCiH8d2DFiyWPENNk659", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1584, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "ampxyK7vCyPWdsTqFEhVhX", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1585, "prompt": "\u56fe\u7247\u4e2d\u5fc3\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "WW5rh4nMamTTHbryX5mTvE", "gt_response": "\u63a8\u52a8\u8840\u6db2\u6d41\u52a8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1586, "prompt": "\u56fe\u4e2d\u5fc3\u810f\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba\u708e", "answer_id": "WVA9ww9NJqvpaZEfZ8SSSx", "gt_response": "\u9ad8\u8840\u538b,\u6269\u5f20\u6027\u5fc3\u808c\u75c5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1587, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "CVeJA89tVCkRdKSqzY2BvC", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1588, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "X-Ray", "answer_id": "mtqYx6ZrHWefe3ThxiJJxr", "gt_response": "X\u5149", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1589, "prompt": "\u8fd9\u662f\u6a2a\u65ad\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "AsygSxkAbvECWLBuQwcxEN", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1590, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "NXb67qnrHLDiC8PWetSaBc", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1591, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "j3Ys5ULBijtZfNDWveA3sb", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1592, "prompt": "\u6c14\u80f8\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4e0b\u80ba", "answer_id": "KerTKwVnuQEEdJa753YbuS", "gt_response": "\u5de6\u4e0a\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1593, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "FrrWzFNFLPmBnSajjtyzDB", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1594, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u9888\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "AUUrUh2ewys4mXbBYewENV", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1595, "prompt": "\u8fd9\u5f20\u56fe\u7247\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "XQ8bruynevA5KN3CQkT57Q", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1596, "prompt": "\u56fe\u7247\u4e2d\u5305\u62ec\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u6c14\u80bf", "answer_id": "e6ZkAc3EjsFC7fxSQEFLNu", "gt_response": "\u6c14\u80f8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1597, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u6c14\u80f8\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "2rijJS8pkA56DLi9bhNMk7", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1598, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "J5GF9uLwjG5NoCAmxXUCXP", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1599, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "UvbAKMjaogrq8mrBG7bVGC", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1600, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u9ed1\u8272\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "3eKyefQXbrdDPB6nNUNbz2", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1601, "prompt": "\u5982\u4f55\u9884\u9632\u80ba\u90e8\u5de6\u4e0a\u53f6\u75c5\u53d8\u7684\u53d1\u751f?\n<image>", "pred_response": "\u5065\u5eb7\u751f\u6d3b,\u907f\u514d\u5438\u70df,\u907f\u514d\u51a0\u72b6\u52a8\u8109\u75be\u75c5", "answer_id": "XEZrXYhv49oqzUm36yXu7c", "gt_response": "\u5fcc\u70df,\u907f\u514d\u5267\u70c8\u8fd0\u52a8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1602, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "MmFnFnVRxgSt6HkXS8EHHD", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1603, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "fWCCEL7vqE4UTXrFcNkvT5", "gt_response": "X\u5149", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1604, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "FWCZK5wfi97AhGoRe27hHc", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1605, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "VP5DkpeXS695QaPPa9kLd3", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1606, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "CPc53juBdFNdeZmnzC86Mk", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1607, "prompt": "\u56fe\u4e2d\u80ba\u708e\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u80ba", "answer_id": "SY7fKJCjEqfj6FousoSU5E", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1608, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "Wj9B6ymc2NoVdN56jYkUQe", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1609, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u8179\u90e8\u4e48?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "3d8U3FLQWiLvhHTHZ4spgb", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1610, "prompt": "\u75c5\u4eba\u7684\u5de6\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u6709", "answer_id": "3Z7RJxk72YDvNAi6ipeGrS", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1611, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Bcg6cUbhnSZUuNAwzGzEKM", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1612, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u708e\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "2xFuEqgv5j54KHJPA5WrE8", "gt_response": "\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1613, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "g7HTvNqLwFrVASETYjM5uK", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1614, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u5668\u5b98?\n<image>", "pred_response": "2", "answer_id": "TTRf75qmWTspLXKu8VgDg2", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1615, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u5fc3\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "KeqNULCy4wBzbxs55inooD", "gt_response": "\u63a8\u52a8\u8840\u6db2\u6d41\u52a8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1616, "prompt": "\u5982\u4f55\u9884\u9632\u80ba\u90e8\u5de6\u4e0b\u53f6\u75c5\u53d8\u7684\u53d1\u751f?\n<image>", "pred_response": "\u907f\u514d\u5438\u70df,\u4fdd\u6301\u5065\u5eb7\u7684\u751f\u6d3b\u65b9\u5f0f", "answer_id": "oSLNQdqJfzc7CVjS7mFrE8", "gt_response": "\u9632\u5bd2\u4fdd\u6696,\u589e\u5f3a\u4f53\u8d28", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1617, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "7gmfewUfR8Rf7AbgwDiVSr", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1618, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "MygcYBeCHhCNXLaDBtkjfY", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1619, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "eepjxrkTqMRPHyQmJaEvKs", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1620, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u68af\u5ea6", "answer_id": "ANZ8Y5yBEEMkUY6VRZ7Rzh", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1621, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "Cazhv2wwdbj9BXsDifmwMf", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1622, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "TE4XsPJYmfUf57UCEJE36B", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1623, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "BNJzqpkAhggXL9JJz9kX9U", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1624, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Cf2D2ooZmohobEoKhToEyp", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1625, "prompt": "\u813e\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "X7doU6QP8Lqa6c4BVWUGqu", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1626, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7X\u5149\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "f6BJU38EikUQfky3PEPNcm", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1627, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "gbpgLrNGuP36odf6M8QL2p", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1628, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "5tGTu2q3MUEgwSVScgY7zK", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1629, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u80ba", "answer_id": "mXBF8YyvzznR4rqEoNLFhx", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1630, "prompt": "\u56fe\u4e2d\u80ba\u708e\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0b\u80ba", "answer_id": "6s2jPWuBG67YZd2BoiVDVC", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1631, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "bsCDEYhw84wRippuLupzEw", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1632, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "H64x6xn5qUcfKbfJBthHfD", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1633, "prompt": "\u75c5\u4eba\u7684\u5de6\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u6709", "answer_id": "hVDbwUQXQL8UVT9RGLvbhh", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1634, "prompt": "\u56fe\u7247\u4e2d\u5305\u62ec\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u7ed3\u8282", "answer_id": "i8ruZPoqAaDLtNQXCEAoSn", "gt_response": "\u80ba\u708e", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1635, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u708e\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "aUBJTDxdBZYooavK3rBEdw", "gt_response": "\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1636, "prompt": "\u56fe\u4e2d\u7684\u80ba\u662f\u4ec0\u4e48\u989c\u8272\u7684?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "GF6hZDajwu66xap7kZTAou", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1637, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "hS7AHGZmKm73daNzkN6UoM", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1638, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u9ed1\u8272\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "QEiRqBoM8FMyokx5yfJcWQ", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1639, "prompt": "\u56fe\u4e2d\u53f3\u4e0b\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u547c\u5438\u56f0\u96be", "answer_id": "VzLCHgwzRMbh9hzA77mdoY", "gt_response": "\u54b3\u6ce1\u6cab\u9ecf\u6db2\u75f0,\u547c\u5438\u56f0\u96be,\u54b3\u55fd,\u80f8\u75db", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1640, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "kmGmd4kVpUKmqujWB8fEQM", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1641, "prompt": "\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u88ab\u8fd9\u5f20\u56fe\u7247\u4f7f\u7528?\n<image>", "pred_response": "X-Ray", "answer_id": "BkpTHm9ZazKfZ6bbP7sPwi", "gt_response": "X\u5149", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1642, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "7wj4Lwho52YZoJigQzARdb", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1643, "prompt": "\u56fe\u50cf\u5728\u8eab\u4f53\u91cc\u4ee3\u8868\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "QPBBBSjkYDLebB6ytT2cWh", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1644, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u53f3\u4fa7", "answer_id": "Qvjcob6oK7diVkmNx5APiw", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1645, "prompt": "\u56fe\u4e2d\u80ba\u708e\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0b\u80ba", "answer_id": "RFissKCG4h34zcjbfDom6R", "gt_response": "\u53f3\u4e0a\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1646, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "Q3ova3sXRU9PP8EWFkdYyR", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1647, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "44cDLLkajDrLUBHhWzyjWG", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1648, "prompt": "\u75c5\u4eba\u7684\u53f3\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u6709", "answer_id": "4Yo6bzoWU2AwRBadPDuoAV", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1649, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "Ez3hwRKNpyCHFhDbtG9Gip", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1650, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u708e\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "i6wThPUWuaDWCvVfaQmynp", "gt_response": "\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1651, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "7ZbRGVYqtE6SWk2BkpNnuW", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1652, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u5fc3\u810f?\n<image>", "pred_response": "2", "answer_id": "iYymZyqR9QvXAj4aKDs8o6", "gt_response": "1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1653, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "8vt2abLxhCgMFSDeDXaG8P", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1654, "prompt": "\u56fe\u4e2d\u53f3\u4e0a\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u547c\u5438\u56f0\u96be,\u80f8\u75db,\u54b3\u55fd,\u547c\u5438\u56f0\u96be,\u80f8\u75db,\u54b3\u55fd", "answer_id": "6B3NTqTEGCMizfEBZenMUD", "gt_response": "\u54b3\u6ce1\u6cab\u9ecf\u6db2\u75f0,\u547c\u5438\u56f0\u96be,\u54b3\u55fd,\u80f8\u75db", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1655, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "RsRhGLjzSrh5Q9fAWMeeyK", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1656, "prompt": "\u8fd9\u662f\u6838\u78c1\u5171\u632f\u626b\u63cf\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "arZFzjAq2u54bkNG3pWpnE", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1657, "prompt": "\u8fd9\u662f\u6a2a\u65ad\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "cyrkmEKU9GGo32G2BswXti", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1658, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "eVq3zZhhumh3xGFMovTppD", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1659, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "Roc8UDB7WEvjafCarjqXWw", "gt_response": "\u53cc\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1660, "prompt": "\u56fe\u4e2d\u7ed3\u8282\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0b\u80ba", "answer_id": "VHhVZnkc5Y7nfcq8Kr6gwm", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1661, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "YJgichvKkjDnd3DA827hAE", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1662, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "ZEwLvPwLcgNez9Pgsgrr45", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1663, "prompt": "\u80ba\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "YcsrBAhBWwpDiDd6xcCAbM", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1664, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "KVVbRjuydt2xAQMvYMCMDa", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1665, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u7ed3\u8282\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "X2ui2TayZTESjG8HSNqFad", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1666, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "RoSWtina2nCdSYeMy8zJ7t", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1667, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u5668\u5b98?\n<image>", "pred_response": "2", "answer_id": "cJskaAR4qnRio3yp6FN22d", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1668, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "h2rAVeqkqbCJCLNLFHtXbX", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1669, "prompt": "\u56fe\u4e2d\u80ba\u90e8\u5de6\u4e0b\u89d2\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba\u6c14\u80bf", "answer_id": "KJghqs2JYirogxQ9tz2e5E", "gt_response": "\u80ba\u90e8\u611f\u67d3,\u6162\u6027\u523a\u6fc0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1670, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "gHngAQUt8uF7RFD2GzuqNr", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1671, "prompt": "\u8fd9\u662f\u6838\u78c1\u5171\u632f\u626b\u63cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "WbcUn4RVvPFQZY5oupw3Mo", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1672, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "beNogeAunwmjd9o8jEoGoi", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1673, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "TYchgzCSrFGrk5h7cz8Uab", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1674, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0b\u80ba", "answer_id": "ZtVWwp2EteuNzbLFtx5rjZ", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1675, "prompt": "\u56fe\u4e2d\u80ba\u4e0d\u5f20\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4e0b\u80ba", "answer_id": "hZHpcYKSpBHX8RKKV4RnU6", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1676, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u8179\u90e8\u4e48?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Wn5mXBKFZTydMz6M4VQs3J", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1677, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "PPpXm5VUjXJnvtdBhxAFLK", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1678, "prompt": "\u80ba\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "ePCfaa8StGvWMFH4shfHXz", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1679, "prompt": "\u75c5\u4eba\u7684\u53f3\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u6709", "answer_id": "cYY65jFpdcCAH9SqnHDequ", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1680, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u4e0d\u5f20\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "XwrD3c6mTUydtU3vNkKhon", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1681, "prompt": "\u56fe\u4e2d\u7684\u80ba\u662f\u4ec0\u4e48\u989c\u8272\u7684?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "imcT5VRFgVtS7F74e82ax2", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1682, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u5fc3\u810f?\n<image>", "pred_response": "1", "answer_id": "nnAyKeUb9chDCykZqrxh7E", "gt_response": "1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1683, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "X3PCiCfrRVvXHYRYVYS4pj", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1684, "prompt": "\u5982\u4f55\u9884\u9632\u80ba\u90e8\u53f3\u4e0b\u53f6\u75c5\u53d8\u7684\u53d1\u751f?\n<image>", "pred_response": "\u907f\u514d\u5438\u70df,\u4fdd\u6301\u5065\u5eb7\u7684\u751f\u6d3b\u65b9\u5f0f", "answer_id": "hjaHDzfX94wrDCCrK6Qagq", "gt_response": "\u5fcc\u70df,\u589e\u5f3a\u4f53\u8d28", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1685, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "KoaopoN3Gs7wybxKndRnKD", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1686, "prompt": "\u8fd9\u662fX\u5149\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "9qpumUFbRhjxej55q5veaH", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1687, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "8WDbHrFMVHdHmVj8S65vnj", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1688, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "RY5WZAgGKBSN2B7yP7MdHu", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1689, "prompt": "\u56fe\u4e2d\u80f8\u8154\u54ea\u4e00\u90e8\u5206\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u53f3\u4e0b\u80ba", "answer_id": "nZDCjaMbrbGUUAvjBfWAhK", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1690, "prompt": "\u5fc3\u810f\u80a5\u5927\u4f4d\u4e8e\u80f8\u8154\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4e0b\u80ba", "answer_id": "ejaQNweWoGrw3Xz43LCgJa", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1691, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u9888\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "ZkXcDjhTAEmMwmCaNeSMNH", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1692, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "aWg8z96UMqSmnQGfKb6aqj", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1693, "prompt": "\u56fe\u4e2d\u80f8\u8154\u5b58\u5728\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u6c14\u80bf", "answer_id": "3iwgvVSWSRg9XNWxKRA2tV", "gt_response": "\u5fc3\u810f\u80a5\u5927", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1694, "prompt": "\u56fe\u4e2d\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "GybmW6R5PS6qusC4M8CBWX", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1695, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53ef\u4ee5\u89c2\u5bdf\u5230\u5fc3\u810f\u80a5\u5927\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "YZXzpyqQRxakfK2mLMRUxu", "gt_response": "\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1696, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "BbCnzR8exdAvauKzGMHCft", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1697, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u5fc3\u810f?\n<image>", "pred_response": "2", "answer_id": "NaguHA3jfJHMXqw5QUitrV", "gt_response": "1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1698, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u7684\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "MYzkr7SQ2ggzGwqWMkGdCS", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1699, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u5fc3\u810f\u7684\u75be\u75c5?\n<image>", "pred_response": "\u836f\u7269\u6cbb\u7597", "answer_id": "gKQRSSTrG5JXdPYV4E3Bti", "gt_response": "\u7269\u7406\u6cbb\u7597,\u836f\u7269\u6cbb\u7597", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1700, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "32maDtAbkRteqEsuk6pGPL", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1701, "prompt": "\u8fd9\u5f20\u7167\u7247\u662f\u600e\u4e48\u62cd\u7684?\n<image>", "pred_response": "X-Ray", "answer_id": "C5vXR5RB8neNzsoZW9HX64", "gt_response": "X\u5149", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1702, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "eameJuqKGew3ar7qJDTpUc", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1703, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u80f8\u8154?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "EVvC6waPTttVxdmM4YCYpW", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1704, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u53f3\u4fa7", "answer_id": "EyX9x69HMtgkVvcgusS8tv", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1705, "prompt": "\u56fe\u4e2d\u79ef\u6db2\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4e0b\u80ba", "answer_id": "HosxjemhoszLKuWWh57TFf", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1706, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "X5b2Yyr465eD2UYDBWQmxw", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1707, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "jAc3BSP5DzAnQruy7bsvjC", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1708, "prompt": "\u56fe\u7247\u4e2d\u5305\u62ec\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u708e", "answer_id": "77D6hqUaKGYTGYTaJFm2gz", "gt_response": "\u80f8\u8154\u79ef\u6db2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1709, "prompt": "\u56fe\u4e2d\u80ba\u7684\u53f3\u4e0b\u4fa7\u662f\u4ec0\u4e48\u75c5?\n<image>", "pred_response": "\u80ba\u6c14\u80bf", "answer_id": "DYfn3upUDYzS6Jmgdi2MDH", "gt_response": "\u80f8\u8154\u79ef\u6db2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1710, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u79ef\u6db2\u5417?\n<image>", "pred_response": "\u53ef\u4ee5", "answer_id": "XQpuQmcYYjvN7KEhbPppyD", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1711, "prompt": "\u56fe\u4e2d\u7684\u80ba\u662f\u4ec0\u4e48\u989c\u8272\u7684?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "jmG45Ayc8fcctCVKkB5Med", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1712, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u5668\u5b98?\n<image>", "pred_response": "2", "answer_id": "H55AVnsJh6zvNffQBYWuG7", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1713, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u9ed1\u8272\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "A4AdCBderNQ3j6rDEWn57W", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1714, "prompt": "\u56fe\u4e2d\u53f3\u4e0b\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u547c\u5438\u56f0\u96be", "answer_id": "LmDfLTeUXwi4Sya3uLimNP", "gt_response": "\u80f8\u95f7,\u547c\u5438\u56f0\u96be,\u80f8\u75db", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1715, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "25UVdxsTgqNfH8X2xEVRXh", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1716, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "SofaBDHU63Ea7YWhZz7wZu", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1717, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "cU4KcTYWoRsRoW686D8QcH", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1718, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "6QasGkkyZ9YoGphJ3QjhHT", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1719, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "JgDCP2CRkEkKAZFKZ2kAVB", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1720, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "gGQ5fp4ehahzNyrnMvYktL", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1721, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "XuDfAJpE7QNsCQ7dhxGjcK", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1722, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "aZCywH6u5sksHJemvE5dX8", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1723, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "TyN5jSSyzEAcNYfr8eHKsD", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1724, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "2D", "answer_id": "awUpaLSK3RUCyq53omZcuv", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1725, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "mfJVnYxtv6TgFXKshkzDZi", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1726, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "ia6LXmBMiMSk2WaR7xcMzi", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1727, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "cgjUmAJWgFbZxfEM42iEk9", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1728, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "HBoTxgT8VwBe8DHxhZpgbr", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1729, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "4kEVAbSJCToNpbkCj9M3C9", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1730, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "DVjxGUEg5FGSnzkiHfQPvb", "gt_response": "\u5065\u5eb7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1731, "prompt": "\u80be\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "3RYdj8cLek46dmFnCEESkv", "gt_response": "\u5065\u5eb7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1732, "prompt": "\u8fd9\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u533b\u5b66\u56fe\u50cf?\n<image>", "pred_response": "\u6838\u78c1\u5171\u632f", "answer_id": "Lvb3Z7PwFaatKRj39iFuwm", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1733, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "Nx5xLScjW2H9LfmLLQpCGV", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1734, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u9888\u90e8?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "EJB4cEkXEQ4Xmt3bun4KwK", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1735, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u662f\u5589?\n<image>", "pred_response": "\u5de6", "answer_id": "nqSP49dTtM7dEkzh9ATDkY", "gt_response": "\u9ed1\u8272\u7a7a\u6d1e", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1736, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "Qznw5mZksBfFgvQbYsSVCG", "gt_response": "\u9888\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1737, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u9888\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "TgznpHddtEd3Xrb7Ci7GSm", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1738, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u98df\u9053\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "3oAtTep8btDaVwPjAN6tnp", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1739, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "DKCBJ5qfvAyyJ6V9qWscrf", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1740, "prompt": "\u56fe\u4e2d\u5589\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "ivisqAuoaZ2N5bqXsFspz6", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1741, "prompt": "\u56fe\u4e2d\u5589\u662f\u4ec0\u4e48\u5f62\u72b6\u7684?\n<image>", "pred_response": "\u5706\u5f62", "answer_id": "B6VrVMoqx46E6uuRunr2pi", "gt_response": "\u692d\u5706", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1742, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u4e0a\u65b9\u7684\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "cW8fBn3GQ87tW4WdJ48TDL", "gt_response": "\u901a\u6c14,\u53d1\u97f3", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1743, "prompt": "\u4f4d\u4e8e\u56fe\u50cf\u9876\u90e8\u7684\u5668\u5b98\u5c5e\u4e8e\u4ec0\u4e48\u7cfb\u7edf?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "Sxt3nanJi88J9Bx2xsrtBb", "gt_response": "\u547c\u5438\u7cfb\u7edf,\u6d88\u5316\u7cfb\u7edf", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1744, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u795e\u7ecf\u7cfb\u7edf?\n<image>", "pred_response": "\u8111", "answer_id": "3D8MfYtqQcoV2QKkjZ7ZpL", "gt_response": "\u810a\u9ad3", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1745, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "TahzizEuAKDBw8esrJXgXF", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1746, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u5206\u6ccc\u9176\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "er29QTYKQs8Bqy684irnVP", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1747, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "JehQLFMWSeK7EyhQ3fCi2h", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1748, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "AA3wDQFRKc3wba7rUFMkgW", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1749, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "BjNAswdQNU8TN8nYap4tLr", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1750, "prompt": "\u4ec0\u4e48\u5668\u5b98\u6807\u5fd7\u7740\u5668\u5b98\u7cfb\u7edf\u7684\u4e0b\u7aef?\n<image>", "pred_response": "\u8111\u888b", "answer_id": "LqriNMjee22bR2M54rUN8a", "gt_response": "\u4e0b\u988c\u9aa8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1751, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "6uKzpQjbF5HXFmWgfLwS5M", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1752, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u5c0f\u80a0\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "J9pSVF3SF7rdEk3nGg83k3", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1753, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u662f\u5426\u5305\u542b\u8180\u80f1?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "RM7YGRiqzge9wcrgimFHwE", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1754, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8111", "answer_id": "dhrboYBMgtZcgnaWUB9n8p", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1755, "prompt": "\u56fe\u4e2d\u6709\u6ca1\u6709\u4e00\u4e9b\u5668\u5b98\u63a7\u5236\u5fc3\u8df3\u548c\u547c\u5438?\n<image>", "pred_response": "\u6709\u7684", "answer_id": "oW86uSK6mPHuvFwckmnQ3m", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1756, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7CT\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "QAem2tyxNMFA6D4W7yH3pN", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1757, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "4CdmrFn7UXTLkfAbSAJuNk", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1758, "prompt": "\u4ece\u6574\u4e2a\u8eab\u4f53\u7684\u89d2\u5ea6\u770b,\u8fd9\u5e45\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u9888\u90e8", "answer_id": "DfYv2PdhAhsWsfbJUZqVCf", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1759, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u9885\u8154", "answer_id": "frc3ZY2d2H7Q4Ct2rE5WLS", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1760, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u989e\u8154", "answer_id": "Td9NbjkYrkMqoaEqmmkkxq", "gt_response": "\u816e\u817a", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1761, "prompt": "\u8fd9\u5e45\u56fe\u50cf\u91cc\u6709\u816e\u817a\u5417?\n<image>", "pred_response": "\u6709", "answer_id": "i393ayagu5b7FZ9g7ZkxDq", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1762, "prompt": "\u4e0b\u988c\u9aa8\u51fa\u73b0\u5728\u8fd9\u5f20\u56fe\u4e0a\u4e86\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Ff6Bc5N5rbErPnizYGnziV", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1763, "prompt": "\u56fe\u7247\u4e2d\u7684\u4e0b\u988c\u9aa8\u662f\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "PpGvXCnrf9fAWNBPwbedv4", "gt_response": "\u767d\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1764, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8111\u888b", "answer_id": "oVvvsGWdqmtr5aGciunvH3", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1765, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u795e\u7ecf\u7cfb\u7edf?\n<image>", "pred_response": "\u8111\u888b", "answer_id": "gyDWyQmVriakSLRcTa9Fjc", "gt_response": "\u810a\u9ad3", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1766, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u53ef\u4ee5\u6392\u6cc4\u673a\u4f53\u5e9f\u7269\u8d28\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "ejaeLKY9uvSLbE4QbsyRVM", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1767, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "5HRFtMyRNzyFVagwWAQWqR", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1768, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "MRI", "answer_id": "QXtQnmJUHRiX4MNhzTJejo", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1769, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u51a0\u72b6\u9762", "answer_id": "TAqCDeW2HvQJxJD76bXAtM", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1770, "prompt": "\u56fe\u50cf\u5728\u8eab\u4f53\u91cc\u4ee3\u8868\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "6Rqj3NEBMQNuSrvctr7uMr", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1771, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "XUDJszyVvUEVwYeiCZTSmb", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1772, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "T3mqLm7WSk5ZbYRpWpo9Zw", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1773, "prompt": "\u8fd9\u5e45\u56fe\u50cf\u91cc\u6709\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "DJgeAEffr9mXGPBTYmwToV", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1774, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u76f4\u80a0\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "XZRUFqMWS8LherDFjegodF", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1775, "prompt": "\u810a\u9ad3\u5728\u56fe\u7247\u4e2d\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "S7TFsrGMAxjbKXYedJiivC", "gt_response": "\u7070\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1776, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8111", "answer_id": "ZiWXhRFRuPy4jXaemeBe28", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1777, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u795e\u7ecf\u7cfb\u7edf?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "7K6HANUNv6RxPAFSUPBM3f", "gt_response": "\u810a\u9ad3", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1778, "prompt": "\u8fd9\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u533b\u5b66\u56fe\u50cf?\n<image>", "pred_response": "MRI", "answer_id": "6ZgAQtoYKDbBCRJqSPdnWK", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1779, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "itXmPug7ucQXPfo4Z48Esm", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1780, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u9888\u90e8?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "UnYfdtunGKhgwBGRKPZPcx", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1781, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u662f\u6c14\u7ba1?\n<image>", "pred_response": "\u5de6\u4e0b", "answer_id": "C4XSkx6WzdKKpe7uH8FTCK", "gt_response": "\u9ed1\u8272\u7a7a\u6d1e", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1782, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8111\u888b, \u8111\u5e72, \u810a\u9ad3, \u810a\u9ad3, \u810a\u9ad3, \u810a\u9ad3, \u810a\u9ad3, \u810a\u9ad3, \u810a\u9ad3, \u810a\u9ad3, \u810a\u9ad3, \u810a\u9ad3, \u810a\u9ad3, \u810a\u9ad3, \u810a\u9ad3, \u810a\u9ad3,", "answer_id": "GsvB6iiE2UbogG2vTv3uCP", "gt_response": "\u9888\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1783, "prompt": "\u816e\u817a\u51fa\u73b0\u5728\u56fe\u50cf\u4e2d\u4e86\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "5R2tERyyTVXBsNK9QEKCqa", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1784, "prompt": "\u8fd9\u5e45\u56fe\u50cf\u91cc\u6709\u6c14\u7ba1\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "RmPCAscqHbWTYzBct64Vst", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1785, "prompt": "\u56fe\u4e2d\u98df\u9053\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "b3tnEe6zxHseNpLJj4ATnz", "gt_response": "\u7070\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1786, "prompt": "\u8fd9\u4e2a\u75c5\u4eba\u7684\u6c14\u7ba1\u662f\u4ec0\u4e48\u5f62\u72b6\u7684?\n<image>", "pred_response": "\u5f27\u5f62", "answer_id": "a5udxDHqsjhDUobehjULLs", "gt_response": "\u4e0d\u89c4\u5219", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1787, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u9876\u90e8\u7684\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "cAzag26GWRv7yt4BYkH7sS", "gt_response": "\u8fd0\u9001\u6c14\u4f53", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1788, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u795e\u7ecf\u7cfb\u7edf?\n<image>", "pred_response": "\u8111\u888b", "answer_id": "SYTWvx8eFvDrMxTxoWkJaL", "gt_response": "\u810a\u9ad3", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1789, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Ss3p7MZsXrnyQqjAGR4yDf", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1790, "prompt": "\u56fe\u4e2d\u662f\u5426\u6709\u53ef\u4ee5\u611f\u77e5\u4e8b\u7269\u7684\u5668\u5b98?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "REQuAuZUtLGPNCYd3xY9d5", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1791, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7X\u5149\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Nn2CLAPLUEZvgX2NReSHmg", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1792, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "g5C5ZchUvuevabjxNMUwJK", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1793, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u810a\u690e", "answer_id": "NW4gadxwjFBcStNZDqujtX", "gt_response": "\u9888\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1794, "prompt": "\u56fe\u4e2d\u98df\u7ba1\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "dwjQF9nxBoHquKvXNMuDeM", "gt_response": "\u6c14\u7ba1\u4e0b\u65b9", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1795, "prompt": "\u810a\u9ad3\u5728\u8fd9\u5f20\u56fe\u4e0a\u51fa\u73b0\u4e86\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "X23wuRVrDjpCunjjh9b7A2", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1796, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u810a\u9ad3\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "K4sfyT4ejgEp8of8dKcGVP", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1797, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u80ba\u5417?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "nUgzYz83EgLSNZ94bUcmh2", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1798, "prompt": "\u8fd9\u4e2a\u75c5\u4eba\u7684\u98df\u9053\u662f\u4ec0\u4e48\u5f62\u72b6\u7684?\n<image>", "pred_response": "\u5706\u5f62", "answer_id": "h596Xac2siozLsQmoEb3Dj", "gt_response": "\u4e0d\u89c4\u5219", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1799, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u7684\u9ed1\u8272\u7a7a\u6d1e\u7ec4\u7ec7\u6709\u4ec0\u4e48\u529f\u80fd?\n<image>", "pred_response": "\u5438\u6536", "answer_id": "bNDQr7VopMLW5sbqaJGy2G", "gt_response": "\u98df\u7269\u8fd0\u9001", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1800, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "8m8ftXYHmbZpKH9mKfYQGP", "gt_response": "\u6c14\u7ba1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1801, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "BTVVnDZxHDNXDDs3wmQNfj", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1802, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u53ef\u4ee5\u8fd0\u9001\u6c14\u4f53\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Zs9WHGfofYbyGqVGLeQ7Zr", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1803, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "fsLphy4VReP77BPzY3sK5x", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1804, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "dc8kgzsZZHiKHhgKK5tcNg", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1805, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "BwSGLDaEpzoECJZHB7KLh5", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1806, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "LWzoCkXBkoosRaTK9BkMHH", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1807, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "JVV8GzBxLgW9S6xDDJedPN", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1808, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u8111\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "UNiRb8JVu9zyZshVCnzHDm", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1809, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf\u7624", "answer_id": "RTDWkSueczTnbNALREPcbK", "gt_response": "\u8111\u6c34\u80bf", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1810, "prompt": "\u5927\u8111\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "o3X9SQqzywWxC28sJbF4x5", "gt_response": "\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1811, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "jJFZutgQDDtfg4XBogFjJh", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1812, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "MRI", "answer_id": "fz8CGQxrAYtx83fXCS8Qpi", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1813, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "5pZSvhYf9uJtccm7iczXbV", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1814, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "RyDycUaDrsBQ32dz7EYS2T", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1815, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u9888\u90e8?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "hVZKRCEVHkstdcAJcdskNh", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1816, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "QPk8mYBHkFQDRmx7koZe6D", "gt_response": "\u9876\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1817, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "QPvrGmNEJqVF34XMbZs4sz", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1818, "prompt": "\u56fe\u7247\u4e2d\u53ef\u4ee5\u89c2\u5bdf\u5230\u8111\u6c34\u80bf\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "RgREdfYJKgXNx7goXKVeRt", "gt_response": "\u53ef\u4ee5", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1819, "prompt": "\u5982\u4f55\u6cbb\u7597\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u9876\u90e8\u7684\u75be\u75c5?\n<image>", "pred_response": "\u624b\u672f", "answer_id": "QYimYn37grjEtP3WYt7Yip", "gt_response": "\u836f\u7269\u6cbb\u7597,\u652f\u6301\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1820, "prompt": "\u8fd9\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u533b\u5b66\u56fe\u50cf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "kK5uCz84cUWrL96xKAuroN", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1821, "prompt": "\u8fd9\u5f20\u6838\u78c1\u5171\u632f\u56fe\u50cf\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "SnaXVbLWVCbFhKm7pSPYZQ", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1822, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u51a0\u72b6\u9762", "answer_id": "WxNvM8sY3tjfEbMdfEGM3T", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1823, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "NxB2CpPfvCJ4obghvezCyL", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1824, "prompt": "\u56fe\u4e2d\u8111\u6c34\u80bf\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "MB5jc9pNKrKUPvnyEDbtSM", "gt_response": "\u5de6\u4e0a\u53f6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1825, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "7gT9egqXD3DYdFMgVXCLPB", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1826, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf\u5757", "answer_id": "iukjZVDpb3dq8sn97zDxdd", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1827, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "2", "answer_id": "3862div4ibATR62WZvPLxg", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1828, "prompt": "\u5982\u4f55\u9884\u9632\u56fe\u7247\u4e2d\u5927\u8111\u5de6\u4e0a\u53f6\u75be\u75c5\u7684\u53d1\u751f?\n<image>", "pred_response": "\u5065\u5eb7\u751f\u6d3b\u65b9\u5f0f", "answer_id": "krsr6zSf8JWhBGhP2mPyGy", "gt_response": "\u6ce8\u610f\u996e\u98df\u536b\u751f,\u52a0\u5f3a\u8eab\u4f53\u7d20\u8d28,\u907f\u514d\u8111\u90e8\u5916\u4f24", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1829, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "MRI", "answer_id": "dkZ3oEPRNT8r2V7sqiKwQH", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1830, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528\u4ec0\u4e48\u6837\u7684\u6743\u91cd\u751f\u6210\u7684,t1\u8fd8\u662ft2?\n<image>", "pred_response": "T2", "answer_id": "VH5zvzaAy3nEnz6chWKNKL", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1831, "prompt": "\u8fd9\u662f\u6a2a\u65ad\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "KtfgtSw8cTMxTs2YbZ5KWR", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1832, "prompt": "\u56fe\u50cf\u5728\u8eab\u4f53\u91cc\u4ee3\u8868\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "TeoZydKiEMFY3DEmerZeX4", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1833, "prompt": "\u56fe\u4e2d\u8111\u90e8\u589e\u5f3a\u80bf\u7624\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "F2pYxkkbDh9xE8rzxiUzp9", "gt_response": "\u5de6\u53f6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1834, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "PvV4em6p63mBzE6CiMxyLZ", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1835, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf\u7624", "answer_id": "jSHHDWpbxT9RmCARYgb6B6", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1836, "prompt": "\u76f8\u5bf9\u4e8e\u5176\u4ed6\u7ec4\u7ec7,\u8111\u80bf\u7624\u662f\u767d\u8272\u7684\u8fd8\u662f\u7070\u8272\u7684?\n<image>", "pred_response": "\u7070\u8272\u7684", "answer_id": "PyfQD9uWpoKQgqEBw77ZYf", "gt_response": "\u767d\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1837, "prompt": "\u5982\u4f55\u6cbb\u7597\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u5de6\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u624b\u672f", "answer_id": "hANpVjkZfxes8Mc7fmManK", "gt_response": "\u836f\u7269\u6cbb\u7597,\u652f\u6301\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1838, "prompt": "\u8fd9\u662f\u6838\u78c1\u5171\u632f\u626b\u63cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "UFgogjJfxngPKC6VgG5x3n", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1839, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "FUSqM2tqzRu5waZZyZd55H", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1840, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "aHBTvYC6WsCUYSCy4gu7iC", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1841, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "74NdvXByAGsDEuLU8ArnPX", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1842, "prompt": "\u56fe\u4e2d\u8111\u90e8\u589e\u5f3a\u80bf\u7624\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "T6hjcy3FXuP25ptbLCC7C5", "gt_response": "\u5de6\u53f6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1843, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "8XpgVNDahWnyCSfxi2CnBV", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1844, "prompt": "\u5927\u8111\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "XPnDvB2wDpZJxYYehWqTtD", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1845, "prompt": "\u5982\u4f55\u6cbb\u7597\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u5de6\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u624b\u672f", "answer_id": "LqaocBCZTFzEDL9H6dY6ff", "gt_response": "\u836f\u7269\u6cbb\u7597,\u652f\u6301\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1846, "prompt": "\u8fd9\u662fCT\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Zoxiz5gQm33VbrY9VCMyqS", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1847, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "ikZbtpPFNQNAgNQJQaB3zi", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1848, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "6r37s9MXbrx768PcBk8L8j", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1849, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "erRLuY9tRcsAwZvcc7y8cS", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1850, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "fXcQxGJ4ET4stCqUxcQi89", "gt_response": "\u5de6\u4e0a\u53f6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1851, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "ZtoFkQAo2f7m9JnWRaRr2X", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1852, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf\u7624", "answer_id": "PgvrQ6yqWC3VPD5UyZYN9e", "gt_response": "\u8111\u6c34\u80bf", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1853, "prompt": "\u4ea7\u751f\u56fe\u4e2d\u5927\u8111\u5de6\u4e0a\u4fa7\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8111\u80bf\u7624", "answer_id": "jkvyskPmBtJWNgYJeiYbsq", "gt_response": "\u9885\u8111\u635f\u4f24,\u9885\u5185\u5360\u4f4d\u6027\u75c5\u53d8,\u9885\u5185\u708e\u75c7,\u8111\u8840\u7ba1\u75c5\u53d8,\u8111\u7f3a\u6c27", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1854, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "RzP5P3hkBCuip54dPgXvTA", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1855, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528\u4ec0\u4e48\u6837\u7684\u6743\u91cd\u751f\u6210\u7684,t1\u8fd8\u662ft2?\n<image>", "pred_response": "T2", "answer_id": "RNLuyoxQFHqEpUCxXBYhF6", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1856, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u51a0\u72b6\u9762", "answer_id": "nUiNyW8n3dbhd5JNJGRMFh", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1857, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "YnJGyHVxXrPKFWceeyWVyR", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1858, "prompt": "\u56fe\u4e2d\u8111\u54ea\u4e2a\u53f6\u662f\u6b63\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "nvgrQWE2GQUtdx8TmSupTj", "gt_response": "\u5de6\u53f6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1859, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "H9pGuw6YLz8Dz4V8iEq95L", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1860, "prompt": "\u56fe\u4e2d\u8111\u90e8\u53f3\u4fa7\u663e\u793a\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf\u5757", "answer_id": "9sFRhR7BFNDkDh2ukUfvVT", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1861, "prompt": "\u5982\u4f55\u6cbb\u7597\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u53f3\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u624b\u672f", "answer_id": "J6VfKzQPpZxTyNry5GiJ2w", "gt_response": "\u836f\u7269\u6cbb\u7597,\u652f\u6301\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1862, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "MRI", "answer_id": "GCPrERG2wF6hAMRJFP9PVS", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1863, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u8fd8\u662fT2\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf?\n<image>", "pred_response": "T2", "answer_id": "htJ3h5zqfmDnXJ44zujLYj", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1864, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u51a0\u72b6\u9762", "answer_id": "aaRnaJDUnjWwS8ZtyMzcbE", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1865, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u80f8\u8154?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "WtBXGLffg4cvoVTFLAHbiu", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1866, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "NRZ4VP22SnJ2u6yWJyKB9Q", "gt_response": "\u53f3\u53f6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1867, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "bpMvz8rJfVdaEk3Kaf5GmY", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1868, "prompt": "\u5927\u8111\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "hJsfxuDVjtCWatqLxxHQTe", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1869, "prompt": "\u56fe\u7247\u4e2d\u5927\u8111\u53f3\u4fa7\u7684\u75be\u75c5\u4f1a\u7ed9\u60a3\u8005\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u5934\u75db,\u8033\u9e23,\u808c\u8089\u65e0\u529b,\u547c\u5438\u56f0\u96be", "answer_id": "6UEjrUaHGdkMJPiNWviwmj", "gt_response": "\u89c6\u529b\u969c\u788d,\u5455\u5410,\u8033\u9e23,\u9885\u5185\u538b\u589e\u9ad8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1870, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u663e\u793a\u7684\u662f\u54ea\u79cd\u7c7b\u578b\u7684\u6210\u50cf,\u6838\u78c1\u5171\u632f\u3001CT\u8fd8\u662fX\u5149?\n<image>", "pred_response": "\u6838\u78c1\u5171\u632f", "answer_id": "UsFQdD4zebhd3pbAX4w5NK", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1871, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528\u4ec0\u4e48\u6837\u7684\u6743\u91cd\u751f\u6210\u7684,t1\u8fd8\u662ft2?\n<image>", "pred_response": "T2", "answer_id": "NzdvMNH9ZYWvp7HYA8JY7z", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1872, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u51a0\u72b6\u9762", "answer_id": "5zjC8FX8G6Up6E9j2D8SeK", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1873, "prompt": "\u4ece\u6574\u4e2a\u8eab\u4f53\u7684\u89d2\u5ea6\u770b,\u8fd9\u5e45\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "PCjwiiT4vye5ssuwuW38yj", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1874, "prompt": "\u56fe\u4e2d\u54ea\u4fa7\u7684\u8111\u534a\u7403\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u53f3\u4fa7", "answer_id": "7CWF5zM3KQzSFzU9accjtf", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1875, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "5Q7992V5rw3NfRLxLkiG6f", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1876, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf\u5757", "answer_id": "hLidJLqqAHZST8DbuZAhX2", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1877, "prompt": "\u5982\u4f55\u9884\u9632\u56fe\u7247\u4e2d\u5927\u8111\u53f3\u4e0a\u53f6\u75be\u75c5\u7684\u53d1\u751f?\n<image>", "pred_response": "\u907f\u514d\u9152\u7cbe\u66b4\u996e", "answer_id": "X2BjMgSUMNWg2NHLDQBhXS", "gt_response": "\u6ce8\u610f\u996e\u98df\u536b\u751f,\u52a0\u5f3a\u8eab\u4f53\u7d20\u8d28,\u907f\u514d\u8111\u90e8\u5916\u4f24", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1878, "prompt": "\u8fd9\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u533b\u5b66\u56fe\u50cf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "QRqZAZmqG4TBTizgEULRpk", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1879, "prompt": "\u8fd9\u5f20\u6838\u78c1\u5171\u632f\u56fe\u50cf\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "gk43nUo4YWe2Kt4sZNgyVe", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1880, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "2vYc9BMBPiHEdtTPMfHk99", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1881, "prompt": "\u4ece\u6574\u4e2a\u8eab\u4f53\u7684\u89d2\u5ea6\u770b,\u8fd9\u5e45\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "YZXjQVDwgNfx5ati7jSjdY", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1882, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "PyceLjxn6sVsugwGAG2NkG", "gt_response": "\u53f3\u4e0a\u53f6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1883, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u76c6\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "fpVarRDLn8ZVDY33nKdibQ", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1884, "prompt": "\u56fe\u4e2d\u8111\u90e8\u53f3\u4e0a\u4fa7\u663e\u793a\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u8111\u8840\u7ba1\u75be\u75c5", "answer_id": "mBHBAeUDQif67hTjCaZBsU", "gt_response": "\u8111\u6c34\u80bf,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1885, "prompt": "\u56fe\u4e2d\u80bf\u7624\u662f\u589e\u5f3a\u6027\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "fuvSZKRjaZtYmYALfgSGxv", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1886, "prompt": "\u4ea7\u751f\u56fe\u4e2d\u5927\u8111\u53f3\u4e0a\u4fa7\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8111\u8840\u7ba1\u75be\u75c5", "answer_id": "aXtaxGThHUy3w4HbT5wbQa", "gt_response": "\u8111\u80da\u80ce\u7ec4\u7ec7\u53d1\u80b2\u5f02\u5e38,\u9057\u4f20\u56e0\u7d20,\u5316\u5b66\u56e0\u7d20", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1887, "prompt": "\u8fd9\u5f20\u7167\u7247\u662f\u600e\u4e48\u62cd\u7684?\n<image>", "pred_response": "MRI", "answer_id": "826b79W94qGjwhPxuSmnJ2", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1888, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528\u4ec0\u4e48\u6837\u7684\u6743\u91cd\u751f\u6210\u7684,t1\u8fd8\u662ft2?\n<image>", "pred_response": "T2", "answer_id": "c4A68ejafUg69ir3vKkoFo", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1889, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "H4pevvr8kguU6hUmrqVJ4X", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1890, "prompt": "\u4ece\u6574\u4e2a\u8eab\u4f53\u7684\u89d2\u5ea6\u770b,\u8fd9\u5e45\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "2iLM4JLduZrcJGWAKhMoDT", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1891, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "32a3QAPZ4CBEjPrPFtS3Hb", "gt_response": "\u53f3\u53f6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1892, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "WHv3GE2Ft7jXex4mkkzUH4", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1893, "prompt": "\u56fe\u4e2d\u8111\u90e8\u53f3\u4fa7\u663e\u793a\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf", "answer_id": "MkzzTiKviyhW3oomkhXMQA", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1894, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "2", "answer_id": "nfqV6vv8juSuV7obLwFbJU", "gt_response": "3", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1895, "prompt": "\u4ea7\u751f\u56fe\u4e2d\u5927\u8111\u53f3\u4fa7\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8111\u8840\u7ba1\u75be\u75c5", "answer_id": "koQJBKJVCFcJvNkKbwMkAM", "gt_response": "\u8111\u80da\u80ce\u7ec4\u7ec7\u53d1\u80b2\u5f02\u5e38,\u9057\u4f20\u56e0\u7d20,\u5316\u5b66\u56e0\u7d20", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1896, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7X\u5149\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "WzDCs7TRQyZbASt9DgeAkt", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1897, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528\u4ec0\u4e48\u6837\u7684\u6743\u91cd\u751f\u6210\u7684,t1\u8fd8\u662ft2?\n<image>", "pred_response": "T2", "answer_id": "7Ex6mCJTibsDi25ipAGna2", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1898, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "AVCQpsU6P9p3usLgV5sEyi", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1899, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "gbwEBKmpRWYKAm5WZmjkjm", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1900, "prompt": "\u56fe\u4e2d\u8111\u54ea\u4e2a\u53f6\u662f\u6b63\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u53f6", "answer_id": "HZKYCsbFLydzf6so4RyjmY", "gt_response": "\u53f3\u53f6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1901, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "fnaykUXujnapsfJcL8FqKy", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1902, "prompt": "\u56fe\u4e2d\u8111\u90e8\u5de6\u4fa7\u663e\u793a\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf\u7624", "answer_id": "eLo2vfuBSpDepKjLwEoVrH", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1903, "prompt": "\u56fe\u4e2d\u8111\u6c34\u80bf\u662f\u4ec0\u4e48\u5bc6\u5ea6?\n<image>", "pred_response": "0.2", "answer_id": "LPvZxc42miqgKm24uHUUsm", "gt_response": "\u4f4e\u5bc6\u5ea6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1904, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u662f\u5f88\u5927", "answer_id": "mojot7Knzq85x7GZ4Ftrvi", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1905, "prompt": "\u8fd9\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u533b\u5b66\u56fe\u50cf?\n<image>", "pred_response": "\u6838\u78c1\u5171\u632f", "answer_id": "nkXJCk4hu3EKaru8uHyxEx", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1906, "prompt": "\u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u6838\u78c1\u5171\u632f\u7684\u54ea\u79cd\u7c7b\u578b?\n<image>", "pred_response": "\u6838\u78c1\u5171\u632f", "answer_id": "ZzFB8gQzFkaoPQfQHYfrZL", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1907, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "YgnaUL7iXDiWpNXMvxrq5Y", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1908, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u8179\u90e8?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "SRksFmksUHw4vWpHryD6Hz", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1909, "prompt": "\u56fe\u4e2d\u54ea\u4fa7\u7684\u8111\u53f6\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "aEDk64mETAa2kFQciU62ok", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1910, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "dMkHXXtkPtKMZjKqZPaEWd", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1911, "prompt": "\u56fe\u4e2d\u75c5\u4eba\u60a3\u6709\u8111\u7624\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "n99fwSeTxXTkfLB4BkPbAa", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1912, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "2", "answer_id": "9gXtzWw8dBokBTySFFDHPF", "gt_response": "3", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1913, "prompt": "\u5982\u4f55\u6cbb\u7597\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u5de6\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u624b\u672f", "answer_id": "f3DHTiyDNSdRMoNY9L3dii", "gt_response": "\u836f\u7269\u6cbb\u7597,\u652f\u6301\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1914, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u663e\u793a\u7684\u662f\u54ea\u79cd\u7c7b\u578b\u7684\u6210\u50cf,\u6838\u78c1\u5171\u632f\u3001CT\u8fd8\u662fX\u5149?\n<image>", "pred_response": "\u6838\u78c1\u5171\u632f", "answer_id": "FmwqDzbQeoKXJ4ANJxV4fS", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1915, "prompt": "\u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u6838\u78c1\u5171\u632f\u7684\u54ea\u79cd\u7c7b\u578b?\n<image>", "pred_response": "\u6838\u78c1\u5171\u632f", "answer_id": "6ZY4cVzf4JHEjvTtibsmhq", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1916, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u51a0\u72b6\u9762", "answer_id": "9t2yzAWaKhooDunWBkMa36", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1917, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "5xxhNFViDCRPf6D55pZiEt", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1918, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "JroSuzcwQYXjyHPbfaVGnN", "gt_response": "\u5de6\u4e0b\u53f6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1919, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "Xw6yYxcqutpYL4EjYRugLc", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1920, "prompt": "\u56fe\u4e2d\u8111\u90e8\u5de6\u4e0b\u4fa7\u663e\u793a\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf\u7624", "answer_id": "Ag94zd3E3oy7im5rREBHrV", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1921, "prompt": "\u5982\u4f55\u9884\u9632\u56fe\u7247\u4e2d\u5927\u8111\u5de6\u4e0b\u53f6\u75be\u75c5\u7684\u53d1\u751f?\n<image>", "pred_response": "\u5065\u5eb7\u751f\u6d3b\u65b9\u5f0f", "answer_id": "jm3knbUp5UJHpXRoHPjy3z", "gt_response": "\u6ce8\u610f\u996e\u98df\u536b\u751f,\u52a0\u5f3a\u8eab\u4f53\u7d20\u8d28,\u907f\u514d\u8111\u90e8\u5916\u4f24", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1922, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "MRI", "answer_id": "DSy4pmifbkfGrgbaHnR4jK", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1923, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u8fd8\u662fT2\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf?\n<image>", "pred_response": "T2", "answer_id": "gPNKpTqQrUgXzBygUjqkNz", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1924, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u51a0\u72b6\u9762", "answer_id": "Gawe9EttfBKv9Hwq5yWYPN", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1925, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "SAra2XoTRxo6ApBcNbsJq7", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1926, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "PoBgxv7TSi6JZE8a4WjmSm", "gt_response": "\u53f3\u53f6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1927, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "QzcLSr59jFewmT74obS2sb", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1928, "prompt": "\u60a3\u8005\u7684\u5de6\u8111\u53f6\u662f\u5426\u5f02\u5e38?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "PmtznGrtRFXSAjMGAfcMy2", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1929, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u662f\u9ad8\u5bc6\u5ea6\u8fd8\u662f\u4f4e\u5bc6\u5ea6?\n<image>", "pred_response": "\u9ad8\u5bc6\u5ea6", "answer_id": "LAFzeHBmqsiqdmFZV8vA8d", "gt_response": "\u4f4e\u5bc6\u5ea6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1930, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "2", "answer_id": "EMnerpquneofQYdASdBAbY", "gt_response": "2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1931, "prompt": "\u56fe\u7247\u4e2d\u5927\u8111\u5de6\u4fa7\u7684\u75be\u75c5\u4f1a\u7ed9\u60a3\u8005\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u5934\u75db,\u8033\u9e23,\u89c6\u89c9\u969c\u788d,\u547c\u5438\u56f0\u96be", "answer_id": "dwqv9yUNjNJNNU6BerscG3", "gt_response": "\u89c6\u529b\u969c\u788d,\u5455\u5410,\u8033\u9e23,\u9885\u5185\u538b\u589e\u9ad8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1932, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7CT\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Fob2u9XMMxnV2XTspGh487", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1933, "prompt": "\u8fd9\u5f20\u6838\u78c1\u5171\u632f\u56fe\u50cf\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "H8kqX2QGcTBxVaPF3t69rB", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1934, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "mddfewyCUFiaX2NvFDeQiB", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1935, "prompt": "\u56fe\u50cf\u5728\u8eab\u4f53\u91cc\u4ee3\u8868\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "jKrUYESjeFwsMNJ3xcNL5h", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1936, "prompt": "\u56fe\u4e2d\u54ea\u4fa7\u7684\u8111\u53f6\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "oJhvmJ4GwMHCHiyWyR2QVf", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1937, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u76c6\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "62fibfXk4pdqzvLAebJQ5n", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1938, "prompt": "\u56fe\u7247\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "khKz28PgMCnGdpZfsMgCBE", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1939, "prompt": "\u56fe\u4e2d\u80bf\u7624\u662f\u589e\u5f3a\u6027\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Bs4SGEMCsAYnmW6xbRkt7H", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1940, "prompt": "\u5982\u4f55\u9884\u9632\u56fe\u7247\u4e2d\u5927\u8111\u53f3\u53f6\u75be\u75c5\u7684\u53d1\u751f?\n<image>", "pred_response": "\u5065\u5eb7\u751f\u6d3b\u65b9\u5f0f", "answer_id": "ngQcuLhsUHpHZhqY6HrnMb", "gt_response": "\u6ce8\u610f\u996e\u98df\u536b\u751f,\u52a0\u5f3a\u8eab\u4f53\u7d20\u8d28,\u907f\u514d\u8111\u90e8\u5916\u4f24", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1941, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "AUQbZYiSvibRbbpM2LVocn", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1942, "prompt": "\u8fd9\u5f20\u6838\u78c1\u5171\u632f\u56fe\u50cf\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "WkRrekVUhdPQmCAUbAtdNF", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1943, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "AxCJPJh5hZcQ5PPNVR465q", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1944, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u80f8\u8154?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "ddjeLTfrSu2pGN2JbU4N43", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1945, "prompt": "\u56fe\u4e2d\u8111\u6c34\u80bf\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "EMPo6FvwowxMrXBBLiUeTY", "gt_response": "\u53f3\u4e0b\u53f6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1946, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "8PMhpQiu2qZVQwD7ydZ6ck", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1947, "prompt": "\u5927\u8111\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "kL97kj3JP9c5wvWXBztbPX", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1948, "prompt": "\u56fe\u7247\u4e2d\u5927\u8111\u53f3\u4e0b\u4fa7\u7684\u75be\u75c5\u4f1a\u7ed9\u60a3\u8005\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u5934\u75db,\u8033\u9e23,\u808c\u8089\u65e0\u529b,\u534a\u8eab\u4e0d\u9042", "answer_id": "Wfz8tSJPbxhAM76AD573PP", "gt_response": "\u89c6\u529b\u969c\u788d,\u5455\u5410,\u8033\u9e23,\u9885\u5185\u538b\u589e\u9ad8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1949, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7X\u5149\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "GXiUHxX2fNvXs9BWHELmi7", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1950, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u8fd8\u662fT2\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf?\n<image>", "pred_response": "T2", "answer_id": "ExtNUHM2oa3n3A5XMRD9zx", "gt_response": "T1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1951, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u51a0\u72b6\u9762", "answer_id": "JERjw5FgSaeGeTRtq8JXUB", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1952, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "i4XLjmmFxHmkYDzeFhEySq", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1953, "prompt": "\u56fe\u4e2d\u8111\u6c34\u80bf\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "TEBCtYW59dQ26Qn9YKxToF", "gt_response": "\u53f3\u4e0b\u53f6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1954, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "daykXAYhwWu5HjRpJDpVfV", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1955, "prompt": "\u5927\u8111\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "TLRfhv9VBAyJaYQmAgTUuG", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1956, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u7684\u589e\u5f3a\u578b\u8111\u80bf\u7624\u662f\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "A3Npqv5sognsvTpWdyKDEd", "gt_response": "\u767d\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1957, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "2", "answer_id": "7XddAHHeWfkEwZEAQPgPuJ", "gt_response": "3", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1958, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "FJQuNFHD4w72FAq34ArXYT", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1959, "prompt": "\u8fd9\u5f20\u7167\u7247\u662f\u600e\u4e48\u62cd\u7684?\n<image>", "pred_response": "MRI", "answer_id": "bvCeWebTNWXm9rosKyzZVD", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1960, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "WUaVBWPPYaGenP6eFWew9A", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1961, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "PZQHV9MHcn5YmQBxgpWznT", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1962, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u662f", "answer_id": "KZYj6JaRdTDzJphtCwou4x", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1963, "prompt": "\u56fe\u4e2d\u8111\u90e8\u589e\u5f3a\u80bf\u7624\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "HTSFvH9HXoBtbTjCkufpup", "gt_response": "\u53f3\u4e0a\u53f6", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1964, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "bfpo4LT4hTjVEDXHstNxiA", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1965, "prompt": "\u60a3\u8005\u7684\u5de6\u8111\u53f6\u662f\u5426\u5f02\u5e38?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "RrnBAXQTwyb2BEVA6vueYh", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1966, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "2", "answer_id": "XtUx3c9WYoKdo6cUidsLs7", "gt_response": "3", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1967, "prompt": "\u4ea7\u751f\u56fe\u4e2d\u5927\u8111\u53f3\u4e0a\u4fa7\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8111\u8840\u7ba1\u75be\u75c5", "answer_id": "dUiNHLAjNtZiVU4HtAFCGX", "gt_response": "\u8111\u80da\u80ce\u7ec4\u7ec7\u53d1\u80b2\u5f02\u5e38,\u9057\u4f20\u56e0\u7d20,\u5316\u5b66\u56e0\u7d20", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1968, "prompt": "\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u88ab\u8fd9\u5f20\u56fe\u7247\u4f7f\u7528?\n<image>", "pred_response": "MRI", "answer_id": "Fdue4AQvEsFte6YQffgj8M", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1969, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u8179\u90e8\u4e48?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "ciQDqrCcT992h2ys5JBrnB", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1970, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "eafNWtNDdsBnZpTbFn2bc7", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1971, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u76f4\u80a0\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "42sidqwys54ZW6hGhSKydE", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1972, "prompt": "\u5728\u8fd9\u4e2a\u56fe\u50cf\u4e2d\u5de6\u80ba\u548c\u809d\u810f\u54ea\u4e2a\u4f53\u79ef\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "gfVSXXRLWWTpBQ9tuTZ2qW", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1973, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "VMD2jriGFA8qqixdYsm7m5", "gt_response": "0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1974, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u810a\u9ad3\u7684\u5f62\u72b6\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u4e0d\u89c1", "answer_id": "jqFRqUPCfB8FBmzB7vHZnS", "gt_response": "\u5706\u5f62", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1975, "prompt": "\u8fd9\u5f20\u56fe\u7247\u5de6\u8fb9\u7684\u7070\u8272\u5668\u5b98\u6709\u4ec0\u4e48\u4f5c\u7528?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "PQCpx9T6GU3XMn5et9bF6Y", "gt_response": "\u751f\u7269\u8f6c\u5316\u3001\u89e3\u6bd2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1976, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "jxZZPYkya8NJFuyggyrGDp", "gt_response": "\u80c3,\u7ed3\u80a0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1977, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "UpvjKPXSnv7VfHVEumN3xo", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1978, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u53ef\u4ee5\u5206\u6ccc\u9176\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "N8a99zdPdxtJpaspn6ML7L", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1979, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u76c6\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "EFfN8dRj9ubVaA2WcQTovR", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1980, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "32ERUcp8rpM3MiZqaem9zQ", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1981, "prompt": "\u8fd9\u5e45\u56fe\u50cf\u91cc\u6709\u6c14\u7ba1\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "iihA66R84nYkFv9XEzESck", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1982, "prompt": "\u56fe\u4e2d\u80be\u810f\u7684\u989c\u8272\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "fGFspLw7F3XDNNCgGqCTBD", "gt_response": "\u7070\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1983, "prompt": "\u5728\u8fd9\u4e2a\u56fe\u50cf\u4e2d\u5c0f\u80a0\u548c\u80be\u810f\u54ea\u4e2a\u4f53\u79ef\u66f4\u5c0f?\n<image>", "pred_response": "\u80be\u810f", "answer_id": "BxxHrjdGHaeiAtb3dzmgd8", "gt_response": "\u80be\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1984, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u5668\u5b98?\n<image>", "pred_response": "2", "answer_id": "eeHuSARvYDhJr2ugAaakWp", "gt_response": "3", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1985, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u4e2d\u6700\u53f3\u8fb9\u7684\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80a0", "answer_id": "oY2jNY5VYy5JTEx5GFs4NB", "gt_response": "\u5438\u6536\u8425\u517b,\u5206\u6ccc\u9176,\u6d88\u5316\u98df\u7269", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1986, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u5206\u6ccc\u9176\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "MmUErVobo9RF4nYsJtwBK2", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1987, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u89e3\u6bd2\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "KcTsnNC38w5pL4xdhGJkpL", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1988, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7CT\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "JcUoSfvaKh4MBZ6i4toVNv", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1989, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "VuWsjQjTDD4Qq4ZjwfohQk", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1990, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "3htbg6YtBNd5CfbJTWrVkS", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1991, "prompt": "\u56fe\u50cf\u662f\u5426\u5305\u542b\u7ed3\u80a0?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "8gtVZC4RCPhB5xghWkT9SJ", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1992, "prompt": "\u8fd9\u5f20\u56fe\u7247\u91cc\u6709\u80a1\u9aa8\u5934\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "B3xU8YwCb7664c5sWKnvXJ", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1993, "prompt": "\u5728\u8fd9\u4e2a\u56fe\u50cf\u4e2d\u53f3\u80ba\u548c\u5fc3\u810f\u54ea\u4e2a\u4f53\u79ef\u66f4\u5c0f?\n<image>", "pred_response": "\u53f3\u80ba", "answer_id": "dEMpwvVkTVpWEVVdHEsGoJ", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1994, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u5668\u5b98?\n<image>", "pred_response": "2", "answer_id": "bgP7zJTkZNY8qjRoYTwnSC", "gt_response": "4", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1995, "prompt": "\u56fe\u50cf\u53f3\u4fa7\u7684\u9ed1\u8272\u5668\u5b98\u6709\u4ec0\u4e48\u4f5c\u7528?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "cJtnNAAuVzY4nGuYraYCts", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1996, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "Rf5TcmmcjNUPSqKHvAVruH", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1997, "prompt": "\u56fe\u4e2d\u6709\u6ca1\u6709\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u547c\u5438?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "nQjPcTkbPVCM4ybyihyEdC", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1998, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u795e\u7ecf\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "AUudytB7XqPqZybrWwfGP3", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 1999, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "THqKsKp7gwEjBuvKD8sSbd", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2000, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u80ba\u5417?\n<image>", "pred_response": "\u6709", "answer_id": "dUk85PmyBYQMCqHR7rHfrm", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2001, "prompt": "\u56fe\u50cf\u91cc\u6709\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "U8WVciacMNoSfXr4DLWbFa", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2002, "prompt": "\u56fe\u50cf\u53f3\u4fa7\u7684\u9ed1\u8272\u5668\u5b98\u6709\u4ec0\u4e48\u4f5c\u7528?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "SC4gC4U6HMsHDusYzWRDwf", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2003, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "8eHezagSM5RFeLaMFzhwGj", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2004, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "GLwexVRZP2PMUPQnwguLy6", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2005, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "6n4s4HLRvUqr3SYSLZYD9g", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2006, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "ncmufcM9HmbKm5zG3Cagdb", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2007, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "nzmbopRQjBqgY3oVCJtczG", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2008, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "nnusMf2xNCEUvEkVeDy9um", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2009, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "P5Z2XHyqXjJ5EXDZnYKjqX", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2010, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "4wEnEDS5ea5d43gQ4ed7oV", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2011, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "kbeoMMYjE36qoYtAW3tjbu", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2012, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "DA8RgzTyinXi2VugZoJqau", "gt_response": "\u76c6\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2013, "prompt": "\u8fd9\u5f20\u56fe\u7247\u91cc\u6709\u76f4\u80a0\u5417?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "RDU8qLRbg2FwXeFjnbbNj3", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2014, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u98df\u9053\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "KRaYVu2YARSd4QG5fa6LKi", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2015, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf?\n<image>", "pred_response": "\u80a0", "answer_id": "WUEKjhR6PRCH4aqJTU4H3h", "gt_response": "\u76f4\u80a0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2016, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u8180\u80f1", "answer_id": "m66pLFMq3kd7gSRzP5A2iS", "gt_response": "\u76c6\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2017, "prompt": "\u56fe\u4e2d\u76f4\u80a0\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0b\u80a0", "answer_id": "fxwjTecBKCCxfDSCWr76XR", "gt_response": "\u5e95\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2018, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "hvJHBeXkNiRXBM8wqSPePd", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2019, "prompt": "\u8fd9\u5e45\u56fe\u50cf\u91cc\u6709\u773c\u775b\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "jGXiaUm2tbn3TcesMesNcB", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2020, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u5c0f\u80a0\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "ZZBEMNY8mdzUprHamitN3C", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2021, "prompt": "\u5728\u8fd9\u4e2a\u56fe\u50cf\u4e2d\u8180\u80f1\u548c\u76f4\u80a0\u54ea\u4e2a\u4f53\u79ef\u66f4\u5c0f?\n<image>", "pred_response": "\u8180\u80f1", "answer_id": "i9hMKvWL73v6xq46yFBTW5", "gt_response": "\u76f4\u80a0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2022, "prompt": "\u4f4d\u4e8e\u56fe\u50cf\u5e95\u90e8\u7684\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5c3f\u9053", "answer_id": "QcFdEHWoGMv65SfRcyuNq5", "gt_response": "\u50a8\u5b58\u7caa\u4fbf,\u6392\u6cc4\u7caa\u4fbf", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2023, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf?\n<image>", "pred_response": "\u80be\u810f", "answer_id": "EEjzqLbwiqMynEHUcUYAHQ", "gt_response": "\u5c0f\u80a0,\u76f4\u80a0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2024, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u547c\u5438\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "BrheDSFgApFqHLRpRgbThh", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2025, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u53ef\u4ee5\u63a5\u6536\u58f0\u97f3\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "4PA8dMCVQQNjNeMEtiTwcG", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2026, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8179\u80a0", "answer_id": "6fRveUnuVuTKNNMqkovKKD", "gt_response": "\u76c6\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2027, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u5c0f\u80a0\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "CpvAQ8pP5tHiUvzkz7mFB2", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2028, "prompt": "\u56fe\u7247\u4e2d\u7684\u8180\u80f1\u662f\u4ec0\u4e48\u989c\u8272\u7684?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "Bw8xL3PBv3h3Pby35EJcKS", "gt_response": "\u6df1\u7070\u8272", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2029, "prompt": "\u56fe\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "bk4iE3LTvyAS34ZypSUkSS", "gt_response": "\u8180\u80f1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2030, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u6ccc\u5c3f\u7cfb\u7edf?\n<image>", "pred_response": "\u8180\u80f1", "answer_id": "kbTrrjmfqdbxUGFVs3vKWW", "gt_response": "\u8180\u80f1", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2031, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "RonKGcicuTSMnpd5Ny3Mu9", "gt_response": "\u76c6\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2032, "prompt": "\u56fe\u4e2d\u76f4\u80a0\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0b\u80a0", "answer_id": "FGfpUfBKaQHRZCDPLBatur", "gt_response": "\u5e95\u90e8", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2033, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u76c6\u8154\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "jsvuLY4zwJgjBvAoQxnqo8", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2034, "prompt": "\u56fe\u50cf\u662f\u5426\u5305\u542b\u7ed3\u80a0?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "aRZN9i3uG2PMCU9E3iqdjM", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2035, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u76f4\u80a0\u5417?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "ipjrQNhyJyYqM7bFCSsJkX", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2036, "prompt": "\u5728\u8fd9\u4e2a\u56fe\u50cf\u4e2d\u76f4\u80a0\u548c\u7ed3\u80a0\u54ea\u4e2a\u4f53\u79ef\u66f4\u5927?\n<image>", "pred_response": "\u7ed3\u80a0", "answer_id": "CG8LotwYz7aCiNsfSdN65Y", "gt_response": "\u7ed3\u80a0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2037, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf?\n<image>", "pred_response": "\u80a0", "answer_id": "BDLYkYq4ZRKo3wcfoTYK3Z", "gt_response": "\u5c0f\u80a0, \u76f4\u80a0, \u7ed3\u80a0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2038, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "RYTowzkUcmZXQFFVtpx5yt", "gt_response": "\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2039, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "nP8iqoZkSDL4JB8Cg52H5D", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2040, "prompt": "\u56fe\u50cf\u662f\u5426\u5305\u542b\u76f4\u80a0?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "3TYWaB6D9axzZPAAAVgGFy", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2041, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u662f\u5f88\u5927", "answer_id": "DzJ725iTRYbwLpH6pPGmjH", "gt_response": "\u8f83\u591a", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2042, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6ccc\u5c3f\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "eYsdijKkb9o2UgQ5wdBGVf", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2043, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "eUfeNuz24EfnJxjKntMDGT", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2044, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u8180\u80f1", "answer_id": "UNNdXnWpKXyHQGR8qy9nob", "gt_response": "\u76c6\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2045, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u76c6\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Df2wFkXEFf85manL2QKy49", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2046, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "CXUEcGdWL8njf528HTTmDr", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2047, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u6d88\u5316\u98df\u7269\u5417?\n<image>", "pred_response": "\u4e0d\u53ef\u4ee5", "answer_id": "XRj7E9uHmi3vkSq9bjbwK9", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2048, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "fNcSUNk7jjYrB9bAhzPjmb", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2049, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "cJnhu4a29znUxdhfgg8JSA", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2050, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "V3AWGxFUvXotE9dFJwa988", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2051, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "XV2GKEjrBQcnrfmg8DZ3LB", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2052, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "DEkSe8cJXvkPzRir3CKd3T", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2053, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u8111\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "eQXUueTgz8ayyHzRQusSFb", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2054, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf\u7624", "answer_id": "nDTk5XA2cBivhktbpEgYco", "gt_response": "\u8111\u80bf\u7624,\u8111\u6c34\u80bf", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2055, "prompt": "\u5927\u8111\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "MKhEGhkKwcFa3fC56V2U33", "gt_response": "\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2056, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "iQGeYQ8oPf2Mpb7QJFm7BT", "gt_response": "\u53f3\u4e0b\u89d2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2057, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "SzcXVtZnitoAbqDtSgpNXc", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2058, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "a76aoX3QEwdqMtZ2BDQUue", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2059, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "6m2tUfj6Z9mfg2i7eVUeRF", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2060, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "RpNgL9Knb9dt7fbmML2jP2", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2061, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "UFwBpWbvcGyXEgW24woXKL", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2062, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u8111\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "j3N4Nrcd4fSfT3j8GzsuBR", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2063, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf\u7624", "answer_id": "7EuSs2Qat7iq5gKgn4jwKe", "gt_response": "\u8111\u80bf\u7624,\u8111\u6c34\u80bf", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2064, "prompt": "\u5927\u8111\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "NsTPo7raieuKcKjhxbHc8h", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2065, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "VEZtoYBmNBBXjLB4bKoiLW", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2066, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "MRI", "answer_id": "StiRSJM8N2MKvj8ymBmYmG", "gt_response": "MRI", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2067, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "2JMYk55rST5DbaP5Pd3Hbs", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2068, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "T2", "answer_id": "GGTvePF7rqSpXT8GAuhQmc", "gt_response": "T2", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2069, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "h5SNzYK8vQ6tPVHcaZYFuJ", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2070, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "ZCcbQcV9wXAwEkrwx7ypZy", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2071, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u8111\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "58WF4MhEUSMSM3WdmtNsvh", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2072, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u8111\u80bf\u7624", "answer_id": "Njw3T6ZQecDZ4fK6nyxNX9", "gt_response": "\u65e0", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2073, "prompt": "\u5927\u8111\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "ZENkLzjTKGqZ7HQNcFAwJj", "gt_response": "\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2074, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "X-Ray", "answer_id": "LtvajruccDjTvLoY2gGBHZ", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2075, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "7vqZuL46TdtjSbhCBdCNME", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2076, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "efXnA7BiZyarExoH3XJfrA", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2077, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "52MLWyTZNYvNG4AWDscL9g", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2078, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "mv6p3M9ryi9KfjWbMwzjwv", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2079, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "3ZyFz3HWhRMf2yb24NzLt8", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2080, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "dCBaQhx5kZqzNzQp8p3LSq", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2081, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u708e", "answer_id": "Hp4vMh5cmEkxDPNTcKoQoa", "gt_response": "\u80ba\u764c", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2082, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "YFcQUJ7C2v3CJ3rACYKUH7", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2083, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4e0b\u80ba", "answer_id": "Ri9z6EJttxdruWZ3aHSy2U", "gt_response": "\u53f3\u80ba, \u5de6\u4fa7", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2084, "prompt": "\u56fe\u4e2d\u80ba\u3001\u5fc3\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u80ba", "answer_id": "GpFBqrBDEzmhyE9apHVNYQ", "gt_response": "\u80ba", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2085, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "L33Z8yyYiNR6vPdxZqgJuy", "gt_response": "CT", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2086, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "WW7jwCWGmXin5mFygKagbs", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2087, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "8ef7LAjBeRYdPo8tg26dGX", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2088, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "6BPntYHQYUoFpgRdcmD8oW", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2089, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "hXv4W7dsfGHSxiLygkUVzU", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2090, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "4YJuweSVMehC3QyKjcp86c", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2091, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u5305\u542b", "answer_id": "Gf7PZudBzwHF9ZY7v6bMXN", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2092, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "FcMyFpGZTqyrimiChaxDSR", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
{"question_id": 2093, "prompt": "\u56fe\u4e2d\u80ba\u3001\u5fc3\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "QkQdVLhv5sNvVcw73mm5zd", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct_smri_sxray", "metadata": {}}
