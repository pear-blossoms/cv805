{"question_id": 0, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "FtxL2RDoCsfD2q6Uhm3yTJ", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "9b7kGArzzHH9G4FjQ8sa6v", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "BKx6pZonSkunwxTboeZUmn", "gt_response": "Lung, Spinal Cord", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 3, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "82AYtWYkqvx5qv6fN3QPqG", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 4, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "f597eg2mzGzXFkSQjR8qDF", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 5, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "iXJmTY9azmmUCEYr4nhSq4", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 6, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "LMQJRGDGxtLMPGbijNHc7D", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 7, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "Ry3QSyV6CuLX2ucCh8Ut5y", "gt_response": "Lung Cancer", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 8, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "BmCrSwnMLLJvQuNnP8WAxY", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 9, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "f2erenJnMiUnDVWPvbrDP2", "gt_response": "Left Lung, Right", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 10, "prompt": "Which is the biggest in this image,lung,liver or heart?\n<image>", "pred_response": "Lung", "answer_id": "YmHpf9HyMnifE5t8HenEkJ", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 11, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "PtiwvJzMc4KCxXwM6LwSXL", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 12, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "K33XXVnog55wGYGQw9iEhH", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 13, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "ij4W3bfS7N9mQDzKx3FXWp", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 14, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "WPmGxpNcjyxFjA3bonHGFy", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 15, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "5munwU7N9tQvrdwwwaGc9K", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 16, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "ivGHgLtawDQ8SPM922c4g5", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 17, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "bW8CFt4WgBHkyAhUw39rMR", "gt_response": "Lung Cancer", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 18, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "3fQZ8YnP6hSvJ2wruahsNB", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 19, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Chest", "answer_id": "m5Ky28hjbRgtqBTzN4neeL", "gt_response": "Left Lung, Right", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 20, "prompt": "Which is the biggest in this image,lung,liver or heart?\n<image>", "pred_response": "Lung", "answer_id": "jkWuNbvQHEaR3PcTtYrqpE", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 21, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "hPCYsYne4tNKPvUmmMjXdt", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 22, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "em4knTYEd4b3Z3Ba6crRny", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 23, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "62arY24BX6gubDJqNq62F2", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 24, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "mPdr9smT29Q4PaSD8t8mMh", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 25, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "WyqkPXJCpQGqnAy5nmkhC4", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 26, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "2MjeVHC6u5pWgYZtTrq9tB", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 27, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "m9g2KFSh8933vxGkbdwG3L", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 28, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "G8FpXY4HkPqp54LkCZsT59", "gt_response": "Lung Cancer", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 29, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "a8nVp2tis8jHXUfhJC3jeh", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 30, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "Y2wHGRiv9yuTpazJ64Lm4f", "gt_response": "Right Lung, Left", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 31, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "RTGqPXG97xW3oJWHkxLjT2", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 32, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "EWp93CwxbXCHLPRWBv6pDb", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 33, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "Nd5X7GhMPgavrHLHkVu8oX", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 34, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "dqNUqztddi8P6DXQZaPCfD", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 35, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Chest", "answer_id": "aFpLpHaenrSFgdV5NXPr3Q", "gt_response": "Center", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 36, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "H79mHFhQ36TKQgSphRtj56", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 37, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Heart", "answer_id": "ExGfsQuEeEbiwDTM3qdoHt", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 38, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "USG3dRUypgQErUp5EDYUkG", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 39, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "CpUuoPAhZskgjuaSCjfrgx", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 40, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "0", "answer_id": "m3PQfFFLeL4mBhYuPocApL", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 41, "prompt": "Is there any abnormality in the spleen?\n<image>", "pred_response": "Yes", "answer_id": "bUyJAbMgUqBDkjbJzUfEre", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 42, "prompt": "Does the liver look normal?\n<image>", "pred_response": "No", "answer_id": "PEbFrB2UZruNVLPA5U7jAy", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 43, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "idcHfHvm364HyTydkoygcL", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 44, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "epsnVfuxiuHVMHSCM8gS6A", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 45, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "c7NckoWjkX4EufiRL8TyDc", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 46, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "3hP2Uk9fPpMxVNytqw6RCd", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 47, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "ZQJG7CyeZWdanuMB5EzAPe", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 48, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "No", "answer_id": "SpKU6RWzL2DMLYeeh4Mih5", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 49, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "2sVEeiEyHQBAocERdBcvJa", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 50, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "5M9QKTWysDMDrxkKaZMaV3", "gt_response": "Pneumonia", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 51, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Chest", "answer_id": "nEKgZZ6xo5JC6uKiaPKT34", "gt_response": "Left Lung, Lower Right", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 52, "prompt": "Does the lung look healthy?\n<image>", "pred_response": "Yes", "answer_id": "3wziRMfbmxTun9NXY4gfqP", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 53, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "9RYFuqMjAbEnAiuyFFQKzA", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 54, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "5AjgLMbuxy3cxfRfuC55Ns", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 55, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "DzVhHf6orMXtgoasgHBMLi", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 56, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "JDAtXe6TrmwxViTbHAdTam", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 57, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "PS6CyXJqWq2Mw2cDVkVBTr", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 58, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left Lung", "answer_id": "AuvvhLh98bHDcfxvRTknCU", "gt_response": "Center", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 59, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Heart", "answer_id": "fjbYLSNuDrFrr9mdy8MapN", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 60, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "mpmZxeTTSXxH3sU2ASn9x8", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 61, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "HrUHvMmvgv8CGQDrWr8cDb", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 62, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "9GDuJAZVynmGE4wDGBKpF7", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 63, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "6RAmm4Enuwx62HrLfz47Sb", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 64, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "kDBx6kTqahw9NXMn6Fuq8A", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 65, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "gdN5wBjhfsM6pdMB7KTKRF", "gt_response": "Center", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 66, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Heart", "answer_id": "ii4Ao9cE4PQiBeYsuWjPSj", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 67, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "jZ8t29ieKMv6pzXQhzqxjG", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 68, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "8AySV5qhLXvzJQFmWGtcjj", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 69, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "U4mRa3YvEepPVMc3j5CG6R", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 70, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "dCF95AkCBcsYUWjo5Corcw", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 71, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Heart", "answer_id": "arnRgpVduoz7iY5xPiC3yM", "gt_response": "Atelectasis, Mass", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 72, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "Bn6i3qxn6XWbfWuuAdnw6Z", "gt_response": "Right Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 73, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "S8cNHLAxNHuzBxfYBDHyUR", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 74, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Heart", "answer_id": "mRcyh5d6pcC4GDXf7bPZTk", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 75, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "B7bu74nehRR3gNcgHBQn4v", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 76, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "RogwdEGoY7bcAGrgNV9KJH", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 77, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "No", "answer_id": "M4Y4eVgWJfk9uR3kiQe2Aj", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 78, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "Lgz8B6iUmFCngLe97dXfFq", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 79, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Cancer", "answer_id": "2oG2VYkEbVvK9tPBk8obSd", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 80, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Chest", "answer_id": "9pVXaQT6FC9cJyRb4ndDus", "gt_response": "Center", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 81, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Heart", "answer_id": "eeyPcFa5E7JJa7VoiHogKx", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 82, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "MURSpHs23vDuTyvDqGnncB", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 83, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "QCuxnxvDyp3vkhe3cH53yv", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 84, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "GjhWknLHJVg8WWwDxUYuii", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 85, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "dhckfUcaZovFKiorop6SMd", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 86, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Cancer", "answer_id": "bLTEh3UWd4pKvdtAV9VCwH", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 87, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Chest", "answer_id": "HgnS64JnoEteG78DbWcb2C", "gt_response": "Center", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 88, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Heart", "answer_id": "Pc4YWy4uWP96JtioFPagBz", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 89, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "5yUTYCthk6JM2QtKSCJFgA", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 90, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "GhHJTx7mLkZzYzitES7GoJ", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 91, "prompt": "Does this image look normal?\n<image>", "pred_response": "Yes", "answer_id": "AqGC2BMyPa6F9vfF9PiPzW", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 92, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "HWQPk7FgzPf6NHq6AmoTap", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 93, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "66vpGpnUeoqbeG3f88sdNe", "gt_response": "Nodule", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 94, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "2mg47RFAyMy6uFeqH6w45J", "gt_response": "Left Lung, Right", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 95, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "eqmpTUFRPwBzvSZD7uX3Pb", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 96, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Heart", "answer_id": "M6PYRsaxubiJaZGMP5ph88", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 97, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "aCyJBaJsfVx6nr22aHot3d", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 98, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "YJVMQ3dUmmFCictksr6in8", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 99, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "YqvnYzL4BbbwdoJAStzMMU", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 100, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "LqViMja7Zk3RdGTLnTxr2J", "gt_response": "Nodule", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 101, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Chest", "answer_id": "eSD69ehubzCUxUP89iDT4u", "gt_response": "Right Lung, Left", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 102, "prompt": "Does this image look normal?\n<image>", "pred_response": "No", "answer_id": "GvpgFwN5RU7KJTnrCPop5g", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 103, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "hsvUuXvycgevFr7rdWWTdy", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 104, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Heart", "answer_id": "Hnfd5HfUJmHkCWDFzsQMFz", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 105, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "4eyr7wjg63LYS7CxeQjrHJ", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 106, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "jfumxaZhykLMCc5ewa3Gug", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 107, "prompt": "Does this image look normal?\n<image>", "pred_response": "Yes", "answer_id": "mBFTwLSghbDXMaEyye6yCz", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 108, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "nRHvHW2Kr8Mqo5GLreshcx", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 109, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "The image shows the lungs, heart, and the ribcage, which are all part of the human body.", "answer_id": "AkaxDSDHSrzhCGgnFGBAfp", "gt_response": "Atelectasis", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 110, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left Lung", "answer_id": "LbknJyinQzLzUQrH4hk9cP", "gt_response": "Right Lung, Lower Left", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 111, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "mY3FQGKybSoQXGmP42SKeW", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 112, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Heart", "answer_id": "JHPDjPNDQrcqSAZaitMHfd", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 113, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "aP5SdM43nKFk5cJyQSNk6f", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 114, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "9AHpMopo8fgEtE397SuS9r", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 115, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "QdY7eB9YaPaTec6PUJ6HJV", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 116, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "khA3qpZSd9GYP5PtKGodYE", "gt_response": "Pneumonia", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 117, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Lung", "answer_id": "hHLa5Ld57kaH5tXxFCAUhR", "gt_response": "Left Lung, Right", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 118, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "CSePUBBzQ2e9nsacpeg86E", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 119, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "No", "answer_id": "aMUsS48Y9iyEA2p4H9q2ts", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 120, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Heart", "answer_id": "TpN9TQe3zcNFcP6oGu9XHK", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 121, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "S6kAytGpWqPmf99zU2isJY", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 122, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "8WygWDoopjJTYkXibrS9zc", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 123, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "0", "answer_id": "H6a5HJuYEhDyumEvuixPdH", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 124, "prompt": "How many kidneys are there in this image?\n<image>", "pred_response": "2", "answer_id": "R7o8atN2nTZavxpVZeHvHN", "gt_response": "1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 125, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "kfdTFB5UVRTWFnwbSmZafG", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 126, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "Da7AFKD54BsooxjSRmB4zR", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 127, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "nRTbGaQtBGFdsgbyzv9HEV", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 128, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "EhpdPHgJaKG3sCaVbRVn6t", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 129, "prompt": "Does the liver look normal ?\n<image>", "pred_response": "Yes", "answer_id": "Fg5MJpQhCEocETtVAzAPeY", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 130, "prompt": "Does the kidney look abnormal?\n<image>", "pred_response": "Yes", "answer_id": "36iCRJubjy23q6LE7Cqz4n", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 131, "prompt": "Do the spleen look abnormal?\n<image>", "pred_response": "Yes", "answer_id": "ZEQDBM7RMTvN6rUeViJ6ka", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 132, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "kQ3sfnXMoufw2XieHkibNw", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 133, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "aPuXnJthYmjXCmMHBTd24w", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 134, "prompt": "Does this image look abnormal?\n<image>", "pred_response": "Yes", "answer_id": "eDiVyyXd4YeqFwx3PsMoUv", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 135, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "Qhi2HBXD5m33NqVaqvpsc8", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 136, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Cancer", "answer_id": "cDrveBa2JzVLenW6zqCGJM", "gt_response": "Pneumothorax", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 137, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Chest", "answer_id": "dDkFha8cFB2ufconcyjMio", "gt_response": "Right Lung, Left", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 138, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "fjTY7sstnPQMgr8fSzXqe4", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 139, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "F8ENUxYP2dVWqEe5ZcbSba", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 140, "prompt": "Which organ is abnormal, heart or lung?\n<image>", "pred_response": "Heart", "answer_id": "MsNGq4CKxGB8z7rPkdGCym", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 141, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "X5SL2MsBEG2WYBru4VDp4W", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 142, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "5HF2HXhPFQXJmzCNiuSBAy", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 143, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "fhMaxsXsjcB9Lujt4wLotu", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 144, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "NkTbcixbffzfKQC6LLdtgn", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 145, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "Temo4ADPyUL2g6EFoWEqwi", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 146, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "Hx64U7kKdeWsQJNTaToj2d", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 147, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "L3p6qdzabJymAw8zibueah", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 148, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "4FfbqSXqUdkfUDb6KjiYNE", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 149, "prompt": "Where is the liver?\n<image>", "pred_response": "Left", "answer_id": "coANk4TWmRr7vAVHqJyaqa", "gt_response": "Left", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 150, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "6S73BGFJPVQYu3XADoo29Y", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 151, "prompt": "Is the liver healthy?\n<image>", "pred_response": "No", "answer_id": "CmMQWmqQRmQCQS6wfcuMez", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 152, "prompt": "Which is bigger in this image, kidney or liver?\n<image>", "pred_response": "Liver", "answer_id": "9npxnoeWKgZRFdg99HtooK", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 153, "prompt": "What is the organ located in the lower right corner of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "ScVpmn8pkN2LFfHRRMVx6C", "gt_response": "Left Kidney", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 154, "prompt": "What is the organ on the left side of this image?\n<image>", "pred_response": "Liver", "answer_id": "NrtiWRBaGGM4pPPzePZeiV", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 155, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "ai6a9KmMe5q34Ei77dCfsQ", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 156, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "emLFhdDz76UFsPGgWsf5rx", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 157, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "54c4hEJgNRX69XBGevjJ3A", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 158, "prompt": "Does this image look normal?\n<image>", "pred_response": "Yes", "answer_id": "MgKhXRRYvakyar5sDTA9Yu", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 159, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "2", "answer_id": "DkZjiD2T3RKgupBGcbFf3E", "gt_response": "4", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 160, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "fsDSKsperHyY9N9TbJz9Vp", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 161, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "cb5sKWPq7GVvxvzdnhccxe", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 162, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "ciGFq6x8xDy99yjUTVyQtt", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 163, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "cZEbhfNai5P9g3bs5unzc7", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 164, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "oX8kjfvjvpJDWw2atrdWkV", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 165, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "G4qdnLGFCXu4uCSnSHJAPd", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 166, "prompt": "Where is the liver?\n<image>", "pred_response": "Left", "answer_id": "nkwCd5VZkNxdm6MJMwERUq", "gt_response": "Top", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 167, "prompt": "Is the liver healthy?\n<image>", "pred_response": "Yes", "answer_id": "F5jjXqiX2tsLBaD2GZxo7U", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 168, "prompt": "Which is the biggest in this image, lung,heart or liver?\n<image>", "pred_response": "Lung", "answer_id": "V4UXRt7NMBk2HcvnMsJNKw", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 169, "prompt": "What is the rightmost organ in this image?\n<image>", "pred_response": "Lung", "answer_id": "BpPokuDubGnFQYTfPDQJry", "gt_response": "Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 170, "prompt": "What is the organ on the left side of this image?\n<image>", "pred_response": "Lung", "answer_id": "m3NyYGLX9SCQWXkPi23Wm2", "gt_response": "Right Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 171, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "3oucF4bqwXLgBnDBBesLXL", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 172, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "YQWvmYHWpa2D43gcgkpbYe", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 173, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "CY7KUAyunupMsBrrcZRsLj", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 174, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "XdL4VdFuUWXABYf5U3kmoB", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 175, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "R3Zcr7SMriT5MyWp2L2Le2", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 176, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "WUw5SoiDcrWs7YSeMyXKCj", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 177, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "hkfmVueMFE6jFnfjHqQBTi", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 178, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "CgNKT2zuoeH2FFsEB6i5yE", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 179, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "GDwyFDsjeHpb7eoy3Pzvhk", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 180, "prompt": "Where is the liver?\n<image>", "pred_response": "Left", "answer_id": "BAcpB9in2GEDNSdS5qqvbN", "gt_response": "Left", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 181, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "0", "answer_id": "2xLRGKPTSfd7FEsiFDzgVy", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 182, "prompt": "Which is the biggest in this image, spleen,lung, or liver?\n<image>", "pred_response": "Liver", "answer_id": "bWsXX3mdvSaDjPAzaFY8Au", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 183, "prompt": "What is the organ on the left side of this image?\n<image>", "pred_response": "Liver", "answer_id": "5QFt5zkzXhPqZ9w9CDNC4L", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 184, "prompt": "What is the organ on the lower right corner of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "eAjjz46djxbam7FN7jss6q", "gt_response": "Spleen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 185, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "ja34qXqQYAJxgqpdeLMLAW", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 186, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "gt8cPsnrhiJtwbQ6JzS569", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 187, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "n7NLjMefP8GM7K8PnQ3z5e", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 188, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "c5sUzw7Xb92MeKWwtFZcoy", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 189, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "aQ8WdYmC55923vj9oc2pLe", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 190, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "fmS82RWj4oGs2QYDJ9s7X7", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 191, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "BDrZoxQXAVg7Ueomv8ws49", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 192, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Liver Cancer", "answer_id": "iVe8gYrM85tQy2hL96MQ86", "gt_response": "Liver Cancer", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 193, "prompt": "Where is the liver?\n<image>", "pred_response": "Left", "answer_id": "42jzVc32wNFV4xwJCXLxxo", "gt_response": "Left and top", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 194, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Liver", "answer_id": "JXF7RDSXSG5yuwYHizYGL4", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 195, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "0", "answer_id": "eyDBReGXxdALhJccZn24n9", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 196, "prompt": "Which is bigger in this image, kidney or spleen?\n<image>", "pred_response": "Spleen", "answer_id": "TApPqwYVBngSgCxENupNDA", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 197, "prompt": "What is the organ on the left side of this image?\n<image>", "pred_response": "Liver", "answer_id": "TNQu84sTezPAQfbmp4u3qm", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 198, "prompt": "Does this image look normal?\n<image>", "pred_response": "No", "answer_id": "keLGCihmuLuZ5eRGVQWiAi", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 199, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "NHSGc6Zyz8kXF4sWkTjrEk", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 200, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "fwYHL6KKjua5oVNvK3ctC4", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 201, "prompt": "What is the rightmost organ in this image?\n<image>", "pred_response": "Liver", "answer_id": "JK4s5eNvsED3Z5VVKpA9YZ", "gt_response": "Spleen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 202, "prompt": "What is the organ on the top of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "oCeQ5Wf6xJewPSxn6NMLLk", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 203, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "aRzBgvRCjB4Ygkte9NBCny", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 204, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "EarbbxY9BABL8LpEENsS5k", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 205, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "nPfWwuWuZsQtmGE6sTTWSS", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 206, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "3PNhNRtde5hfVCPfi43z5m", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 207, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "cSjhoNHGC7y6vpsTyYYzH7", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 208, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "EGqo454qT9TD9bRd2ph7v9", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 209, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "ZSzTHZZ4feaVAv4cEKvxbY", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 210, "prompt": "Where is the liver?\n<image>", "pred_response": "Left", "answer_id": "ZBJ47X6TSGR5UF2wqzufkZ", "gt_response": "Left and top", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 211, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Liver", "answer_id": "UBmA4jX8XU5Mo3WUmYWptE", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 212, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "0", "answer_id": "h2sj3dxoGbR8NRJ2iUfzLb", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 213, "prompt": "Does this image look normal?\n<image>", "pred_response": "No", "answer_id": "9PnRjo68TuQ3PGxFbsqRCr", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 214, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "GnjZHnJvrcokLmAMygFi2V", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 215, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "5dkwjcKZGg6uR8caoTZtih", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 216, "prompt": "Which is bigger in this image, liver or spleen?\n<image>", "pred_response": "Liver", "answer_id": "Fs4L89XcvY5wvtxxJag6ec", "gt_response": "Spleen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 217, "prompt": "What is the organ on the lower right corner of this image?\n<image>", "pred_response": "Liver", "answer_id": "AfZPAuPFJsvS9NXU4XVxFA", "gt_response": "Spleen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 218, "prompt": "What is the organ on the top of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "f6y7DsbKEQBqViZQkBwB8T", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 219, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "SrJHVYm6fPXXkJVGEiC3VT", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 220, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "cHgxxr3HazyUTqKM5NdZuH", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 221, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "ERdqeqGC3CqGTTVf3x7ux3", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 222, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "0", "answer_id": "mGZacYNfYhoKM3vNNScsb7", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 223, "prompt": "How many kidneys are there in this image?\n<image>", "pred_response": "0", "answer_id": "fXTfnLYxHh3eCfxuKUDaXo", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 224, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "CAeudGaC63LQDED6qYGSVj", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 225, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "8jpBez3qrcA8C5tXdqGLw6", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 226, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "Jbwvnc2cDqvYf7Bk5vMdUE", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 227, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "icuKVF4KtPhiLUpjV48ELK", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 228, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "ebKjwBCihMqVx328VTB39z", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 229, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "0", "answer_id": "Vif28H62tWL2rMBhrbmW3o", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 230, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "Wv6mVuhBBjjeeZqpXv3QNG", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 231, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "7unHkqBEjvSAHo4Nm9cvvM", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 232, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "kvboEmJpgb4pw7Sjb8vSpH", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 233, "prompt": "Where is the liver?\n<image>", "pred_response": "Left", "answer_id": "W7f6q8YeGcd7sZLehUyZDp", "gt_response": "Not seen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 234, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "0", "answer_id": "ih2TGtdijRCWtMoucY2iKu", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 235, "prompt": "What is the organ on the left side of this image?\n<image>", "pred_response": "Heart", "answer_id": "FZyodBZtGbwGJAFaXfrzcB", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 236, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "C8wj3sxCEGeWKDpEc3QQVt", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 237, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "aRrC6DuBkNQEtXQeG9yYV5", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 238, "prompt": "Does this image look normal?\n<image>", "pred_response": "Yes", "answer_id": "Uxt6NmaFrAmQX4EMyYWA6v", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 239, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "3huLfznwEmfdNkwqGpGuT7", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 240, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "NtytntTBE8tqCTg8df9itA", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 241, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "hcGazc76whZ9ixsrFWVbfP", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 242, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "FP4pYYN6aRF3mxHMMRe4Ha", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 243, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "33gHwUaWLTjP44zmcAEG3V", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 244, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "AgxGJ8abwbtToxjBVqhedE", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 245, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Liver Cancer", "answer_id": "b4P39GSAUvgyNJJ49rVyku", "gt_response": "Liver Cancer", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 246, "prompt": "Where is the liver?\n<image>", "pred_response": "Left", "answer_id": "HhbfvNUJie9ozk9CReEsPx", "gt_response": "Upper Left", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 247, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Liver", "answer_id": "ifsPexcPfdjotMeq7mxwWx", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 248, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "0", "answer_id": "ZAC2vaggZ5KhCPZ8yt95Aa", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 249, "prompt": "Which is bigger in this image, liver or heart?\n<image>", "pred_response": "Heart", "answer_id": "4HkEHsFjruWu7HKoyK52tj", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 250, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "9vCH64VmJWHnHrmAX7BcFg", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 251, "prompt": "What organ is the black part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "CvUKSBeEQEZqUn4FdV9apf", "gt_response": "Right Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 252, "prompt": "What is the organ on the top of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "QgHJ5iHy6xyE5eYQ3EBoax", "gt_response": "Heart and Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 253, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "LYaWFbQfjQaf5qfTb6cMPk", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 254, "prompt": "What are the white points in the black part of the body?\n<image>", "pred_response": "Lung", "answer_id": "N5hr4ddnpdSWPUXrV5JuYL", "gt_response": "Pulmonary bronchus", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 255, "prompt": "Which is the kidney?\n<image>", "pred_response": "Left", "answer_id": "Kq8fWSVZrXgGvnan3UhFki", "gt_response": "Symmetrical to the bone marrow", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 256, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "eEBEwhxrpsw5kxvHRbkmJC", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 257, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "4rHr7kS6UARybDB6DJk5Tv", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 258, "prompt": "Does this image look normal?\n<image>", "pred_response": "No", "answer_id": "hhEEz38c4zQKRqUy2QmCGJ", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 259, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "JpHEbgbMQDExMC9WikJL4P", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 260, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "BPrhrxd7B5uBbfvcYAarob", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 261, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "AzVgL23Rezk2KEzzrjVTMG", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 262, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "KgPhXBHufZ9pqAgQkJpPTs", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 263, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "LXYaRYkBaaco952odTqA9P", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 264, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "NYGQTLKE3V3TCmKvwkw4Hs", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 265, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Liver", "answer_id": "Foe3zeuoTS9auVnUUuUbhn", "gt_response": "Liver Cancer", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 266, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Liver", "answer_id": "7gAhmBeQqZKqJVESQcYyqV", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 267, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "59fSBNE72E6gW7geB9pv8V", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 268, "prompt": "Which is bigger in this image, kidney or spleen?\n<image>", "pred_response": "Spleen", "answer_id": "FMBBnkFSwNXZsKvw2kim3i", "gt_response": "Spleen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 269, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "6w2n9fYkVoG9kueYSTWDmn", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 270, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "JNRYA4pstzxNzWEhk8fALa", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 271, "prompt": "What is the shape of the kidney in the picture?\n<image>", "pred_response": "Oval", "answer_id": "6m9zWsq5KWUZq2H4agrH4T", "gt_response": "Irregular", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 272, "prompt": "Which is the kidney?\n<image>", "pred_response": "Left", "answer_id": "YjceE5UVWeymLHcjt8fuBf", "gt_response": "Symmetrical to the bone marrow", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 273, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "M9bPYTMEhFRvUz2X9q9aet", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 274, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "ELHFY758EpTAqzWWWFqYmv", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 275, "prompt": "Does this image look normal?\n<image>", "pred_response": "Yes", "answer_id": "LE7gkTbi3EyPkYudw3bLqV", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 276, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "6VggxrmwDdWuY7Rn7rQvQP", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 277, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "JfxdAkoVhRJfmpL9Wz4Nsb", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 278, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "YfmckWKXTSypJYWxmVnGAZ", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 279, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "GSWjvbcXTyvz2ywMYSszCF", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 280, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "Cn7uFCvWNf6qopbgsyEDkA", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 281, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "JzjNeVfEC3ePL9yLsmfhth", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 282, "prompt": "Where is the liver?\n<image>", "pred_response": "Left", "answer_id": "2WUnrFfrrEqu4i9UHavJay", "gt_response": "Left", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 283, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "8Tjv8yc4awoDDfGrWtZAUe", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 284, "prompt": "What organ is the black part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "jRFc4D2PKdZ5XizPRFeEMA", "gt_response": "Right Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 285, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "AMPwESyvKxELH2BaTxeL9F", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 286, "prompt": "What are the white spots in the black part of the lower part of body?\n<image>", "pred_response": "Liver", "answer_id": "MTn48naWr7t5Q9WREQbu3h", "gt_response": "Pulmonary bronchus", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 287, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "GdpQaSBwdjKhvmNX4ce9Mg", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 288, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "Df7kQmGUJ86oLYArJB7uNY", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 289, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "2kTFaz7MbxCykzKCoWub4W", "gt_response": "Small Bowel", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 290, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "jVufzcB3Pn9tGBQ7VBzgPT", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 291, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "Yes", "answer_id": "enFkjPiLMcLoQEGuoufARg", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 292, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "argTy7DfRkn6vrK4JYLpF8", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 293, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "MX8tft4qzVe8F3jRYoYXe9", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 294, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "PiqBQarmPGpzwP7K83A6pG", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 295, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "5LEomLjTJN9rV36RcDoDL4", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 296, "prompt": "What is the rightmost organ in this image?\n<image>", "pred_response": "Liver", "answer_id": "JujuYqoCAAJsSWu7wk24Kv", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 297, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "mTNzcNhSEhjKYkqhLYJcdH", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 298, "prompt": "What is the organ on the top of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "jhzgkCpBbUtZuDrPTY9D3J", "gt_response": "Small Bowel", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 299, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "avka56qjf4TuudfAaJJ69b", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 300, "prompt": "Which is the kidney?\n<image>", "pred_response": "Left", "answer_id": "UsYtWfcHQuUHJzqPnqSa55", "gt_response": "Symmetrical to the bone marrow", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 301, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "QMdTWTVMZBeFBFbmGj6eHF", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 302, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "StBtVS5KJpG9f83Axo2n47", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 303, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "WkcMjteCGkTyzHa2Ciw9qM", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 304, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "GsD8hgVCxeDeHtv5nMeqni", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 305, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "i2Jg7ZkFZdVSTJdaZwi4ev", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 306, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "TDu2arDwST5rAv2AdkwCh7", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 307, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "kbMjNopg9VWmdvY9X8kDFp", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 308, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "NVukWGpt2gmcXRYGC6JRiA", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 309, "prompt": "What is the rightmost organ in this image?\n<image>", "pred_response": "Liver", "answer_id": "QkTXKXSYbE93HPxqQhn8Mp", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 310, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "5QzWEVrQYBfjpqFo2oTxVY", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 311, "prompt": "What organ is the black part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "oSU5BA9DNcFAw6gAZXiYT2", "gt_response": "Right Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 312, "prompt": "What is the organ on the top of the body in this image?\n<image>", "pred_response": "Liver", "answer_id": "FphFYVbd6EK9PXEkcXWjyQ", "gt_response": "Stomach", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 313, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "JMSBeSwKHAJNgpoFgg6bAz", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 314, "prompt": "What organ is the black hole in the upper right corner?\n<image>", "pred_response": "Liver", "answer_id": "joTesxf2xwksKcYzL9T8tC", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 315, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "MzTLeLijqDp9aGS4Qh7cz4", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 316, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "jFhqzsZkVwupVmntvUgkRx", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 317, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "MkWMKx2WwdmCHRQXiJqV73", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 318, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "54cF89W4GtRuPFxCPnbDtd", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 319, "prompt": "Does the picture contain kidney?\n<image>", "pred_response": "No", "answer_id": "eqKbb8E3LW5DqeMP7Tv8RW", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 320, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "Yes", "answer_id": "Mg9UpTnpqrNB4mthLXy5v6", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 321, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "krPdz9R84MkeKTzQi6mVCX", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 322, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "P8ajb8ufch4K58cnbmGbfy", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 323, "prompt": "How many kidneys in this image?\n<image>", "pred_response": "2", "answer_id": "mAHLiNVHmjJzvB8o73QjjL", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 324, "prompt": "Which is bigger in this image, liver or spleen?\n<image>", "pred_response": "Liver", "answer_id": "JHoemRpkC8hFhfVg44z4m8", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 325, "prompt": "What is the rightmost organ in this image?\n<image>", "pred_response": "Liver", "answer_id": "mQuB5jUSfLEoCa5AmKqnfR", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 326, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "HYjv52hSJ2ArKa5tkY2vfr", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 327, "prompt": "What organ is the black part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "DJZNEFc6zMFcDgvYk9ED8e", "gt_response": "Right Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 328, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "Fc8cmLbzWeFB5ZUdSpwjCi", "gt_response": "Transverse  Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 329, "prompt": "What organ is the black hole in the upper right corner?\n<image>", "pred_response": "Liver", "answer_id": "hdmcxEEV6gvQ2YQg333owb", "gt_response": "Large Bowel", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 330, "prompt": "What type of medical image is this?\n<image>", "pred_response": "CT", "answer_id": "iREg4ocDaajozcQMoTUzYZ", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 331, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "MtddH4P2R3vegUUAWNjMDb", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 332, "prompt": "Which part do the organs in this image locate from the aspect of the whole body?\n<image>", "pred_response": "Left", "answer_id": "5igckThhGjZ8eGWmYHMJfG", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 333, "prompt": "Which side of lung is abnormal in this image,left or right?\n<image>", "pred_response": "Left", "answer_id": "Svbdra9HXdvzbcmYZe3bKX", "gt_response": "Right", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 334, "prompt": "What part of the lung is the pneumothorax located in?\n<image>", "pred_response": "Right", "answer_id": "9haqRg9bh34bqutc99y7uF", "gt_response": "Upper Right Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 335, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Respiratory", "answer_id": "K7Y7vivHAzB9SpYdg2k9vb", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 336, "prompt": "Which organs appear in pairs?\n<image>", "pred_response": "Lung", "answer_id": "AfNr8h6bntMYQk2LXZq2AB", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 337, "prompt": "Is/Are there abnormalities in the patient's right lung?\n<image>", "pred_response": "Yes", "answer_id": "QRzCyW7nPKQMfwgXTcZk3L", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 338, "prompt": "Is this image normal?\n<image>", "pred_response": "Yes", "answer_id": "V4QGp8HThfnJNqgMw6EAZj", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 339, "prompt": "Can pneumothorax be observed on the lower left lung in this picture?\n<image>", "pred_response": "No", "answer_id": "3XNLZmBrFAtGXXhSQm8PBA", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 340, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "Gray", "answer_id": "GVW4DC4LpBu9xhx7zLJeBF", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 341, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "jCNVthDW8FP62bL6oHvoZ8", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 342, "prompt": "What is the effect of the main organ in this picture?\n<image>", "pred_response": "The main organ in the picture, the heart, is responsible for pumping blood throughout the body.", "answer_id": "Y6PPf3tVt8RRmoecSsqUKz", "gt_response": "Breathe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 343, "prompt": "What kind of symptoms will the disease on the upper right of lung bring to the patient?\n<image>", "pred_response": "The disease on the upper right of the lung, which is a large mass, can cause various symptoms to the patient. These may include shortness of breath, coughing, chest pain, and fatigue. The presence of the mass can also lead to a reduced lung capacity, making it difficult for the patient to breathe and get adequate oxygen. In some cases, the mass may be cancerous, which can further exacerbate the symptoms and require more aggressive treatment. It is essential to consult a medical professional to determine the cause of the mass and receive appropriate treatment.", "answer_id": "TySq7yAm5koSejv4qdejLX", "gt_response": "Chest pain, dyspnea", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 344, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "hdVrAjG4EgF2MDzbXetyJ2", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 345, "prompt": "What imaging modality was used?\n<image>", "pred_response": "CT", "answer_id": "eWJkQgLGjV86jUzBW2VgTR", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 346, "prompt": "Is this a transverse plane?\n<image>", "pred_response": "Yes", "answer_id": "EvnEQ8nF86h2jbTKNC988P", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 347, "prompt": "Which part of the human body is the organ located in the image?\n<image>", "pred_response": "Chest", "answer_id": "GFRqChkMRw3GjNWSE2CGaa", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 348, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Left Lung", "answer_id": "GaEGanFaC9cdxZFGwJMdWx", "gt_response": "Lower Right Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 349, "prompt": "Where is the atelectasis in this image?\n<image>", "pred_response": "Left Lung", "answer_id": "Z9yTKkXTCSWBE7xJg4zq9T", "gt_response": "Lower Right Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 350, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Respiratory", "answer_id": "ar58wJT4KvZ4rJ64jwvABd", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 351, "prompt": "Is this a study of the chest?\n<image>", "pred_response": "Yes", "answer_id": "cu6SdE68hTrnHMnsmzuv7A", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 352, "prompt": "What disease is/are shown on the right of lung?\n<image>", "pred_response": "Cancer", "answer_id": "h3LRg2E3MtG7RzapeETv8B", "gt_response": "Atelectasis", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 353, "prompt": "Is/Are there abnormalities in the patient's right lung?\n<image>", "pred_response": "Yes", "answer_id": "joqZraQ5QoFDF4JmwiLSkn", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 354, "prompt": "Can atelectasis be observed on the upper right lung in this picture?\n<image>", "pred_response": "Yes, atelectasis can be observed on the upper right lung in the CT scan image.", "answer_id": "AjUaCzqZ6J5otYznzm58wD", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 355, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "Gray", "answer_id": "T5momfSSDjgxoABwRrdNJw", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 356, "prompt": "How many existing lungs in this image?\n<image>", "pred_response": "2", "answer_id": "gTAMNy4R4X6jwXznPHKE9T", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 357, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "The main organ in the picture is the heart, which is responsible for pumping blood throughout the body.", "answer_id": "QJMnKySobkf8xkKRSAPzEh", "gt_response": "Breathe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 358, "prompt": "What is the main cause of the disease on the lower right of the lung in the picture?\n<image>", "pred_response": "Cancer", "answer_id": "N4VWBmCqT6S3vzu5ntWb8b", "gt_response": "Bronchial obstruction", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 359, "prompt": "Which organs/organ in the picture belong to the circulatory system?\n<image>", "pred_response": "Heart", "answer_id": "gzrqAfZuaVsu33vSYso2DS", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 360, "prompt": "Is this image taken via X-Ray?\n<image>", "pred_response": "Yes", "answer_id": "bNVwpRXC8aZYEX98CsJA9q", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 361, "prompt": "Is this a transverse plane?\n<image>", "pred_response": "Yes", "answer_id": "7G3nDdAGiyGhRMgrMVAWMJ", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 362, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "K5ZJ5Q9MfUiZ78dEN6MwYY", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 363, "prompt": "Which side of lung is abnormal in this image,left or right?\n<image>", "pred_response": "Left", "answer_id": "j6C6ifbEupyzjYqFAY4np4", "gt_response": "Left", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 364, "prompt": "Where is the mass in this image?\n<image>", "pred_response": "Chest", "answer_id": "c8ds33K4qXA6Hmy59rpyMs", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 365, "prompt": "Is this a study of the chest?\n<image>", "pred_response": "Yes", "answer_id": "NxmTSfVmFinuESaEh8W3bR", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 366, "prompt": "Is this a study of the pelvic cavity?\n<image>", "pred_response": "No", "answer_id": "JAPYWWPBPLTjy3GXihe7WP", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 367, "prompt": "What disease is/are shown on the left of lung?\n<image>", "pred_response": "Cancer", "answer_id": "UqMj6xHiLENp8rjy8wCXQh", "gt_response": "Pulmonary Mass", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 368, "prompt": "Is the lung abnormal?\n<image>", "pred_response": "No", "answer_id": "Wdq4KupqCfxA8CymH53yVy", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 369, "prompt": "Can mass be observed on the lower left lung in this picture?\n<image>", "pred_response": "Yes", "answer_id": "dhJAGrQNbmY3rfgbrY8Ehw", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 370, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "Gray", "answer_id": "WSexosnxiJgeMt3M4xRWX8", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 371, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "1", "answer_id": "iRFKHGV7WuhxiKboQT2aPQ", "gt_response": "1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 372, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "Taking an image of the body, such as the one displayed in the image, can cause some damage to the body. The image is a CT scan, which involves the use of ionizing radiation to create detailed images of the body's internal structures. While the radiation dose is typically low, it can still cause some damage to the body over time, such as an increased risk of cancer or other health issues. However, the benefits of the CT scan, such as early detection and diagnosis of diseases, often outweigh the risks associated with the radiation exposure.", "answer_id": "3fDCL2xovfvUFZvQmTQr4R", "gt_response": "A Little", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 373, "prompt": "How to prevent the occurrence of the disease on the lower left lobe of lung in this image?\n<image>", "pred_response": "To prevent the occurrence of the disease on the lower left lobe of the lung, the patient should follow a healthy lifestyle and maintain good overall health. This includes regular exercise, maintaining a balanced diet, avoiding smoking, and minimizing exposure to air pollution. Additionally, the patient should follow the advice of their healthcare provider, such as receiving regular check-ups, vaccinations, and adhering to any prescribed medications or treatments. It is also essential to practice good hygiene, such as washing hands frequently and covering the mouth and nose when sne", "answer_id": "NoscNAxDjHrcPKFCh23sTB", "gt_response": "Keep healthy", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 374, "prompt": "Which organs/organ in the picture belong to the circulatory system?\n<image>", "pred_response": "Heart", "answer_id": "fSeZUTNYq2en6VaxoKhAqJ", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 375, "prompt": "What type of medical image is this?\n<image>", "pred_response": "CT", "answer_id": "HGiMUSaJ6m97jjkvN9hXZq", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 376, "prompt": "Is this a coronal section?\n<image>", "pred_response": "Yes", "answer_id": "UmTNrqrQ82bwat7EwJFjgx", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 377, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "TarCps3KwcNmSWPzVSipXL", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 378, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Left", "answer_id": "ciSYsEx7esLe6c5BU6rcBz", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 379, "prompt": "Where is the atelectasis in this image?\n<image>", "pred_response": "Left Lung", "answer_id": "MKbHhH9WEV2QNMgpz3HT2y", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 380, "prompt": "Is this a study of the neck?\n<image>", "pred_response": "No", "answer_id": "in9NYTNnPWVzDNebRqW9XU", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 381, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "aJQGfqLFDA2joNmQK4kKNi", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 382, "prompt": "Does the patient have any abnormalities?\n<image>", "pred_response": "Yes", "answer_id": "NCTf2qNZDAf4MRoykqTuiN", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 383, "prompt": "Is this image normal?\n<image>", "pred_response": "Yes", "answer_id": "HcAujhbKDFS4s23zLebZ2f", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 384, "prompt": "Can atelectasis be observed on the lower left lung in this picture?\n<image>", "pred_response": "No", "answer_id": "9ksxEhaoeBreQVNHTiSoQ9", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 385, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "fxtKKDnQCxPpTDJGgoZy89", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 386, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "cDnXyKaFpsfinEx7VA3Dx7", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 387, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "Taking an image of the body, particularly the chest, can cause some damage to the body. The image is taken using a CT scan, which involves the use of ionizing radiation to create detailed images of the body's internal structures. While the radiation dose is typically low, it can still cause some damage to the body over time, such as an increased risk of cancer or other health issues. It is essential to follow proper safety measures, such as using the lowest radiation dose necessary and adhering to the recommended guidelines for medical imaging, to minimize the risk of harm.", "answer_id": "jCq3aqcFWuk5A6QjQjncFP", "gt_response": "A Little", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 388, "prompt": "How to treat the disease located on the left of lung in this image?\n<image>", "pred_response": "The disease located on the left side of the lung in the image is likely a pneumonia, which is an infection of the lungs. The treatment for pneumonia typically involves antibiotics, such as penicillin, to help fight the infection and alleviate symptoms. In addition to antibiotics, patients may also receive supportive care, such as oxygen therapy, to help improve breathing and prevent complications. It is important to consult a healthcare professional for proper diagnosis and treatment, as pneumonia can be a serious condition if left untreated.", "answer_id": "HyBsdpsukVHX596Y5gFS3j", "gt_response": "Physical therapy, surgical treatment", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 389, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "niA6Qe7wrNM2DTYxdCiBMH", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 390, "prompt": "Is this image taken via X-Ray?\n<image>", "pred_response": "Yes", "answer_id": "3XdAkE6cHSCCyxqxzgpAap", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 391, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "S3hALmjfSwk9fKFGFvnMsQ", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 392, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "nDgtirszx9YvLue9J28SsC", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 393, "prompt": "Which side of lung is abnormal in this image,left or right?\n<image>", "pred_response": "Left", "answer_id": "nFhFbXocgDAgkYLTeGFTZb", "gt_response": "Right", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 394, "prompt": "Where is the pneumonia in the lung?\n<image>", "pred_response": "Left", "answer_id": "DxBf26mypSiVTZSJJUUiQF", "gt_response": "Right Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 395, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Respiratory", "answer_id": "T43oQJETMtJf6B3PNu8Pn4", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 396, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Respiratory", "answer_id": "eJRGUSvqyxCc2KJNirnLpL", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 397, "prompt": "What disease is/are shown on the right of lung?\n<image>", "pred_response": "Cancer", "answer_id": "LhR97JNeaFhdepR9BajfpU", "gt_response": "Pneumonia", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 398, "prompt": "Does the lung look abnormal?\n<image>", "pred_response": "No", "answer_id": "58XxnkaEXB8mPHRNiTwSmB", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 399, "prompt": "Can pneumonia be observed on the right lung in this picture?\n<image>", "pred_response": "Yes, the image shows the right lung of a person with pneumonia, which is a medical condition characterized by inflammation and infection of the lung tissue.", "answer_id": "J2QNgcmByFhPMZY5VFYADk", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 400, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "g4wk4rvbcCvqJcV7ZDYHdy", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 401, "prompt": "How many existing lungs in this image?\n<image>", "pred_response": "2", "answer_id": "7ewgx4JxgLoVbEBxqdxpWj", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 402, "prompt": "What can the organ with black color in this image be used for?\n<image>", "pred_response": "The organ with black color in the image is the heart, which is responsible for pumping blood throughout the body.", "answer_id": "QTwTHYictcAc5dSSdEsCZm", "gt_response": "Breathe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 403, "prompt": "How to treat the disease located on the right of lung in this image?\n<image>", "pred_response": "Lung Cancer", "answer_id": "VHZ2cjR7UFUt4QnmSdW2RQ", "gt_response": "Medical therapy, supportive therapy", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 404, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Heart", "answer_id": "czkdq2QQYzD5gwuiSjwH52", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 405, "prompt": "What modality is shown in this image?\n<image>", "pred_response": "CT", "answer_id": "awDbmaoSzeiwyjizV4CYLW", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 406, "prompt": "Which plane is the image scanned, transverse plane or coronal plane?\n<image>", "pred_response": "Transverse plane", "answer_id": "72zYHovkSgh6GQk3GXBydi", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 407, "prompt": "Do the organs in the image exist in the abdomen?\n<image>", "pred_response": "Yes", "answer_id": "3tEaxVGXgpQ3FEjzyDDmwL", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 408, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Left", "answer_id": "beD5UJP9B2PvhS4vVTMKZY", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 409, "prompt": "Where is the cardiomegaly in this image?\n<image>", "pred_response": "Left", "answer_id": "NXuyicybYhDrXcHCRoBkXX", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 410, "prompt": "Which organs appear in pairs?\n<image>", "pred_response": "Lung", "answer_id": "aqpsFauSSCKeYJ5zqbfYEL", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 411, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Respiratory", "answer_id": "WwPP5bDY3yK42w22F5xrtq", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 412, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Heart", "answer_id": "DkzvG4SbLRy8bNSfsarCs3", "gt_response": "Cardiomegaly", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 413, "prompt": "Is this image normal?\n<image>", "pred_response": "Yes", "answer_id": "3rFgyyks8zmzjugVfYUbPF", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 414, "prompt": "Can cardiomegaly be observed on the heart in this picture?\n<image>", "pred_response": "Yes", "answer_id": "BQTwXnbZRJfTL7LGU4hHHG", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 415, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "Gray", "answer_id": "XnQLhCEB2iLvvt6SnkLxxW", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 416, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "1", "answer_id": "9UX33CAomXPEJwwBuZd6ii", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 417, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "The main organ in this picture is the heart, which is responsible for pumping blood throughout the body.", "answer_id": "mFJASpLN8vHJWPGWR7XuXt", "gt_response": "Breathe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 418, "prompt": "What kind of symptoms will the disease on the heart in this image bring to the patient?\n<image>", "pred_response": "The disease on the heart, which appears to be a heart attack, can cause various symptoms to the patient. These symptoms may include chest pain, shortness of breath, rapid heartbeat, and fatigue. In the image, the heart is shown to be enlarged, which is a common sign of heart disease. The presence of the heart attack can also lead to other complications, such as cardiac arrhythmias, heart failure, and even death if not treated promptly and effectively. It is essential to seek medical attention and follow the recommended treatment plan to manage the disease and prevent further comp", "answer_id": "i53osmaW4pwbDEMSEuedDA", "gt_response": "Chest tightness, arrhythmia", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 419, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Heart", "answer_id": "hujktc8JUjtxqxbNteh95L", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 420, "prompt": "What type of medical image is this?\n<image>", "pred_response": "CT", "answer_id": "UnqUrtQoipMgpSc2x6G4bX", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 421, "prompt": "Is this sagittal plane?\n<image>", "pred_response": "Yes", "answer_id": "igUDaHwC8Ddkvdrmjc5gGv", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 422, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "Bok9XaMzZBGncMahV8ZU85", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 423, "prompt": "Which lobe is abnormal in this image?\n<image>", "pred_response": "Left", "answer_id": "QXsbgVez9vkkrLeoBU7Mgc", "gt_response": "Left", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 424, "prompt": "Where is the pulmonary infiltration in this image?\n<image>", "pred_response": "Left", "answer_id": "fpUNYip7hn6PqxfovZYZ5o", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 425, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Respiratory", "answer_id": "4jyPTuibz4sxyKmjZKSrBw", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 426, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Respiratory", "answer_id": "EiaQzdDYdFqLmzxuqbekbL", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 427, "prompt": "Does the lung look abnormal?\n<image>", "pred_response": "No", "answer_id": "PKo4VmyfCHKb5UNyfNYm8S", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 428, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Heart", "answer_id": "csyqMs3rSdbeduhE4hQAJN", "gt_response": "Pulmonary Infiltration", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 429, "prompt": "Can pulmonary infiltration be observed on the upper right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "Tad2qHc9iH7QEMJ2Jqr58a", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 430, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "Gray", "answer_id": "YKqNb7j28LNMmuK9BA2oKK", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 431, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "1", "answer_id": "A3bCEDPnjGapjkxu3wY8QK", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 432, "prompt": "What is the effect of the main organ in this picture?\n<image>", "pred_response": "The main organ in the picture is the heart, which is responsible for pumping blood throughout the body.", "answer_id": "2BquDY2EEEjsC6WgFKqkXf", "gt_response": "Breathe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 433, "prompt": "How to prevent the occurrence of the disease on the lower left lobe of lung in this image?\n<image>", "pred_response": "To prevent the occurrence of the disease on the lower left lobe of the lung, the patient should follow a healthy lifestyle, including regular exercise, maintaining a balanced diet, and avoiding smoking. Additionally, the patient should be regularly screened for early detection of any signs of the disease, and if detected, seek medical attention promptly. In some cases, medications or surgical interventions may be necessary to manage the disease and prevent further complications.", "answer_id": "W77c3U7Vy7FVFBNAZC3fpt", "gt_response": "Live healthy, enhance physical fitness", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 434, "prompt": "Which organs/organ is part of the respiratory system in this image?\n<image>", "pred_response": "Lung", "answer_id": "VHkeBAA7TwH4ZGerQxBZHN", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 435, "prompt": "What imaging modality was used?\n<image>", "pred_response": "CT", "answer_id": "4nAmnn9BDgFpKDcwJsNXA3", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 436, "prompt": "Which plane is the image scanned, transverse plane or coronal plane?\n<image>", "pred_response": "Transverse plane", "answer_id": "DaF7AbqRQdpQAYWFNYEZoo", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 437, "prompt": "Which part do the organs in this image locate from the aspect of the whole body?\n<image>", "pred_response": "Left", "answer_id": "5jLk3Bmr2KH7fLv4ffW8PE", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 438, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Left side", "answer_id": "AHjAhieeKoXVLrt98CCMgt", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 439, "prompt": "Where is the pulmonary nodule located in this image?\n<image>", "pred_response": "Left", "answer_id": "azGwfzKQaxt7oVy8v56C5m", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 440, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Respiratory", "answer_id": "CYdoLGDRFcLfBY2faeMpjC", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 441, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Respiratory", "answer_id": "2QCpd4G6EHz2fvJtMKeHKU", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 442, "prompt": "Is the lung normal appearing?\n<image>", "pred_response": "Yes", "answer_id": "QWGFi9TB8qiZiN4U7utXYA", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 443, "prompt": "Is this image normal?\n<image>", "pred_response": "Yes", "answer_id": "UkY6hk4fkrdMW9cF4JDSH8", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 444, "prompt": "Can nodule be observed on the lower left lung in this picture?\n<image>", "pred_response": "Yes", "answer_id": "BZ7ovPfXeMZqE975vaH494", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 445, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "ThWEShkWECoNabNZ5XamUu", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 446, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "k8DKTJeXvEt6B8XncvdcFZ", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 447, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "AojvjF7WH46CFqhpNvUC4N", "gt_response": "Breathe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 448, "prompt": "How to treat the disease located on the lower left of lung in this image?\n<image>", "pred_response": "The disease located on the lower left of the lung in the image is likely a case of pneumonia. Pneumonia is an infection of the lungs, typically caused by bacteria, viruses, or fungi. It can be treated with antibiotics, which are prescribed based on the specific bacteria or pathogen causing the infection. In some cases, hospitalization may be necessary, especially if the patient is experiencing severe symptoms or has underlying health conditions that make the infection more severe. It is important to seek medical attention promptly to ensure the best possible outcome and to prevent", "answer_id": "dyYgjNUPxGggcpfhq4rKex", "gt_response": "Medical treatment, surgical treatment", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 449, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Heart", "answer_id": "eMK6XZ2CnbdCy33yYmDT7u", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 450, "prompt": "What imaging modality was used?\n<image>", "pred_response": "CT", "answer_id": "GAaYmb8YpWFNS5iW8mPX6n", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 451, "prompt": "Which plane is the image scanned, transverse plane or coronal plane?\n<image>", "pred_response": "Transverse plane", "answer_id": "gDdhbKUSxgrd5uG29He2z3", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 452, "prompt": "Which part of the human body is the organ located in the image?\n<image>", "pred_response": "Chest", "answer_id": "MWCXja8bQLmUaNfjkmUuVd", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 453, "prompt": "Where is/are the abnormality located in this picture?\n<image>", "pred_response": "Chest", "answer_id": "dNtCF7V8vEKutpfdzcKqPQ", "gt_response": "Upper Right Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 454, "prompt": "Where is the nodule in this image?\n<image>", "pred_response": "Chest", "answer_id": "9kDPJwc3owcD7uu83TEuHv", "gt_response": "Upper Right Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 455, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Respiratory", "answer_id": "Gwf4Rh9rDBZxKS3rjth3LS", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 456, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Respiratory", "answer_id": "NkaGMZwPwGgU2uphpZqaB6", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 457, "prompt": "Does the patient have any abnormalities?\n<image>", "pred_response": "Yes", "answer_id": "3gKkdY4unxoByGmf2znP8n", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 458, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "aTjtwFsAc8U6YMehCGsKQ8", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 459, "prompt": "Can nodule be observed on the upper right lung in this picture?\n<image>", "pred_response": "Yes", "answer_id": "AZXKYyZ52k4drAKkcVN6q6", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 460, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "Gray", "answer_id": "QzuvRby5BuEvabZBqq8BGt", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 461, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "o9u3hYvF2cmF9nsBVsMWH5", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 462, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "The main organ in this picture is the heart, which is responsible for pumping blood throughout the body.", "answer_id": "bfMvMgz93QhaYxhL2dPx5S", "gt_response": "Breathe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 463, "prompt": "What kind of symptoms will the disease on the upper right lung bring to the patient?\n<image>", "pred_response": "The disease on the upper right lung, which appears to be a tumor, can cause various symptoms depending on its size, location, and the extent of the spread. Symptoms may include shortness of breath, coughing, chest pain, and fatigue. In some cases, the tumor may obstruct the airways, leading to difficulty breathing, wheezing, or even respiratory failure. Additionally, the tumor may cause chest pain, especially when the patient takes deep breaths, coughs, or moves. The presence of a tumor in the upper right lung may also", "answer_id": "mE5WqU4rxZV5Zu3kAJMk6o", "gt_response": "Chest tightness, fatigue", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 464, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "ahodrXm52gJ6TDPXWqXv38", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 465, "prompt": "Is this an MRI scan?\n<image>", "pred_response": "No", "answer_id": "PR9dJd9UCPfQEZNFVGA9iN", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 466, "prompt": "Is this a coronal plane?\n<image>", "pred_response": "Yes", "answer_id": "5WwSWsCcGn2kqVuogsbGRx", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 467, "prompt": "Do the organs in the image exist in the chest?\n<image>", "pred_response": "Yes", "answer_id": "8Rb4GBKgswPcjnvcdpvUvJ", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 468, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Left", "answer_id": "MUFQjxuSt5WeqKxCTFbMeX", "gt_response": "Upper Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 469, "prompt": "What part of the lung is the pneumothorax located in?\n<image>", "pred_response": "Right", "answer_id": "8eY3KCpjNQpAMM39QeTBnj", "gt_response": "Upper Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 470, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Respiratory", "answer_id": "2K8APQDQPi5gGfyZhXt5fp", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 471, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Respiratory", "answer_id": "NLCRjpyvS5CHXZt7rjFswk", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 472, "prompt": "Is the lung normal appearing?\n<image>", "pred_response": "Yes", "answer_id": "V9QDeqrkBsGDxmxcgyAaM8", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 473, "prompt": "Is/Are there abnormalities in the patient's left lung?\n<image>", "pred_response": "No", "answer_id": "PheW6KtvPLaCh9uZ6svQm4", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 474, "prompt": "Can pneumothorax be observed on the lower right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "kC2iTFvEQMKEuVXNhn4CkX", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 475, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "Gray", "answer_id": "DMFJvsdMAMQg7fz8SusS7M", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 476, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "1", "answer_id": "7jfhyCM5p5dBDyziY4tbzM", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 477, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "The image shows the inside of the body, specifically the chest, lungs, and heart, which are essential organs. Taking such an image can cause minimal damage to the body, as the image is not invasive and does not involve any physical contact. However, the image may be used for medical purposes, such as diagnosing or monitoring health conditions, and the patient's privacy should be respected.", "answer_id": "JmP47v5LkWJVoMc5Ly2Wo4", "gt_response": "A Little", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 478, "prompt": "How to treat the disease located on the upper left of lung in this image?\n<image>", "pred_response": "The disease located on the upper left of the lung in the image is likely a case of lung cancer. Lung cancer is a malignant growth of cells in the lung tissue, and it can be treated with a combination of surgery, chemotherapy, and radiation therapy. The specific treatment plan will depend on the stage and location of the cancer, as well as the patient's overall health and preferences. Surgery, such as a lobectomy or a segmentectomy, may be necessary to remove the affected portion of the lung, while chemotherapy and radiation therapy can be used to", "answer_id": "2HqxuwN3eXhqyTRQJsMUj5", "gt_response": "Medical therapy, supportive therapy", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 479, "prompt": "Which organs/organ in the picture belong to the circulatory system?\n<image>", "pred_response": "Heart", "answer_id": "Cv9FLPogjRqZ4Q2yzafF7V", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 480, "prompt": "Is this a X-Ray?\n<image>", "pred_response": "Yes", "answer_id": "ECGgp5qZsa2uQTCxyMeisH", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 481, "prompt": "What scanning plane does this image belong to?\n<image>", "pred_response": "Transverse", "answer_id": "KUTVGwgn6mtaWACUvPinCc", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 482, "prompt": "Do the organs in the image exist in the abdomen?\n<image>", "pred_response": "Yes, the organs in the image exist in the abdomen.", "answer_id": "aeu2Sbcyw84no6pNMc7sjY", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 483, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Left", "answer_id": "dFAGu9Pwgb3hbve552Avxo", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 484, "prompt": "Where is the pneumothorax in this image?\n<image>", "pred_response": "Left", "answer_id": "bC9JW6K5GZ8jCqrayEg278", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 485, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Respiratory", "answer_id": "7ABnpQRgG47AviD2Gq9WEU", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 486, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Respiratory", "answer_id": "goCnqhTjbBBbMt6mR2nYt7", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 487, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "WB5mhWVLREa25TqGCoxew2", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 488, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "h3BLadNc7vejMwAcGkytUC", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 489, "prompt": "Can pneumothorax be observed on the lower left lung in this picture?\n<image>", "pred_response": "Yes", "answer_id": "bSuDR6aZni3aeGbmjzHG9V", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 490, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "Gray", "answer_id": "QmXPWkLFFTrw9R4FYKYSaf", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 491, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "1", "answer_id": "3YGa8p2BhsLysLatmVQyX2", "gt_response": "1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 492, "prompt": "What is the effect of the center organ in this picture?\n<image>", "pred_response": "The center organ, the heart, is responsible for pumping blood throughout the body. In the image, the heart is shown to be located in the center of the body, surrounded by the lungs, liver, and other organs. The heart's function is crucial for maintaining the proper circulation of blood, which is essential for the delivery of oxygen and nutrients to the body's cells and the removal of waste products. The heart's position in the center of the body, as seen in the image, allows it to efficiently distribute blood to the various organs and tissues,", "answer_id": "kLWvcx2m5ByVwy4fzLrK5h", "gt_response": "Promote blood flow", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 493, "prompt": "What is the main cause of the disease on the lower left of the lung in the picture?\n<image>", "pred_response": "The main cause of the disease on the lower left of the lung in the picture is a tumor.", "answer_id": "28YXvDfitrFRD3mnRQWURQ", "gt_response": "Chest injury, lung disease, bullae", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 494, "prompt": "Which organs/organ in the picture belong to the circulatory system?\n<image>", "pred_response": "Heart", "answer_id": "Z2ng4f4rd5UJzswQv8WHC6", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 495, "prompt": "Is this image taken via X-Ray?\n<image>", "pred_response": "Yes", "answer_id": "T3SeSLPhSZqFmyarr6vLZi", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 496, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse", "answer_id": "g483GbnusrgU53JZysy9bq", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 497, "prompt": "Which part of the human body is the organ located in the image?\n<image>", "pred_response": "Chest", "answer_id": "dPJBWqR2KzZZeTspCMC4aH", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 498, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Chest", "answer_id": "mEeqiWxQkfMcFoWYThkq2g", "gt_response": "Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 499, "prompt": "Where is/are the nodule located in this image?\n<image>", "pred_response": "Chest", "answer_id": "aVSqMcCeEQZs9ZxyYz7hV5", "gt_response": "Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 500, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Heart", "answer_id": "KpWHZ5Dc53TVgv34FYPCmU", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 501, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Respiratory", "answer_id": "VbHkEUx8es8nNTkQZtnh2W", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 502, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "nVLayCF2gyRzqiWMgTYxmQ", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 503, "prompt": "Is the lung abnormal?\n<image>", "pred_response": "No", "answer_id": "PeNkMewCekBBxXKnLAK7Gs", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 504, "prompt": "Can nodule be observed on the lower right lung in this picture?\n<image>", "pred_response": "Yes", "answer_id": "h5wDktPtK9hndF7xRHKQ8k", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 505, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "Gray", "answer_id": "SHjSDzn4tFCNCpSjCReKRw", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 506, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "1", "answer_id": "ni7ba99j98pNTSj4FGxDtf", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 507, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "Breathe", "answer_id": "N4mG9uuDePgb4etRwuCCWh", "gt_response": "Breathe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 508, "prompt": "How to prevent the occurrence of the disease on the left lobe of lung in this image?\n<image>", "pred_response": "To prevent the occurrence of the disease on the left lobe of the lung, the patient should maintain a healthy lifestyle, including regular exercise, a balanced diet, and avoiding smoking. Additionally, the patient should follow the advice of their healthcare provider, such as taking medications as prescribed, attending regular check-ups, and adhering to any recommended lifestyle changes.", "answer_id": "CytLbtVGh7oRNqcnE9P5Rz", "gt_response": "Quit smoking, enhance physical fitness", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 509, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "FZkJRbpTQUqdMh7PNbVTDi", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 510, "prompt": "Is this image taken via X-Ray?\n<image>", "pred_response": "Yes", "answer_id": "FQ4thVscygvKRsyqUixgtr", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 511, "prompt": "What scanning plane does this image belong to?\n<image>", "pred_response": "CT", "answer_id": "frJTVHGPZneE5c4guBZ4uu", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 512, "prompt": "Where does the picture show?\n<image>", "pred_response": "Chest", "answer_id": "hujKf8TQk4ivsRXDxnudH6", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 513, "prompt": "Which side of lung is abnormal in this image,left or right?\n<image>", "pred_response": "Left", "answer_id": "8kjHsaDvF5e8RC59Js3Kwg", "gt_response": "Left", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 514, "prompt": "What part of the lung is the pneumonia located in?\n<image>", "pred_response": "Right", "answer_id": "7xp3Cq5xUZbPpyBs2kFqa3", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 515, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Respiratory", "answer_id": "3tmBQuFdJSZA4eorJjmsdQ", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 516, "prompt": "Which organs appear in pairs?\n<image>", "pred_response": "Lung", "answer_id": "JjvzC3zPTcJU7mqmcKceoV", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 517, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "WFUtVgdjNULbuFqQsLVLcH", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 518, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "DFRD9DfsJzutqMiNHymDsn", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 519, "prompt": "Can pneumonia be observed on the upper right lung in this picture?\n<image>", "pred_response": "Yes", "answer_id": "PJZ9Z6GuoH3sSJ9WkEgRrN", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 520, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "Gray", "answer_id": "NVogZVyqchDwgX7MoncYXC", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 521, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "mDJt8DTbBkVxVHhYxayMKF", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 522, "prompt": "What is the effect of the center organ in this picture?\n<image>", "pred_response": "The center organ, the heart, is responsible for pumping blood throughout the body. It is a vital organ that ensures the proper circulation of blood, which is essential for the delivery of oxygen and nutrients to the body's cells and the removal of waste products. In the image, the heart is shown to be located in the center of the body, surrounded by the lungs, which are responsible for the exchange of oxygen and carbon dioxide. The heart's function is crucial to maintaining the overall health and well-being of the body.", "answer_id": "Z4zvxMVuvp3NzaAWsfPVue", "gt_response": "Promote blood flow", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 523, "prompt": "How to treat the disease located on the lower left of lung in this image?\n<image>", "pred_response": "The disease located on the lower left of the lung in the image is likely a case of bronchitis, which is an inflammation of the bronchial tubes. To treat bronchitis, the patient should follow a proper treatment plan, which may include:\n\n1. Resting and avoiding activities that trigger coughing and breathing difficulties.\n2. Drinking plenty of fluids to help thin the mucus in the lungs and reduce inflammation.\n3. Taking over-the-counter medications such as acetaminophen, ibuprofen, or asp", "answer_id": "mRhLpK5Vg8DSvA7jwKXDf3", "gt_response": "Medical therapy, supportive therapy", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 524, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "EcDLpLU4wrfj9wCRHgKiT4", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 525, "prompt": "What type of medical image is this?\n<image>", "pred_response": "CT", "answer_id": "cHEiF5jefsPhG2RbxdMKwZ", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 526, "prompt": "Is this a coronal plane?\n<image>", "pred_response": "Yes", "answer_id": "K4fkVJ7JJjR636fCrY5uH3", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 527, "prompt": "Which part do the organs in this image locate from the aspect of the whole body?\n<image>", "pred_response": "Left", "answer_id": "B7YDQNui62ZiVxBEhztyWh", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 528, "prompt": "Where is the abnormality in this image?\n<image>", "pred_response": "Right side", "answer_id": "MxLCQaji64jyUcDfrXisG4", "gt_response": "Upper Right Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 529, "prompt": "What part of the lung is the pneumonia located in?\n<image>", "pred_response": "Right", "answer_id": "dwBasgaGcPTMxKaBRuTbKM", "gt_response": "Upper Right Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 530, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Respiratory", "answer_id": "7PQHdzoCJxEbtsc7H4KBvY", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 531, "prompt": "Is this a study of the neck?\n<image>", "pred_response": "No", "answer_id": "jqFAS6HzsUnqeYGDZ6ZHLZ", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 532, "prompt": "Does the lung look abnormal?\n<image>", "pred_response": "Yes", "answer_id": "ZQ6HiiANHmQHVNEuC9uHJY", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 533, "prompt": "Are there abnormalities in this image?\n<image>", "pred_response": "Yes", "answer_id": "KdUhkPD3hkvDbnXAPQXW7N", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 534, "prompt": "Can pneumonia be observed on the upper right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "4hqJYn7Gx6AKvAbzDDMfVJ", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 535, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "Gray", "answer_id": "BFWt2vtQZa4hbN3vNsi4Xt", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 536, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "1", "answer_id": "RY68dQPv444LZtcXDh2gwi", "gt_response": "1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 537, "prompt": "What is the function of the main organ in this picture?\n<image>", "pred_response": "The main organ in the picture is the heart, which is responsible for pumping blood throughout the body.", "answer_id": "6sWsnnZY2qj6jPsPNKCuLp", "gt_response": "Breathe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 538, "prompt": "How to prevent the occurrence of the disease on the upper right lobe of lung in this image?\n<image>", "pred_response": "To prevent the occurrence of the disease on the upper right lobe of the lung, the patient should maintain a healthy lifestyle, including regular exercise, a balanced diet, and avoiding smoking. Additionally, the patient should follow the recommended guidelines for lung cancer screening, which typically involve a low-dose CT scan of the chest every year or two, depending on the patient's risk factors. Early detection and timely treatment are crucial in managing and preventing the progression of the disease.", "answer_id": "dJyR9G7S7cLQdjPgL7uoZR", "gt_response": "Pay attention to prevent cold and keep warm, enhance physical fitness", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 539, "prompt": "Which organs/organ is part of the respiratory system in this image?\n<image>", "pred_response": "Lung", "answer_id": "njoeQiDrhb3ahyxrvDdFiz", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 540, "prompt": "How was this image taken?\n<image>", "pred_response": "CT", "answer_id": "hw4YtNuBeoNesy3vimen2T", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 541, "prompt": "Is this a transverse section?\n<image>", "pred_response": "Yes", "answer_id": "mmTxSEMaaD2L6CAap5dugf", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 542, "prompt": "Where does the picture show?\n<image>", "pred_response": "Chest", "answer_id": "UQytEBAdDHb6WUn2LsyiC2", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 543, "prompt": "Which side of lung is abnormal in this image?\n<image>", "pred_response": "Left", "answer_id": "mjtgKSmRLDGCpKHofa2EYP", "gt_response": "Right", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 544, "prompt": "Where is the pneumonia in the lung?\n<image>", "pred_response": "Left", "answer_id": "STKC3y69vxitP5Czy9f6dN", "gt_response": "Lower Right Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 545, "prompt": "Is this a study of the chest?\n<image>", "pred_response": "Yes", "answer_id": "bPbAcpR8hYZSqM7fVXCXpS", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 546, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Lung", "answer_id": "3cyD2nBNp9cq4EWsbF6Tn2", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 547, "prompt": "What disease is/are shown on the right of lung?\n<image>", "pred_response": "Cancer", "answer_id": "UBTA2npMsitTa3hjVUYfhy", "gt_response": "Pneumonia", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 548, "prompt": "Is the lung normal appearing?\n<image>", "pred_response": "Yes", "answer_id": "fCgVHtZan8MwnMFAJmTsEj", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 549, "prompt": "Can pneumonia be observed on the lower right lung in this picture?\n<image>", "pred_response": "No", "answer_id": "NKhqwgRf4ULCdv4rkdy2MM", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 550, "prompt": "What color does the right lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "5KvufuSfEzEVPBRWdKzpVB", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 551, "prompt": "How many existing lungs in this image?\n<image>", "pred_response": "2", "answer_id": "BHhrzMgmheHrh9hvWBwNUq", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 552, "prompt": "What is the effect of the center organ in this picture?\n<image>", "pred_response": "The center organ, the heart, is responsible for pumping blood throughout the body, providing oxygen to the body's cells, and removing waste products.", "answer_id": "C2cyR9tFoJqnBiYBPvvKhF", "gt_response": "Promote blood flow", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 553, "prompt": "How to prevent the occurrence of the disease on the lower right lobe of lung in this image?\n<image>", "pred_response": "To prevent the occurrence of the disease on the lower right lobe of the lung, the patient should maintain a healthy lifestyle, including regular exercise, a balanced diet, and avoiding smoking. Additionally, the patient should follow the advice of their healthcare provider, such as taking medications as prescribed, attending regular check-ups, and adhering to any recommended lifestyle changes.", "answer_id": "Y2vyfRTMUitZZPNGrrFtmQ", "gt_response": "Pay attention to prevent cold and keep warm, enhance physical fitness", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 554, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Heart", "answer_id": "WBq2f6zFgmgPWEhaBaggQ4", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 555, "prompt": "Is this image taken via CT?\n<image>", "pred_response": "Yes", "answer_id": "BsKNU6kyxGvog9E9oAbBjC", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 556, "prompt": "Is this a transverse plane?\n<image>", "pred_response": "Yes", "answer_id": "kpqU7JzevWpNMjvNtaLLeP", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 557, "prompt": "Where does the picture show?\n<image>", "pred_response": "Chest", "answer_id": "n26onBbNqFfKG6yXtDeCXd", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 558, "prompt": "Which side of lung is abnormal in this image?\n<image>", "pred_response": "Right", "answer_id": "BtjcrBZ8tAKi3isyK7MJ98", "gt_response": "Left", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 559, "prompt": "What part of the lung is the infiltration located in?\n<image>", "pred_response": "Right", "answer_id": "YShLgNvrUAnU96wkUDsKFo", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 560, "prompt": "Which organs appear in pairs?\n<image>", "pred_response": "Lung", "answer_id": "ZVd2ETa4wYp7CtCy7JCEuK", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 561, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Heart", "answer_id": "FFSnhMZqFuUjpTmMTHbMg4", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 562, "prompt": "Is/Are there abnormalities in the patient's left lung?\n<image>", "pred_response": "Yes", "answer_id": "2s3Qfww3WeSs6zYcqEXxLv", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 563, "prompt": "What disease is/are shown on the lower left of lung?\n<image>", "pred_response": "Cancer", "answer_id": "XFiVtvSUCCvNob7RE32k7V", "gt_response": "Pulmonary Infiltration", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 564, "prompt": "Can infiltration be observed on the lower left lung in this picture?\n<image>", "pred_response": "Yes", "answer_id": "cbdWKpi6BuYm7BfKwsfsYM", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 565, "prompt": "What color is the lung in the picture?\n<image>", "pred_response": "Gray", "answer_id": "JDKSL5Fqbf3WCXDvBAa4ju", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 566, "prompt": "How many existing lungs in this image?\n<image>", "pred_response": "2", "answer_id": "HVTmjycLDuEFe7HZbYkbkH", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 567, "prompt": "What can the organ with black color in this image be used for?\n<image>", "pred_response": "The organ with black color in the image is the heart, which is a muscular organ located in the chest. It is responsible for pumping blood throughout the body, providing oxygen and nutrients to the body's cells, and removing waste products.", "answer_id": "DYc4A2xzUoTxnzWkmXrhBq", "gt_response": "Breathe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 568, "prompt": "What kind of symptoms will the disease on the lower left of lung bring to the patient?\n<image>", "pred_response": "The disease on the lower left side of the lung, which appears to be a tumor, can cause various symptoms to the patient. These may include shortness of breath, coughing, chest pain, and fatigue. The presence of the tumor can also lead to a reduced lung capacity, making it difficult for the patient to breathe and causing them to become more susceptible to respiratory infections. In some cases, the tumor may also spread to other parts of the body, causing additional symptoms and complications. It is essential for the patient to consult a healthcare professional to", "answer_id": "JTtkzD7rAGM93hGSA4zrTi", "gt_response": "Chest pain, cough, expectoration", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 569, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Heart", "answer_id": "PHLvvHuFAREGY9qQdikJen", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 570, "prompt": "Is this image taken via X-Ray?\n<image>", "pred_response": "Yes", "answer_id": "WeLTgsEAhF4KodwvXasVbF", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 571, "prompt": "Is this a coronal section?\n<image>", "pred_response": "Yes", "answer_id": "kuExJ9jcNDiuNwS3BcphfE", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 572, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Chest", "answer_id": "mAnyCXy8752JfB6aDh6QvJ", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 573, "prompt": "Which part of chest is abnormal in this image?\n<image>", "pred_response": "Left", "answer_id": "MZgEh6zhtR6u2mLTWB4eUH", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 574, "prompt": "Where is/are the effusion located?\n<image>", "pred_response": "Lung", "answer_id": "Pk3ePN2hYvRuY4F2MXYy5J", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 575, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Respiratory", "answer_id": "S5ZsEWwQmAmnFh7kcybNuh", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 576, "prompt": "Is this a study of the head?\n<image>", "pred_response": "No", "answer_id": "nAZsBwErghXcYGLLyheKMH", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 577, "prompt": "Does the patient have any abnormalities?\n<image>", "pred_response": "Yes", "answer_id": "P4Nsk2JWo2eroZnvi9746F", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 578, "prompt": "Does the lung look abnormal?\n<image>", "pred_response": "No", "answer_id": "NKy9rJ4z8HShq66t2cHhcz", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 579, "prompt": "Can effusion be observed on the lower left lung in this picture?\n<image>", "pred_response": "No", "answer_id": "G4Erv5novLwygMqkKQbHou", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 580, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "Gray", "answer_id": "HqvJAJQ8hv7V5QhdFbJDyw", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 581, "prompt": "How many lungs are shown in this image?\n<image>", "pred_response": "2", "answer_id": "nLBkKiFCq8YpMJRkVyMZCG", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 582, "prompt": "What can the organ with black color in this image be used for?\n<image>", "pred_response": "The organ with black color in the image is the heart, which is responsible for pumping blood throughout the body.", "answer_id": "asX6KbQ33pUv4PS7JA9MGF", "gt_response": "Breathe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 583, "prompt": "What is the main cause of the disease on the lower left of the lung in the picture?\n<image>", "pred_response": "The main cause of the disease on the lower left of the lung in the picture is smoking.", "answer_id": "YZmGwKBTbbv8Dyk8K76SD7", "gt_response": "Inflammation, malignant tumor, trauma, etc", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 584, "prompt": "Which organs/organ is part of the circulatory system in this image?\n<image>", "pred_response": "Heart", "answer_id": "JnNoB2fWUVLWgUxg9Hufxq", "gt_response": "Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 585, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "gvh94NRWPgWKeZo8B8rUNn", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 586, "prompt": "What scanning plane does this image belong to?\n<image>", "pred_response": "Transverse", "answer_id": "96CsdShHnEvoEg3w9VZwA9", "gt_response": "Coronal Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 587, "prompt": "Do the organs in the image exist in the chest?\n<image>", "pred_response": "Yes, the organs in the image exist in the chest, as the image is a CT scan of the chest.", "answer_id": "25TabnbNR6PzDrpBgc6Z9Y", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 588, "prompt": "Where is the heart in this image?\n<image>", "pred_response": "Left", "answer_id": "PkkvSKQGkd2vJcyn6TXV8F", "gt_response": "Center", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 589, "prompt": "Where is the pleural effusion?\n<image>", "pred_response": "Left", "answer_id": "XTNjGu7YAQhUqyEZJuXDtd", "gt_response": "Lower Left Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 590, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "BA3wwYBR44VHCA9gtNbpjD", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 591, "prompt": "Is this a study of the pelvic cavity?\n<image>", "pred_response": "No", "answer_id": "PJ4HBQmckH7UQp52AdiwDM", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 592, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "AEufvcuP2MDamygSwN9NVU", "gt_response": "Pleural Effusion", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 593, "prompt": "Is this image normal?\n<image>", "pred_response": "Yes", "answer_id": "6wUhSx4cpthwZcE8kirZMC", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 594, "prompt": "Can effusion be observed on the lower left lung in this picture?\n<image>", "pred_response": "Yes, there is a small amount of effusion on the lower left lung in the image.", "answer_id": "LVcvek7mZBUEqjtbPUbbaH", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 595, "prompt": "What color does the left lung show in the picture?\n<image>", "pred_response": "White", "answer_id": "LdQcY3AoKjRwqEZBh7LYcD", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 596, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "2", "answer_id": "NpUNtL3jS5eeTU467VLCtw", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 597, "prompt": "What is the effect of the main organ in this picture?\n<image>", "pred_response": "The main organ in the picture is the heart, which is responsible for pumping blood throughout the body.", "answer_id": "6vwkhsMLxrXtxHmviroBsp", "gt_response": "Breathe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 598, "prompt": "What kind of symptoms will the disease on the lower left of lung bring to the patient?\n<image>", "pred_response": "The disease on the lower left of the lung, which is a pneumonia, can cause various symptoms such as coughing, chest pain, shortness of breath, fever, and fatigue. These symptoms can be severe and may require medical attention to prevent complications and ensure the patient's recovery.", "answer_id": "e4xJHAN8F4NyBkBgnNwGdD", "gt_response": "Chest tightness, dyspnea, chest pain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 599, "prompt": "Which organs/organ is part of the respiratory system in this image?\n<image>", "pred_response": "Lung", "answer_id": "ahjoPgoksc4FLwhbxS44yk", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 600, "prompt": "Which type of modality is shown about this image, MRI, CT or X-Ray?\n<image>", "pred_response": "CT", "answer_id": "dCAgFEgTATEUsrNbfJ3iiX", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 601, "prompt": "Is this sagittal plane?\n<image>", "pred_response": "Yes", "answer_id": "kpVoubeqPTbz5DxpTVVQcT", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 602, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "ZfEUE5p3J2w52p8gpA9psb", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 603, "prompt": "Where is the brain stem in this image?\n<image>", "pred_response": "Top", "answer_id": "4shswR6T3CfbvwkyYKahgD", "gt_response": "Center", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 604, "prompt": "Which organs appear in pairs?\n<image>", "pred_response": "Eyes", "answer_id": "iYdmmeXstdcdYWzkfjcf65", "gt_response": "Temporal Lobe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 605, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "H7bTkrpfeUTgE7PgAAh9Wp", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 606, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "B6ZwiGiNbETh6wLeJLzwEA", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 607, "prompt": "Does the temporal lobe appear in this image?\n<image>", "pred_response": "Yes", "answer_id": "4xKrWamiUGETMzbmC6xFBY", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 608, "prompt": "What color is the right temporal lobe in the picture?\n<image>", "pred_response": "Gray", "answer_id": "VhzRf3LUHoAmtAdzPdAfid", "gt_response": "Gray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 609, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "AMdr3uyE5A44LXz4DqLBFL", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 610, "prompt": "How many ears are shown in this image?\n<image>", "pred_response": "0", "answer_id": "AhYgwJRt6D9L7ncaj8opzV", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 611, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "0", "answer_id": "jh5AHdsb6uJ2gpoG5hzVN4", "gt_response": "Much", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 612, "prompt": "Which organs/organ in the picture belong to the nervous system?\n<image>", "pred_response": "Brain", "answer_id": "KTEJuVhVWysk95bHab5x5N", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 613, "prompt": "Are there some organs controlling heartbeat and breathing in this image?\n<image>", "pred_response": "No", "answer_id": "HPL3E5C4UprSdUbQn2CHqk", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 614, "prompt": "Do any of the organs in the picture belong to the respiratory system?\n<image>", "pred_response": "No", "answer_id": "YAvZiuPfRs2K43N9JuzakG", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 615, "prompt": "Is this a CT?\n<image>", "pred_response": "Yes", "answer_id": "4EeszWqy8wz3MEsvX9Petu", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 616, "prompt": "Is this sagittal plane?\n<image>", "pred_response": "Yes", "answer_id": "d46QHFTcE4Uykhewac3BJa", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 617, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Head", "answer_id": "mjjGkJJURoJrRam4dswHKg", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 618, "prompt": "Where is the brain stem in this image?\n<image>", "pred_response": "Top", "answer_id": "Xe4hegaJhzUZWNcYKVmswY", "gt_response": "Center", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 619, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Head", "answer_id": "GwyKe3f9boSSLu8UJf2mWw", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 620, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Head", "answer_id": "5NQeRvwsCe79grwFBee7Nd", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 621, "prompt": "Does the ears exist in this picture?\n<image>", "pred_response": "No", "answer_id": "krZJBkVvKVWaap93mUJDKq", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 622, "prompt": "Does the picture contain bladder?\n<image>", "pred_response": "No", "answer_id": "n8xHoT8gqghZ2ZHR2Hkft5", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 623, "prompt": "What color is the ears in the picture?\n<image>", "pred_response": "Black", "answer_id": "M5qp8s5hyTdHfna56Jrw7J", "gt_response": "White", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 624, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "EY6Ugx4pk4YPy8QptSPW6p", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 625, "prompt": "How many temporal lobes in this image?\n<image>", "pred_response": "2", "answer_id": "3PX4S6mKgYAtofX3dE2qvX", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 626, "prompt": "What is the function of the tissue on the center of this image?\n<image>", "pred_response": "The tissue on the center of the image, which is the skull, serves as the bony structure that houses the brain and other vital organs.", "answer_id": "nVSnrEuoBaXFdLJEDMcPaz", "gt_response": "Control heartbeat and breathing", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 627, "prompt": "Does the picture contain the organ which has the effect of storing urine?\n<image>", "pred_response": "No", "answer_id": "USwkssGiWUinXfth7UzNia", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 628, "prompt": "Do any of the organs in the picture belong to the digestive system?\n<image>", "pred_response": "No", "answer_id": "JYAArYhAYLS7md8LokZkXv", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 629, "prompt": "What modality is shown in this image?\n<image>", "pred_response": "CT", "answer_id": "fTCqejHsoZ9x7jwWB89YNS", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 630, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse", "answer_id": "dv2tN4Tjok7kvsX8EQs92V", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 631, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "NKhYWZ8get3n9oeEr7rSmA", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 632, "prompt": "Where is the tooth in this image?\n<image>", "pred_response": "Head", "answer_id": "AZGDympTyofd58p4N4VsQy", "gt_response": "Top", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 633, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "JS4H5kvCK7y8MCgxhmQEyd", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 634, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Head", "answer_id": "WPBkThqXwdz8YL97h8VLRS", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 635, "prompt": "Does the picture contain mandible?\n<image>", "pred_response": "No", "answer_id": "dCQXb64bKkfKmashD5GHUM", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 636, "prompt": "Does the parotid appear in the image?\n<image>", "pred_response": "No", "answer_id": "4ZNcogFyaCWsViSokcezBE", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 637, "prompt": "What color does the brain stem show in the picture?\n<image>", "pred_response": "Gray", "answer_id": "WBQjWVwwaiYNkZ5pgrSVnU", "gt_response": "Gray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 638, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "EHcduFhrK9P6hKRn7sqdUk", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 639, "prompt": "How many mandibles in this image?\n<image>", "pred_response": "0", "answer_id": "nfTjRMGG8Ci8nzrkS93MWd", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 640, "prompt": "What is the function of the organ on the top of this image?\n<image>", "pred_response": "Sensing", "answer_id": "nmSBJjqtWsa2g7QyHbHhfJ", "gt_response": "Cutting, chewing, maintaining facial contour and assisting pronunciation", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 641, "prompt": "Which organs/organ in the picture belong to nervous system?\n<image>", "pred_response": "Brain", "answer_id": "RU68LZosugiBZx9SSF8ZHQ", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 642, "prompt": "Are there organs in the picture that can digest food?\n<image>", "pred_response": "No", "answer_id": "b3hHv4vZsro486Ugd8zXM8", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 643, "prompt": "Do any of the organs in the picture belong to the circulatory system?\n<image>", "pred_response": "No", "answer_id": "inUBWdej3awdh58QssvsKv", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 644, "prompt": "What modality is shown in this image?\n<image>", "pred_response": "CT", "answer_id": "8ueNhWvWDLEr7jpPNGWD3c", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 645, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse", "answer_id": "2opoJ5BBUEyNW3bPcn9d24", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 646, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "TYtybgDMxLnASt8XxoPMU3", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 647, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Neck", "answer_id": "i6iChJQUYLAt9se6Y7NHVH", "gt_response": "Neck", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 648, "prompt": "Is there an esophagus in this image?\n<image>", "pred_response": "Yes", "answer_id": "ZUmyD4c9TVKRtFNs9EgEJk", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 649, "prompt": "Does the trachea exist in this picture?\n<image>", "pred_response": "Yes", "answer_id": "Dw5gfLxo8aqXzZgGdk2Sbz", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 650, "prompt": "What is the shape of larynx in this image?\n<image>", "pred_response": "Oval", "answer_id": "bHb4wYv8FnqJGHBjnrMcp2", "gt_response": "Oval", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 651, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "5JUqRoyWnVDy726DGN9Yay", "gt_response": "Larynx", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 652, "prompt": "Are there organs in the picture that can deliver food?\n<image>", "pred_response": "No", "answer_id": "2oekQqWw9dScCCqaPdyykQ", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 653, "prompt": "Does the picture contain the organ which has the effect of gas delivery?\n<image>", "pred_response": "Yes", "answer_id": "KhBSFh2Riu3tjDJqrAF8m7", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 654, "prompt": "Is this a CT?\n<image>", "pred_response": "Yes", "answer_id": "CttuoC9yYdaG86bg8YU5Ct", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 655, "prompt": "What scanning plane does this image belong to?\n<image>", "pred_response": "Transverse", "answer_id": "fTFaPA3CY473Xn9smqNJ4X", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 656, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Head", "answer_id": "agDRbnsVpNRv3jwY5kXdx3", "gt_response": "Neck", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 657, "prompt": "Where is the esophagus in this image?\n<image>", "pred_response": "Top", "answer_id": "n7sNGy9w9S7erhjFYDcR95", "gt_response": "Under the trachea", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 658, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Neurological", "answer_id": "5ShzNisYby6gQUYe9Epj2A", "gt_response": "Neck", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 659, "prompt": "Does the ears exist in this picture?\n<image>", "pred_response": "No", "answer_id": "doah5qQkBuG3bCNgwKZEzm", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 660, "prompt": "Is there small bowel in this image?\n<image>", "pred_response": "No", "answer_id": "5hFvYTfeQDSfXEq9vEKMqU", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 661, "prompt": "What color do the trachea show in the picture?\n<image>", "pred_response": "Gray", "answer_id": "ML6j3r5M4eATHUpXwv989j", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 662, "prompt": "What is the shape of trachea about this patient?\n<image>", "pred_response": "Oval", "answer_id": "o4SScfNSWuBP8gSPrtb6nV", "gt_response": "Irregular", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 663, "prompt": "What role does the black hollow tissue play in the human body?\n<image>", "pred_response": "The black hollow tissue, which is the spinal cord, plays a crucial role in the human body. It is the central nervous system's main pathway for transmitting signals between the brain and the body. The spinal cord is responsible for controlling and coordinating various body functions, such as movement, balance, and sensation. It also serves as a conduit for the transmission of signals that allow the body to respond to stimuli, maintain homeostasis, and perform various tasks. The spinal cord's structure and function are essential for the overall health and well-being of the individual", "answer_id": "AbzNRSjzZyTpgkxyRp3HqS", "gt_response": "Gas delivery", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 664, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "Y2i52iwZh56TARLAdTHnhX", "gt_response": "Trachea", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 665, "prompt": "Do any of the organs in the picture belong to the nervous system?\n<image>", "pred_response": "No", "answer_id": "6qVMgXYN8Htqnc9h6zGBYg", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 666, "prompt": "Does the picture contain the organ that could be used for detoxicating?\n<image>", "pred_response": "Yes", "answer_id": "UhyqMFQND85BdfRAdGwJed", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 667, "prompt": "Is this an MRI scan?\n<image>", "pred_response": "Yes", "answer_id": "YUWNGPTE2ycfxdPWVPh6D2", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 668, "prompt": "Is this a transverse section?\n<image>", "pred_response": "Yes", "answer_id": "kWid6TbfP2foPGH7GNgQzp", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 669, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "kaESxTpAHQ6XAnKDAazA6p", "gt_response": "Neck", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 670, "prompt": "Which is trachea in this image?\n<image>", "pred_response": "Left", "answer_id": "Qydac8zyfueWuHAT796XsF", "gt_response": "Black Hollow", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 671, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Digestive", "answer_id": "BiD9V3M9w5vYEpDNp73fj9", "gt_response": "Neck", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 672, "prompt": "Does the picture contain spinal cord?\n<image>", "pred_response": "No", "answer_id": "6cMmmsGzvQTKzr3sSfPDz9", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 673, "prompt": "Are/Is there kidneys in the picture?\n<image>", "pred_response": "No", "answer_id": "e9CT3nxARqZyauSdCGt3gs", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 674, "prompt": "What is the shape of trachea about this patient?\n<image>", "pred_response": "Oval", "answer_id": "fuorvSU99Tb4xUdExyoYvT", "gt_response": "Irregular", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 675, "prompt": "What role does the black hollow tissue play in the human body?\n<image>", "pred_response": "The black hollow tissue, which is the spinal cord, plays a crucial role in the human body. It is the central nervous system's main pathway for transmitting signals between the brain and the body. The spinal cord is responsible for controlling and coordinating various body functions, such as movement, balance, and sensation. It also serves as the main conduit for the transmission of pain signals, which is why spinal cord injuries can result in severe pain and limited mobility.", "answer_id": "TVxXyVUZrcSvnm9hyRrQQs", "gt_response": "Gas delivery", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 676, "prompt": "Which organs/organ in the picture belong to the digestive system?\n<image>", "pred_response": "Stomach", "answer_id": "Lm5inqB66QknRXrmSt7UEY", "gt_response": "Esophagus", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 677, "prompt": "Does the picture contain the organ which has the effect of discharging waste?\n<image>", "pred_response": "Yes", "answer_id": "JsuvcYmZYXT83pujpB25at", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 678, "prompt": "Do any of the organs in the picture belong to the nervous system?\n<image>", "pred_response": "No", "answer_id": "ZRhpoNnqqrf52p9RmeTybC", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 679, "prompt": "Is this a CT scan?\n<image>", "pred_response": "Yes", "answer_id": "TTx4SqogxGYCbzgvTQAjp4", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 680, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T1", "answer_id": "bpKsGyHoUvoTWgac4peqWB", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 681, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "QmGJ2YzmkozbfGYJabfQqp", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 682, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "3eh2ry6jXqmGQ9DwhsbxAJ", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 683, "prompt": "Which hemisphere is abnormal,left or right?\n<image>", "pred_response": "Left", "answer_id": "YNSbatiP8X9fctM2zgaHrS", "gt_response": "Right", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 684, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Brain", "answer_id": "6tKwtNFbwpdxnpaoc3ZFdp", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 685, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Headache", "answer_id": "Hhi2FdQvCFTpWDf2jtFiA3", "gt_response": "Brain Edema, Brain Enhancing Tumor", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 686, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "0", "answer_id": "GmZG6vYiBt3RYr2WXWd9d7", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 687, "prompt": "What is the main cause of the disease on the right lobe of brain in this picture?\n<image>", "pred_response": "The main cause of the disease on the right lobe of the brain in this picture is a stroke.", "answer_id": "Nq3AZWvbpyUpp6Jw9ZGMCz", "gt_response": "Brain embryonic tissue dysplasia, genetic factors, chemical factors", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 688, "prompt": "How was this image taken?\n<image>", "pred_response": "CT", "answer_id": "jHdtjAxkGa5BQoYZUJwruj", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 689, "prompt": "Is this an MRI image with T1 weighted?\n<image>", "pred_response": "Yes", "answer_id": "eoHSEuoFRd7yYfoX6U4Bwx", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 690, "prompt": "Is this a coronal section?\n<image>", "pred_response": "Yes", "answer_id": "DztbLADiv9ggER6p5roAtm", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 691, "prompt": "Where does the picture show?\n<image>", "pred_response": "Head", "answer_id": "Nt9ZJtG2U3YbH7KwpyYBks", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 692, "prompt": "Where is the brain non-enhancing tumor?\n<image>", "pred_response": "Left", "answer_id": "AHaF5FnRUyeLcWnbrJqyum", "gt_response": "Upper Left Lobe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 693, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Brain", "answer_id": "U87V8FrrTFUojvJBA5CXTY", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 694, "prompt": "Is the brain healthy?\n<image>", "pred_response": "No", "answer_id": "aujtxyChDnc7c67EXMrwpU", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 695, "prompt": "What is the color of abnormality in this image?\n<image>", "pred_response": "White", "answer_id": "AStSzxMbR4gdkEbALcthvu", "gt_response": "White", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 696, "prompt": "What is the main cause of the disease on the upper left lobe of brain in this picture?\n<image>", "pred_response": "The main cause of the disease on the upper left lobe of the brain in this picture is a tumor.", "answer_id": "Yi5UWVLt8r9YP9AZbGSxJL", "gt_response": "Brain embryonic tissue dysplasia, genetic factors, chemical factors", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 697, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "6JjJwmKtydLhQUgKQFngy3", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 698, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T2", "answer_id": "PraixRTUnRBzwXm3rLhfCp", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 699, "prompt": "Is this sagittal plane?\n<image>", "pred_response": "Yes", "answer_id": "CT7osSphWq2iD9zg4U4TRc", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 700, "prompt": "Where does the picture show?\n<image>", "pred_response": "Head", "answer_id": "QvozM8HSmciqibmZTquihd", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 701, "prompt": "Which hemisphere is abnormal,left or right?\n<image>", "pred_response": "Left", "answer_id": "5mZHAkQxkfu22bEN4V9rrx", "gt_response": "Left", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 702, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Brain", "answer_id": "8WLjxPPSjiWjoi53tz7H4c", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 703, "prompt": "What disease is/are shown on the left of brain?\n<image>", "pred_response": "Brain", "answer_id": "DcgpSpcAHeTniJHpTGV8J2", "gt_response": "Brain Edema, Brain Enhancing Tumor, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 704, "prompt": "Is the tumor enhancing?\n<image>", "pred_response": "Yes", "answer_id": "W9drpjZv5rapho7E9wjpXQ", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 705, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "0", "answer_id": "RGrHj8FsWJgecEH7VZ3Cnw", "gt_response": "3", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 706, "prompt": "What kind of symptoms will the disease on the left lobe of brain in the picture bring to the patient?\n<image>", "pred_response": "The disease on the left lobe of the brain, as seen in the CT scan, can cause various symptoms depending on the specific type of brain tumor or lesion. Symptoms may include headaches, seizures, memory loss, difficulty with speech or language, changes in personality, and visual disturbances. The severity and specific symptoms experienced by the patient will depend on the size, location, and type of the lesion, as well as the patient's overall health and medical history. It is essential to consult a medical professional for a proper diagnosis and treatment plan.", "answer_id": "dNE2b8Edd3qsDvtwUKDTrC", "gt_response": "Visual impairment, vomiting, tinnitus, increased intracranial pressure", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 707, "prompt": "What modality is shown in this image?\n<image>", "pred_response": "CT", "answer_id": "ixqRnXLrhhFvUr5wMXHQ3c", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 708, "prompt": "Is this an MRI image with T1 weighted?\n<image>", "pred_response": "Yes", "answer_id": "kTxmAXT2Gt3QtKFibvTvSe", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 709, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "TFYfpn8G6oD5BhkQt9PaxY", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 710, "prompt": "Do the organs in the image exist in the chest?\n<image>", "pred_response": "Yes, the organs in the image exist in the chest.", "answer_id": "3JoJPmfhV6BPjPmGDeUYGF", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 711, "prompt": "Where is the brain edema?\n<image>", "pred_response": "Left", "answer_id": "RckfuLSdLhuy8NtoMBs2k8", "gt_response": "Left Lobe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 712, "prompt": "Is this a study of the head?\n<image>", "pred_response": "Yes", "answer_id": "GRN9rV7iWVhSjfcBwZdHR8", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 713, "prompt": "Is the brain abnormal?\n<image>", "pred_response": "Yes", "answer_id": "39wp3MgpipK6XETsrRqWCZ", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 714, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "There are two kinds of abnormalities in this image: a cleft in the skull and a cyst in the brain.", "answer_id": "mLzeAbrbYd6rjLs2LxmUaB", "gt_response": "1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 715, "prompt": "How to treat the most severe disease in this image?\n<image>", "pred_response": "The most severe disease depicted in the image is a brain tumor, which is a life-threatening condition. The image shows a CT scan of the brain, which is a diagnostic tool used to identify the presence, size, and location of the tumor. The treatment options for a brain tumor depend on the type, location, and severity of the tumor. In general, the primary goal of treatment is to remove the tumor while preserving as much normal brain tissue as possible. Surgery is often the first step, followed by radiation therapy and chemotherapy, depending on the", "answer_id": "acNkiQpzyWo5txfJVAbTiy", "gt_response": "Pharmacotherapy, rehabilitation", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 716, "prompt": "What modality is shown in this image?\n<image>", "pred_response": "CT", "answer_id": "ChwmvX7ZH7rt6K64ZY6YoR", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 717, "prompt": "Is this a T1 weighted or T2 weighted MRI image?\n<image>", "pred_response": "This is a T1 weighted MRI image.", "answer_id": "GbNJm8nQGaR4VnPrzTiXBU", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 718, "prompt": "Is this a coronal section?\n<image>", "pred_response": "Yes", "answer_id": "MKK942C7YtwUoFj9zsyxQe", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 719, "prompt": "Do the organs in the image exist in the abdomen?\n<image>", "pred_response": "No", "answer_id": "nDfxXkdziqV3qT8gwnKcNt", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 720, "prompt": "Where are the abnormalities in this image?\n<image>", "pred_response": "The abnormalities in this image are the white spots on the brain, which are visible on the CT scan.", "answer_id": "Cjet7m9dBSZ7DCb3BuYN7z", "gt_response": "Upper Left Lobe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 721, "prompt": "Is this a study of the head?\n<image>", "pred_response": "Yes", "answer_id": "gj6W3iiJDHw6aXWy8vxG4X", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 722, "prompt": "Is there evidence of a brain edema?\n<image>", "pred_response": "Yes", "answer_id": "eFG8YG6xxmtTiTJ5fVH6pt", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 723, "prompt": "Is the abnormality hyperdense or hypodense?\n<image>", "pred_response": "The abnormality is hyperdense.", "answer_id": "mb8KoBLK6sLyHevaNVXa5E", "gt_response": "Hyperdense", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 724, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "1", "answer_id": "oDrLRUEq8Lr5P4yoUHPtT5", "gt_response": "1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 725, "prompt": "How to treat the most severe disease in this image?\n<image>", "pred_response": "The most severe disease depicted in the image is a brain tumor, which is a life-threatening condition. The image shows a CT scan of the brain, revealing a large mass in the center of the brain. The treatment options for a brain tumor depend on the type, size, and location of the tumor, as well as the patient's overall health. In general, the primary goal of treatment is to remove the tumor while preserving as much healthy brain tissue as possible. Surgery, radiation therapy, and chemotherapy are the most common treatment options for brain tum", "answer_id": "MqyDdsfykM46JyJytHoznz", "gt_response": "Pharmacotherapy, rehabilitation", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 726, "prompt": "What type of medical image is this?\n<image>", "pred_response": "CT", "answer_id": "HUe6sYchAJxXSUx8j3mft9", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 727, "prompt": "Is this a T1 weighted or T2 weighted MRI image?\n<image>", "pred_response": "T1 weighted", "answer_id": "ZkntUokAQGGV9n3JEHXtCo", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 728, "prompt": "Is this a coronal section?\n<image>", "pred_response": "Yes", "answer_id": "9cBo7fam3p3HstJoQGLwjU", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 729, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "CtbrRLSK2FyhRL2RpX58z4", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 730, "prompt": "Where is the brain tumor?\n<image>", "pred_response": "Left", "answer_id": "ixqCgvroMcZAYcpR2VC5b7", "gt_response": "Upper Left Lobe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 731, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Brain", "answer_id": "n2mPJwZmRMkLbBzdzWLHEW", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 732, "prompt": "Is this image normal?\n<image>", "pred_response": "No", "answer_id": "YUhSBA7GPVygwVBqvDcDbU", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 733, "prompt": "How to prevent the occurrence of the disease on the upper left lobe of brain in this image?\n<image>", "pred_response": "To prevent the occurrence of the disease on the upper left lobe of the brain, it is essential to maintain a healthy lifestyle, including regular exercise, a balanced diet, and stress management. Additionally, it is crucial to practice good hygiene, such as washing hands frequently and avoiding close contact with sick individuals. It is also important to get regular check-ups and screenings, especially if there is a family history of the disease or if the individual has other risk factors. In the case of the image, the CT scan shows the upper left lobe of the brain, which is the area", "answer_id": "g3TJvmdYtJyfauaxVW86u5", "gt_response": "Pay attention to dietary hygiene, strengthen physical fitness and avoid brain trauma", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 734, "prompt": "What imaging modality was used?\n<image>", "pred_response": "CT", "answer_id": "LdMHAYjJcaH7RpNfbG9KcN", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 735, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T2", "answer_id": "RGKkEE8RRJ7MV6Roz5G92c", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 736, "prompt": "Is this a transverse section?\n<image>", "pred_response": "Yes", "answer_id": "RvqBdtLZhwn4hRMnGNiN6A", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 737, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "Cok4ZfGEqrhJsf6WpyGc5c", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 738, "prompt": "Where is the brain tumor?\n<image>", "pred_response": "Left", "answer_id": "cRAW3NfCtmku7NQx9EgiDo", "gt_response": "Upper Right Lobe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 739, "prompt": "Is this a study of the neck?\n<image>", "pred_response": "No", "answer_id": "ECjdFxh8wj9ht5GAXmazTr", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 740, "prompt": "What disease is/are shown on the upper right of brain?\n<image>", "pred_response": "Cancer", "answer_id": "Xiur4fVU3hMGQCRLuj5cTR", "gt_response": "Brain Edema, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 741, "prompt": "Is the abnormality hyperdense or hypodense?\n<image>", "pred_response": "Hyperdense", "answer_id": "CiaRGVtoceMny4LfZbXcL7", "gt_response": "Hyperdense", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 742, "prompt": "What kind of symptoms will the disease on the upper right lobe of brain in the picture bring to the patient?\n<image>", "pred_response": "The disease on the upper right lobe of the brain, as seen in the CT scan, can cause various symptoms depending on the specific type of tumor or lesion present. Symptoms may include headaches, seizures, memory loss, difficulty with speech or language, and changes in personality or mood. In some cases, the tumor or lesion may also cause visual disturbances, such as double vision, blurred vision, or loss of vision in one eye. The severity and specific symptoms experienced by the patient will depend on the size, location, and type of the lesion", "answer_id": "5Z8btp3kSt3bYTE2XCTac9", "gt_response": "Visual impairment, vomiting, tinnitus, increased intracranial pressure", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 743, "prompt": "How was this image taken?\n<image>", "pred_response": "CT", "answer_id": "g5a2bSK9SvA5CgP9x7vYTj", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 744, "prompt": "Is this a T1 weighted or T2 weighted MRI image?\n<image>", "pred_response": "T1 weighted", "answer_id": "XpT3jGMT4g4ihVPM4WCPsf", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 745, "prompt": "Is this a coronal section?\n<image>", "pred_response": "Yes", "answer_id": "5WeUiXwjufBB8ch9VL7cBR", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 746, "prompt": "Do the organs in the image exist in the abdomen?\n<image>", "pred_response": "No", "answer_id": "dG7cqUGLJCejLyVEUUDr9U", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 747, "prompt": "Where is the brain tumor?\n<image>", "pred_response": "Left", "answer_id": "b9fWmUJ7YgKgLhP8YZTjno", "gt_response": "Upper Right Lobe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 748, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Brain", "answer_id": "G52Vt4wfFPnnnD83j6enU8", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 749, "prompt": "Is the brain abnormal?\n<image>", "pred_response": "Yes", "answer_id": "WW8mnoxETSWY7ssokpFeMZ", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 750, "prompt": "How to prevent the occurrence of the disease on the upper right lobe of brain in this image?\n<image>", "pred_response": "To prevent the occurrence of the disease on the upper right lobe of the brain, it is essential to maintain a healthy lifestyle, including regular exercise, a balanced diet, and stress management. Additionally, it is crucial to practice good hygiene, such as washing hands frequently and avoiding close contact with sick individuals. It is also important to get regular check-ups and screenings, especially if there is a family history of the disease or if the individual has other risk factors. In the case of the image, the disease appears to be a tumor, so it is essential to consult a medical", "answer_id": "dsTDbMtyybKrzG3mLbPfwK", "gt_response": "Pay attention to dietary hygiene, strengthen physical fitness and avoid brain trauma", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 751, "prompt": "Is this image taken via MRI?\n<image>", "pred_response": "Yes", "answer_id": "NLEsd3uqo9vkwh37H3yXsN", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 752, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "The mr weighting in this image is 0.", "answer_id": "jtoaPTxWawuxcLg7oEyrNH", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 753, "prompt": "What scanning plane does this image belong to?\n<image>", "pred_response": "Transverse", "answer_id": "cRU2uwcLHu9wsbswbq3e2g", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 754, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Head", "answer_id": "X7PawZPxrRafsAVQuLPAcg", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 755, "prompt": "Which lobe is abnormal,left or right?\n<image>", "pred_response": "Left", "answer_id": "JwV8NBRXLj8uHdgXcwiTtH", "gt_response": "Right", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 756, "prompt": "Is this a study of the head?\n<image>", "pred_response": "Yes", "answer_id": "nPh6cBRFz4WmThyJWtyzku", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 757, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "The picture includes a CT scan of the brain, which shows the skull, the brain, and the spinal cord.", "answer_id": "X3w8nbo8JmXPtGemW3aFbk", "gt_response": "Brain Edema, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 758, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "Taking an image of the brain, as seen in the image, can cause significant damage to the body. The image shows the inside of the skull, which is the bony structure that protects the brain. The skull is not meant to be seen from the outside, and the process of obtaining such an image can be extremely invasive and potentially harmful to the patient. The damage can result from the use of imaging techniques like CT scans, which involve the exposure to ionizing radiation, or from the surgical procedures required to access the skull. The risks associated with these procedures can include in", "answer_id": "Feo6oMrEHBCMRE29sUVC7Q", "gt_response": "None", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 759, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "34pN6cBP58FEoHNRnNCAv9", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 760, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "4Z8XqXWeYv4fu9qLN4JbWc", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 761, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "0", "answer_id": "WRswiscKvDBM8SMWb2bbTe", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 762, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Brain", "answer_id": "gszHYaj6rKCxjih5pEfLKV", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 763, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "jA5v6BKKwUeXTQib3t5wUn", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 764, "prompt": "Does the picture contain brian?\n<image>", "pred_response": "Yes", "answer_id": "ghYBE4GrybKy2BxR35RLuv", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 765, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Headache", "answer_id": "VWdbHFmsCAckJ9ebqdsCdz", "gt_response": "Brain Edema", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 766, "prompt": "Does the brain look abnormal?\n<image>", "pred_response": "Yes", "answer_id": "8LyKpdK543RjhRz3sbfSP8", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 767, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Brain", "answer_id": "hyTssGz7WJDR9g5i4bXXEn", "gt_response": "Lower Right", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 768, "prompt": "Is this a X-Ray?\n<image>", "pred_response": "Yes", "answer_id": "2BPrwsGQfbSd5wcgfcByzQ", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 769, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T1", "answer_id": "9VCW2Uf7RKVhKpY7HtD5m5", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 770, "prompt": "Is this a coronal plane?\n<image>", "pred_response": "Yes", "answer_id": "DgKCwesgzLaoNZjhByK78b", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 771, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "VcHtSy5rRQQRPZesCVULNF", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 772, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Left", "answer_id": "hfKSwYhSzzLEFRhYa8CdT4", "gt_response": "Upper Right Lobe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 773, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Brain", "answer_id": "hi4v8jvXS7deHvtZSJqRea", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 774, "prompt": "Is/Are there abnormalities in the patient's left brain lobe?\n<image>", "pred_response": "Yes", "answer_id": "JUe3zZNqxUnojda6ueRTyH", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 775, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "There are two kinds of abnormalities in this image: a cleft in the skull and a cleft in the brain.", "answer_id": "gQjXUsp2DQE7EzfHDgcxJq", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 776, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "Taking an image of the brain, as seen in the image, can cause significant damage to the body. The image shows the inside of the skull, which is the protective covering of the brain. The skull is a hard, bony structure that shields the brain from external forces and potential injuries. However, if the skull is removed or damaged, the brain would be exposed to the environment, which can lead to severe consequences such as infections, inflammation, and even death. Additionally, the image may also cause distress to the person whose brain is being imaged, as it can", "answer_id": "du7V2oPdc4mbpwK5rNzEBJ", "gt_response": "None", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 777, "prompt": "Is this a CT scan?\n<image>", "pred_response": "Yes", "answer_id": "fRNY4PcUub2YwNMnExwXLh", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 778, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "0", "answer_id": "ANXcCmhRmZsEtMcRV294tw", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 779, "prompt": "Is this sagittal plane?\n<image>", "pred_response": "Yes", "answer_id": "mZetubLKDhdSHWF6id6tSy", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 780, "prompt": "Do the organs in the image exist in the pelvic cavity?\n<image>", "pred_response": "Yes", "answer_id": "6SxgXqCUPvuDobCdwGTgyX", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 781, "prompt": "Where are the abnormalities in this image?\n<image>", "pred_response": "Left", "answer_id": "bbRjopZs3MpK8iZzjWnBeV", "gt_response": "Left Lobe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 782, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Brain", "answer_id": "Jn2whmDUYSjDMoNCerW9Xj", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 783, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain", "answer_id": "MXU2g59qrhMdDx2o6iLMxH", "gt_response": "Brain Edema, Brain Enhancing Tumor, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 784, "prompt": "Is the brain enhancing tumor hyperdense or hypodense?\n<image>", "pred_response": "Hyperdense", "answer_id": "Czmj9QpbRPMX6LLNnuU22U", "gt_response": "Hyperdense", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 785, "prompt": "What kind of symptoms will the disease on the left lobe of brain in the picture bring to the patient?\n<image>", "pred_response": "The disease on the left lobe of the brain, as seen in the image, can cause a variety of symptoms depending on the specific type of brain tumor. Symptoms may include headaches, seizures, memory loss, difficulty with speech or language, changes in personality, and visual disturbances. The severity and specific symptoms experienced by the patient will depend on the size, location, and type of the tumor, as well as the patient's overall health and the effectiveness of the treatment.", "answer_id": "nrGumYskzZTT6pt5VLfUVu", "gt_response": "Visual impairment, vomiting, tinnitus, increased intracranial pressure", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 786, "prompt": "Is this image taken via CT?\n<image>", "pred_response": "Yes", "answer_id": "Mg8XXFqichDywbuCX2KBJV", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 787, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T2", "answer_id": "noBbYc7fn4GfNX6QznTkwQ", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 788, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse", "answer_id": "CCsnYrth9vZ34ZiEMgJiuE", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 789, "prompt": "Which part of the human body is the organ located in the image?\n<image>", "pred_response": "Head", "answer_id": "JWAmWfwfbxsUQT4j2ZY6jn", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 790, "prompt": "Where is the brain tumor located?\n<image>", "pred_response": "Left", "answer_id": "dMzX7ZB2WtSi9YU75FXgYy", "gt_response": "Lower Left Lobe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 791, "prompt": "Is this a study of the chest?\n<image>", "pred_response": "No", "answer_id": "JhpiMpWW9txNAQvSP5KN5p", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 792, "prompt": "Does the patient have brain tumor?\n<image>", "pred_response": "Yes", "answer_id": "5fAno3UPQSN3vcz9qTXq5n", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 793, "prompt": "What is the main cause of the disease on the left lobe of brain in this picture?\n<image>", "pred_response": "The main cause of the disease on the left lobe of the brain in this picture is a tumor.", "answer_id": "LnoXvbRWkuMe9c5KZfDce7", "gt_response": "Brain embryonic tissue dysplasia, genetic factors, chemical factors", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 794, "prompt": "How was this image taken?\n<image>", "pred_response": "CT", "answer_id": "Wc47tWBNtkr6686bc3jK46", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 795, "prompt": "What type of MRI is shown in this image?\n<image>", "pred_response": "A CT (computed tomography) image of the brain is shown in the image.", "answer_id": "Hrwv9nnybX4B7YXjPk4Fbq", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 796, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse", "answer_id": "GYMbDxcz3XXDG9MPAi3TRg", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 797, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Head", "answer_id": "4B2SzM97TGTeyELtDhEaxM", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 798, "prompt": "Where is the brain tumor?\n<image>", "pred_response": "Left", "answer_id": "cZSgAY6auj5szPgtEKJbcW", "gt_response": "Left Lobe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 799, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "JP7XhwhbmDdFgwwRehn7Zn", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 800, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "The image shows a CT scan of the brain, which is a medical imaging technique used to diagnose and monitor various brain diseases and conditions.", "answer_id": "kF57TUFJHKVuS9SoqRV4fH", "gt_response": "Brain Edema, Brain Enhancing Tumor, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 801, "prompt": "What density is the brain enhancing tumor ?\n<image>", "pred_response": "The brain enhancing tumor is white.", "answer_id": "S2E4gESkuJzmbZ2kTaSjEu", "gt_response": "Hyperdense", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 802, "prompt": "What kind of symptoms will the disease on the left lobe of brain in the picture bring to the patient?\n<image>", "pred_response": "The disease on the left lobe of the brain, which is a cerebral tumor, can cause various symptoms depending on its size, location, and the specific type of tumor. Symptoms may include headaches, seizures, memory loss, difficulty with speech or language, and changes in personality or mood. In some cases, the tumor may also cause weakness or paralysis on one side of the body, vision problems, or difficulty with balance and coordination. The severity and specific symptoms experienced by the patient will depend on the specific type of tumor and the extent", "answer_id": "Gv98pnkqPgfLAK2pQT4KJx", "gt_response": "Visual impairment, vomiting, tinnitus, increased intracranial pressure", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 803, "prompt": "Is this a CT?\n<image>", "pred_response": "Yes", "answer_id": "gb2nMnqfp58ttCeN5PQGEt", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 804, "prompt": "Is this an MRI image with T2 weighted?\n<image>", "pred_response": "Yes", "answer_id": "XFkc6UGpdXzHaUTZR9uQMZ", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 805, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "7n4ddPKgz2rMkk6VSe6QyP", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 806, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes", "answer_id": "5cpZ53VBPD6eaivvwLLvzh", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 807, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Brain", "answer_id": "dHyLfvESNcV9e5ngGn7Nuz", "gt_response": "Lower Left Lobe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 808, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Brain", "answer_id": "FBQmTACmvBRTMoeaLTcCVW", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 809, "prompt": "What disease is/are shown on the lower left of brain?\n<image>", "pred_response": "Cancer", "answer_id": "94hcgiu3DmT7xniVoPnYqM", "gt_response": "Brain Edema, Brain Enhancing Tumor", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 810, "prompt": "What is the color of brain enhancing tumor in this image?\n<image>", "pred_response": "Gray", "answer_id": "4dCLSBEqRuhVrcwtqeLnj6", "gt_response": "White", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 811, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "1", "answer_id": "J9roe7WcS7n7pcSSAgjaqT", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 812, "prompt": "How to prevent the occurrence of the disease on the lower left lobe of brain in this image?\n<image>", "pred_response": "To prevent the occurrence of the disease on the lower left lobe of the brain, it is essential to maintain a healthy lifestyle, including regular exercise, a balanced diet, and avoiding excessive alcohol and tobacco consumption. Additionally, it is crucial to practice good sleep hygiene, manage stress, and maintain a healthy weight. Regular check-ups and screenings, such as CT scans, can also help detect any potential issues early on, allowing for timely intervention and treatment.", "answer_id": "A5MyR2wogFWX3bCxoupcJC", "gt_response": "Pay attention to dietary hygiene, strengthen physical fitness and avoid brain trauma", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 813, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "W5UjvAhj4WTWMtPWnYjCeN", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 814, "prompt": "Is this an MRI image with T2 weighted?\n<image>", "pred_response": "Yes", "answer_id": "gAfTeFJSKACtnpJQYNWd8x", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 815, "prompt": "What is the scanning plane of this image?\n<image>", "pred_response": "Transverse", "answer_id": "PtEskmMbmdFnUNMrGFnqWV", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 816, "prompt": "Do the organs in the image exist in the head?\n<image>", "pred_response": "Yes, the organs in the image exist in the head.", "answer_id": "KdSTCXNNEo5DLjN37jdZwo", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 817, "prompt": "Where is the brain non-enhancing tumor?\n<image>", "pred_response": "Left", "answer_id": "bdUMpyNA8b3SVJiqEy9sd4", "gt_response": "Left Lobe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 818, "prompt": "Is this a study of the head?\n<image>", "pred_response": "Yes", "answer_id": "dGDpKDouMP9ezM9gno6FS9", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 819, "prompt": "What disease is/are shown on the left of brain?\n<image>", "pred_response": "Cancer", "answer_id": "Y6dHmjiK8E8XcUopYraTEF", "gt_response": "Brain Edema, Brain Non-enhancing Tumor", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 820, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "There are two kinds of abnormalities in this image: a tumor and a cyst.", "answer_id": "m5wUYgsnemKw6NKa5p8cHn", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 821, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "Taking a CT scan image of the brain, as shown in the image, can cause some level of radiation exposure to the body. While the amount of radiation is generally low, it is still important to consider the potential risks associated with such imaging. The risks include an increased chance of developing cancer, especially when the CT scan is performed frequently or on young children. It is essential to follow the recommended guidelines and safety measures provided by the healthcare facility to minimize the radiation exposure and ensure the safety of the patient.", "answer_id": "KpntpVzK66V9z2KxtXBE7Y", "gt_response": "None", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 822, "prompt": "Is this an MRI scan?\n<image>", "pred_response": "Yes", "answer_id": "5KhmwQn2R6nvcpcnBTJZv4", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 823, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "0", "answer_id": "RoSRuAAufM2o4djFC9phWK", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 824, "prompt": "Is this a coronal section?\n<image>", "pred_response": "Yes", "answer_id": "Kc8Ticibrw4dZHKassT8k4", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 825, "prompt": "Do the organs in the image exist in the neck?\n<image>", "pred_response": "Yes", "answer_id": "UbRfa3hPbV7G7xcY3rR3aw", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 826, "prompt": "Where is the brain edema located?\n<image>", "pred_response": "Left", "answer_id": "iyssagPKfLYxDyUmGbzPfF", "gt_response": "Left Lobe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 827, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Brain", "answer_id": "jKsySop9GBmkDuo9ephBcc", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 828, "prompt": "Is the brain normal appearing?\n<image>", "pred_response": "Yes", "answer_id": "ankiAxTKvP8mAsNeDu6Uh4", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 829, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "Taking an image of the brain, as shown in the image, can cause significant damage to the body. The image shows the inside of the skull, which is the bony structure that protects the brain. The bone is hard and can cause injury to the delicate tissues of the brain, such as the blood vessels, nerves, and brain cells, if the bone were to come into contact with the brain. This can lead to severe consequences, including brain damage, loss of function, and even death. It is essential to use appropriate imaging techniques and protective measures to minimize the risk of injury", "answer_id": "JeYLCAhV5QfYwkD6jsjEub", "gt_response": "None", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 830, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "XCTSBqX8mnUhSPPbAFnenS", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 831, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "8Gf3XMS99iAqJpFtxsXVR4", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 832, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "The mr weighting in this image is 0.", "answer_id": "HXS5LKv2Mz8j25mBRqJ55n", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 833, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Brain", "answer_id": "BtZXXQtEj4NeTUAND6cHmM", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 834, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "ZpPxHqX6F4pmj9QXG5x27S", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 835, "prompt": "Does the picture contain brian?\n<image>", "pred_response": "Yes", "answer_id": "ek4JHV5EP6ZPJQiArnFmfW", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 836, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain", "answer_id": "Tg5rVLNAFdXY2fohhWgNfg", "gt_response": "None", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 837, "prompt": "Does the brain look abnormal?\n<image>", "pred_response": "Yes", "answer_id": "o7iM7sGCXj87eWKjzXVpRp", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 838, "prompt": "Is this image taken via MRI?\n<image>", "pred_response": "Yes", "answer_id": "doQnxdqd5kp2XbXU4KKuxQ", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 839, "prompt": "Which kind of weighting is this image generated with, t1 or t2?\n<image>", "pred_response": "T2", "answer_id": "N4bPiSafD3pMx9qYQoQJdy", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 840, "prompt": "Is this a transverse section?\n<image>", "pred_response": "Yes", "answer_id": "gt2GnfZ6WEAaYVXLvJ6efE", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 841, "prompt": "Which part do the organs in this image locate from the aspect of the whole body?\n<image>", "pred_response": "Left", "answer_id": "ioKRA98h4AAkXSQLCZwLEo", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 842, "prompt": "Where are the abnormalities in this image?\n<image>", "pred_response": "The abnormalities in this image are the white spots on the brain, which are visible in the CT scan.", "answer_id": "nYdqgkDttBA2uNgJWsJXYz", "gt_response": "Lower Left Lobe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 843, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Brain", "answer_id": "KTMTHNk6RRar8XJ77xCdZe", "gt_response": "Head", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 844, "prompt": "What disease is/are shown on the left of brain?\n<image>", "pred_response": "Brain", "answer_id": "LGfA9s3ETwVJ8i9WGNQGFJ", "gt_response": "Brain Edema", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 845, "prompt": "How many kinds of abnormalities are there in this image?\n<image>", "pred_response": "0", "answer_id": "d5NJT48pWfV4o8kWYvsZdk", "gt_response": "1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 846, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "Taking an image of the brain, as seen in the CT scan, can cause some damage to the body. The CT scan uses ionizing radiation, which can be harmful to the body, especially when the exposure is high. The risk of damage depends on the dose of radiation, the duration of the exposure, and the specific type of CT scan being performed. In general, the risk of radiation-induced damage is relatively low when the exposure is kept to the minimum necessary for the diagnosis, and the patient follows the recommended guidelines for radiation protection. However, it is essential to be aware of", "answer_id": "AhqymQZTp4QZjhGBuXoW62", "gt_response": "None", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 847, "prompt": "Which is the kidney in this image?\n<image>", "pred_response": "Left", "answer_id": "K28XcC6yqqZ86dYozNtaRh", "gt_response": "Symmetrical to the bottom spine", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 848, "prompt": "What organ is the gray part on the left of the image?\n<image>", "pred_response": "Liver", "answer_id": "KZHR25P4uiU6GH6sor3cF5", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 849, "prompt": "Does the stomach appear in the image?\n<image>", "pred_response": "No", "answer_id": "To2a3ncW4LjfnLxV75jcdC", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 850, "prompt": "Does the picture contain spleen?\n<image>", "pred_response": "No", "answer_id": "nV4Q6TdCs4rY9HXAs5xvgP", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 851, "prompt": "What color is the spleen in the picture?\n<image>", "pred_response": "Gray", "answer_id": "DZ3r6WxJSPgbhxjXFYCpeR", "gt_response": "Gray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 852, "prompt": "Which is smaller in this image,liver or right kidney?\n<image>", "pred_response": "Right kidney", "answer_id": "nxuDJyiYy4nFd7nw7NdTzD", "gt_response": "Right Kidney", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 853, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "5", "answer_id": "GWUaxu5EeLXhPq4yZSuzor", "gt_response": "6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 854, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "0", "answer_id": "eacNtfWho7RGjRgAg4ESnn", "gt_response": "Much", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 855, "prompt": "Which organs/organ is part of the lymphatic system in this image?\n<image>", "pred_response": "Spleen", "answer_id": "877vo3TAoTGxKqPAGEFVqS", "gt_response": "Spleen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 856, "prompt": "Which organs/organ is part of the urinary system in this image?\n<image>", "pred_response": "Kidney", "answer_id": "3k4vJKVXV5LAJq9G9qgtjB", "gt_response": "Kidney", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 857, "prompt": "Does the picture contain the organ which has the effect of discharging waste?\n<image>", "pred_response": "Yes", "answer_id": "7MGmQP5jhJcb9S5HgUBLwS", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 858, "prompt": "Do any of the organs in the picture belong to the respiratory system?\n<image>", "pred_response": "No", "answer_id": "JqJho3oVAChYq4HQWXwDef", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 859, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse", "answer_id": "kp2egK2wz9G5xfTixuQAQz", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 860, "prompt": "Where is the colon in this image?\n<image>", "pred_response": "Left", "answer_id": "SLzoYa3oLneGDXhigUd8GP", "gt_response": "Upper Right", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 861, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "6t4xQnyCff7s7ydg2RkJEo", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 862, "prompt": "Does the trachea exist in this picture?\n<image>", "pred_response": "No", "answer_id": "QkcxnVwaDZyRVXUhX37npr", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 863, "prompt": "Does the picture contain bladder?\n<image>", "pred_response": "No", "answer_id": "7LXfqpwBNRuoXt4bMtCh62", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 864, "prompt": "Which is smaller in this image,liver or lung?\n<image>", "pred_response": "Liver", "answer_id": "d35FKZ4aXbuCSmbVUJ2XyF", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 865, "prompt": "How many lungs have existed in this image?\n<image>", "pred_response": "0", "answer_id": "nwZuGtT2txJWKgW25hcL6T", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 866, "prompt": "What is the effect of the organ on the upper left of this image?\n<image>", "pred_response": "Liver", "answer_id": "7DNKksTfPGMMpjPN6aXe5V", "gt_response": "Biotransformation, detoxification", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 867, "prompt": "Which organs/organ in the picture belong to the respiratory system?\n<image>", "pred_response": "Lung", "answer_id": "diDCQvN443hRaH5yop3ygz", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 868, "prompt": "Does the picture contain the organ that could be used for secreting enzymes?\n<image>", "pred_response": "Yes", "answer_id": "FUfQLr2WpfPRNDYC3Nj3KF", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 869, "prompt": "Do any of the organs in the picture belong to the digestive system?\n<image>", "pred_response": "Yes", "answer_id": "LVHUHViVSj6mfkzFk2xJJV", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 870, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Liver", "answer_id": "UtnTFBkRETwm7jD7sdW89Z", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 871, "prompt": "Does the larynx appear in the image?\n<image>", "pred_response": "No", "answer_id": "8uznUQZ5VQDqXnnRSzDDXr", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 872, "prompt": "Does the lung exist in this picture?\n<image>", "pred_response": "No", "answer_id": "ebtAYpmzvud6fufyr7mAsr", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 873, "prompt": "Which is smaller in this image,colon or liver?\n<image>", "pred_response": "Colon", "answer_id": "6fXXmYBGZ58szXSwWsG447", "gt_response": "Colon", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 874, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "0", "answer_id": "hyBVgbrMSpZpAssFeXcnhe", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 875, "prompt": "What is the function of the organs appearing in pairs in the picture?\n<image>", "pred_response": "The organs appearing in pairs in the picture are the liver and the spleen.", "answer_id": "aAmV3Ukjv8tFmvgyubRPjZ", "gt_response": "Breathe", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 876, "prompt": "Which organs/organ is part of the digestive system in this image?\n<image>", "pred_response": "Liver", "answer_id": "bfYz7bFMmJ26ZDdApnq3aB", "gt_response": "Stomach, Colon", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 877, "prompt": "Which organs/organ in the picture belong to the urinary system?\n<image>", "pred_response": "Kidney", "answer_id": "kiDKqFTAonMndYFwGhrqGz", "gt_response": "Kidney", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 878, "prompt": "Does the picture contain the organ which has the effect of detoxicating?\n<image>", "pred_response": "Yes", "answer_id": "TMYGRAQz82HuUPsJPDQunJ", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 879, "prompt": "Does the picture contain the organ which has the effect of adjusting water and osmotic pressure balance?\n<image>", "pred_response": "Yes", "answer_id": "8iN7C9ExKBgLRPmUYrSNwB", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 880, "prompt": "What organ is the gray part on the right of the image?\n<image>", "pred_response": "Liver", "answer_id": "gh2bpUKJyyhVCZuQuDDmrP", "gt_response": "Small Bowel", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 881, "prompt": "Does the picture contain colon?\n<image>", "pred_response": "Yes", "answer_id": "BSDcxASu9A7vspE9rtfS9N", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 882, "prompt": "Does the picture contain brain stem?\n<image>", "pred_response": "No", "answer_id": "QUxt6zyTv6PjH3ybgzB5Ax", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 883, "prompt": "Which is smaller in this image,kidney or small bowel?\n<image>", "pred_response": "Small bowel", "answer_id": "APZJHpkaZAz7Jwpde4UDgq", "gt_response": "Kidney", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 884, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "0", "answer_id": "9uGqeJV6vTaRRxHo58aSYj", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 885, "prompt": "What is the effect of the organ on the top of this image?\n<image>", "pred_response": "The organ on the top of the image is the bladder.", "answer_id": "DMT8ZufyRAFtoKDrqvzKAg", "gt_response": "Absorb nutrients, secrete enzymes, digest food", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 886, "prompt": "Are there some organs controlling heartbeat and breathing in this image?\n<image>", "pred_response": "No", "answer_id": "YBzDBvgRPMBVny94aYY45v", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 887, "prompt": "Does the picture contain the organ which has the effect of digesting food?\n<image>", "pred_response": "Yes", "answer_id": "cKBEi6sB4rrKGR3d3Jvfkw", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 888, "prompt": "Which is the kidney in this image?\n<image>", "pred_response": "Left", "answer_id": "QGxCpMasuQycs3wwTo9aPU", "gt_response": "Symmetrical to the bottom spine", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 889, "prompt": "What organ is the black part on the upper left of the image?\n<image>", "pred_response": "Liver", "answer_id": "fg8zo34Hcines9nw39cJom", "gt_response": "Small Bowel", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 890, "prompt": "Does the rectum appear in this picture?\n<image>", "pred_response": "No", "answer_id": "b6ouJKXWGd3FP59S5hkhd7", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 891, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "jE95Howcyp3745pB6gBu4f", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 892, "prompt": "What color does the left kidney show in the picture?\n<image>", "pred_response": "Gray", "answer_id": "5ATmrgKf3AYqJtkfgYHMav", "gt_response": "Gray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 893, "prompt": "Which is smaller in this image, liver or right kidney?\n<image>", "pred_response": "Right kidney", "answer_id": "YPrcGReMuV6VrfkZxmjixg", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 894, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "1", "answer_id": "KjWpbzneXg2NCJ62BbPyDm", "gt_response": "5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 895, "prompt": "What is the shape of spinal cord in this image?\n<image>", "pred_response": "Spiral", "answer_id": "fNH6T8mucHEssGqM39DVJG", "gt_response": "Circular", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 896, "prompt": "What is the effect of the organ on the top of this image?\n<image>", "pred_response": "The organ on the top of the image, the liver, is responsible for the production of bile, which is essential for the digestion of fats in the small intestine.", "answer_id": "VXUo4TqTCGcidXbUoSNVW9", "gt_response": "Absorb nutrients, secrete enzymes, digest food", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 897, "prompt": "Which organs/organ is part of the nervous system in this image?\n<image>", "pred_response": "Spinal cord", "answer_id": "VYGGwsRMegg8MMaWTXVDeV", "gt_response": "Spinal cord", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 898, "prompt": "Does the picture contain the organ which has the effect of digesting food?\n<image>", "pred_response": "Yes", "answer_id": "6HMcKgtpZ6nnLrpGyKJKbo", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 899, "prompt": "Does the picture contain the organ that could be used for secreting enzymes?\n<image>", "pred_response": "Yes", "answer_id": "SdcgFYwKCoB39QpuqfKruM", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 900, "prompt": "Where is the liver in this image?\n<image>", "pred_response": "Left", "answer_id": "VkMG92bAn2nRzf4Lyq658h", "gt_response": "Left", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 901, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "Yes", "answer_id": "k4U8s78zd3CsY3z5WuLxm7", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 902, "prompt": "Does the picture contain colon?\n<image>", "pred_response": "Yes", "answer_id": "45DHD3Qz9wEsSetnXscsQu", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 903, "prompt": "Does the humerus head exist in this picture?\n<image>", "pred_response": "No", "answer_id": "boCewXdZKxgYqw5cJRnKNR", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 904, "prompt": "What color does the duodenum show in the picture?\n<image>", "pred_response": "Gray", "answer_id": "cfDGaBpFmFNYqcVfMyEmfP", "gt_response": "Gray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 905, "prompt": "Which is smaller in this image, spleen or kidney?\n<image>", "pred_response": "Spleen", "answer_id": "92jHgzoy6YuXXQjvWdi7Kf", "gt_response": "Spleen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 906, "prompt": "How many lungs have existed in this image?\n<image>", "pred_response": "0", "answer_id": "2p5NoLsR8EqRm5v5fnmFBR", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 907, "prompt": "What is the shape of spleen in the picture?\n<image>", "pred_response": "Oval", "answer_id": "2MCZMogeTKTz9jCh4PJg5u", "gt_response": "Oval", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 908, "prompt": "What is the function of the organ on the center of this image?\n<image>", "pred_response": "Digest", "answer_id": "bGteULsPnVufgVFMoTcqrR", "gt_response": "Absorb nutrients, secrete enzymes, digest food", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 909, "prompt": "Which organs/organ is part of the nervous system in this image?\n<image>", "pred_response": "Spinal cord", "answer_id": "oYuqoTKcfKnxEjeZFJCjZA", "gt_response": "Spinal cord", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 910, "prompt": "Are there organs in the picture that can adjust water and osmotic pressure balance?\n<image>", "pred_response": "Yes", "answer_id": "hHhBByofXtYdMriDg5gEgc", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 911, "prompt": "Are there any organs used for breathing in this image?\n<image>", "pred_response": "No", "answer_id": "RESZMGVcJBuuWbuKPRAMfd", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 912, "prompt": "Is this a transverse section?\n<image>", "pred_response": "Yes", "answer_id": "72xMEFHZQn2xZBD5yGjuU7", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 913, "prompt": "Where is the stomach in this image?\n<image>", "pred_response": "Left", "answer_id": "arnKBNFvi3bFytM9rzJJnC", "gt_response": "Upper Right", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 914, "prompt": "Which organ system is imaged?\n<image>", "pred_response": "Liver", "answer_id": "88v9ic7vvJvLGCU4pMZUkQ", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 915, "prompt": "Does the picture contain eyes?\n<image>", "pred_response": "No", "answer_id": "GSDPuXwGZTCxVBrnxvSoh3", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 916, "prompt": "Is there femoral head in this picture?\n<image>", "pred_response": "No", "answer_id": "fo9F4qXy7iCYrrANE6aZug", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 917, "prompt": "Which is bigger in this image,liver or stomach?\n<image>", "pred_response": "Liver", "answer_id": "jRjPZRMVbAi5DUsrrmm3Ez", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 918, "prompt": "How many kidneys have existed in this image?\n<image>", "pred_response": "0", "answer_id": "5E6ySxgMoLC8PGuMVBS4vL", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 919, "prompt": "What is the shape of spinal cord in this image?\n<image>", "pred_response": "Spiral", "answer_id": "KDzbiDbA72UK4XcsatbRhs", "gt_response": "Circular", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 920, "prompt": "What is the function of the rightmost organ in this picture?\n<image>", "pred_response": "Digestion", "answer_id": "jyVxn4magxE8Ce5uUTeDrK", "gt_response": "Improve the body's immunity", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 921, "prompt": "Which organs/organ in the picture belong to the lymphatic system?\n<image>", "pred_response": "Spleen", "answer_id": "gc946SG4AZFj5FCSoJaKaN", "gt_response": "Spleen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 922, "prompt": "Does the picture contain the organ that could receive sound?\n<image>", "pred_response": "Yes", "answer_id": "LyW7YcsT3N4VKXMmEez8Tu", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 923, "prompt": "Does the picture contain the organ which has the effect of gas delivery?\n<image>", "pred_response": "Yes", "answer_id": "TnyoCtGeQNWXntnuQTY9Ah", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 924, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse", "answer_id": "mppgaEE4NX2X4wJNfXwrFS", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 925, "prompt": "Where is the esophagus in this image?\n<image>", "pred_response": "Left", "answer_id": "n2fW6anhhXz4gJgKHGENGG", "gt_response": "Center", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 926, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Liver", "answer_id": "hmR6hJto8qADE7XWuescMm", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 927, "prompt": "Does the esophagus exist in this picture?\n<image>", "pred_response": "No", "answer_id": "6mWGsRPQ8tMK6Aze7PM3CZ", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 928, "prompt": "Does the stomach appear in the image?\n<image>", "pred_response": "No", "answer_id": "bz29Jrxqyk2Ahf62njXMb3", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 929, "prompt": "What color is the spleen in this image?\n<image>", "pred_response": "Gray", "answer_id": "fTGmzcZWEb5miGoaZxTs58", "gt_response": "Gray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 930, "prompt": "Which is the biggest in this image, spleen, heart or liver?\n<image>", "pred_response": "Liver", "answer_id": "he6L7tYmTpc5sp6HuTiCWE", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 931, "prompt": "How many existing heart in this image?\n<image>", "pred_response": "0", "answer_id": "ZMVJfGtXZJJng8eG9ofmvH", "gt_response": "1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 932, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "The image shows the inside of the body, specifically the liver, which is a vital organ. The liver is responsible for various functions such as detoxification, nutrient processing, and immune system support. The image is a CT scan, which is a medical imaging technique that uses X-rays to produce detailed images of the body's internal structures. While the CT scan provides valuable information about the body's internal structures, the image itself does not cause any direct damage to the body. However, the CT scan does involve the use of ionizing radiation, which can have potential health risks if the", "answer_id": "f8k2rPwwNNKgQd9Kiwuf7e", "gt_response": "Much", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 933, "prompt": "Are there organs in the picture that can digest food?\n<image>", "pred_response": "Yes", "answer_id": "crLkzqTWvq3meeKe6TioWW", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 934, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "3TjGkcs42CmcjkQBPe5Ecw", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 935, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "a6MDgbKRZtuBJv9iLvAtAn", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 936, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "The mr weighting in this image is 0.", "answer_id": "9bgann53w6LfquMRA34jpL", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 937, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Brain", "answer_id": "QUJfYwGCEzbespeGMN4ytE", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 938, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "FYt5rCghYjr2MHJtpqeac4", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 939, "prompt": "Does the picture contain brian?\n<image>", "pred_response": "Yes", "answer_id": "QC5J7tX2JuoqF5kw5DubPX", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 940, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "The image shows a CT scan of the brain, which is a medical imaging technique used to diagnose and monitor various brain diseases and conditions.", "answer_id": "a3Ybi3RUWZ8aGcCFNyqU4m", "gt_response": "Brain Tumor, Brain Edema", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 941, "prompt": "Is the brain healthy?\n<image>", "pred_response": "Yes", "answer_id": "HHkZ6gv5ass9a8Yf5tiPeg", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 942, "prompt": "Where is/are the abnormality located?\n<image>", "pred_response": "Brain", "answer_id": "iKmLwTY2kMTMcCZukMhcjt", "gt_response": "Lower Left", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 943, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "4VHmExitfq3q9VpjD35JSU", "gt_response": "Abdomen", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 944, "prompt": "Where is the heart in this image,left,top or upper left?\n<image>", "pred_response": "Upper Left", "answer_id": "fhrJrPzGoPhUDf7YaFQqNJ", "gt_response": "Top", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 945, "prompt": "Is this a study of the abdomen?\n<image>", "pred_response": "No", "answer_id": "B487rZpiUfuPza4BpnELvU", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 946, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "Yes", "answer_id": "VzwmUatWrLLx9YpZBYjGZM", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 947, "prompt": "Does the picture contain stomach?\n<image>", "pred_response": "No", "answer_id": "EctyaPNJh874ufxTfBXZND", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 948, "prompt": "Which is smaller in this image, esophagus or liver?\n<image>", "pred_response": "Esophagus", "answer_id": "eyTraUE9qw6T33QgZABdSH", "gt_response": "Esophagus", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 949, "prompt": "How many lungs have existed in this image?\n<image>", "pred_response": "2", "answer_id": "BgSDHgpja5BBvMbMCpWCEE", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 950, "prompt": "How much damage will be caused to the body taking such an image?\n<image>", "pred_response": "0", "answer_id": "bHkwmT3PusrXc9Bk76bD5C", "gt_response": "Much", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 951, "prompt": "Which organs/organ in the picture belong to the digestive system?\n<image>", "pred_response": "Stomach", "answer_id": "fZ3uQiW6rZ4iWA7fkNY7UN", "gt_response": "Esophagus", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 952, "prompt": "Does the picture contain the organ which has the effect of gas delivery?\n<image>", "pred_response": "Yes", "answer_id": "YbYx3oGMk63Dbfs73bKZ9a", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 953, "prompt": "Does the picture contain the organ that could promote blood flow?\n<image>", "pred_response": "Yes", "answer_id": "P9quCVwKURiboDJ423Qawh", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 954, "prompt": "In what plane is this image scanned?\n<image>", "pred_response": "Transverse", "answer_id": "UvqGf8ktEenrkRSvmYSbqs", "gt_response": "Transverse Plane", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 955, "prompt": "What is the leftmost organ in this image?\n<image>", "pred_response": "Heart", "answer_id": "AP8aazTnVRyUoDoRRc9PtD", "gt_response": "Right Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 956, "prompt": "Does the picture contain spinal cord?\n<image>", "pred_response": "No", "answer_id": "6884f9EZ5DPfNpQnRDEwxg", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 957, "prompt": "Does the rectum appear in this picture?\n<image>", "pred_response": "No", "answer_id": "DLX3unJ9vvv64zeSaMsCDc", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 958, "prompt": "What color is the lung in this image?\n<image>", "pred_response": "Gray", "answer_id": "SfimbAtVB2UrgfFaCxUkLi", "gt_response": "Black", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 959, "prompt": "Which is smaller in this image,heart or esophagus?\n<image>", "pred_response": "Heart", "answer_id": "aLzVCsyBrPdknVSZjVkPmr", "gt_response": "Esophagus", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 960, "prompt": "How many organs are there in this image?\n<image>", "pred_response": "1", "answer_id": "Pw6jFRrj5fmEKGVbY9A73h", "gt_response": "4", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 961, "prompt": "What is the function of the organ on the top of this image?\n<image>", "pred_response": "Heart", "answer_id": "DcPYYkfwCZ8SMmgcoi9j6q", "gt_response": "Promote blood flow", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 962, "prompt": "Does the picture contain the organ that could receive sound?\n<image>", "pred_response": "Yes", "answer_id": "npQhjTFyAATEFYX86eZTFK", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 963, "prompt": "Does the picture contain the organ which has the effect of sensing light?\n<image>", "pred_response": "No", "answer_id": "3feFXpDgu36QzKTvk5HRL4", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 964, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "cvwwWDrrFgmjCABErSa6by", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 965, "prompt": "Where does the image represent in the body?\n<image>", "pred_response": "Pelvic", "answer_id": "AV7LsBpmGW4SdwNsABAHfp", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 966, "prompt": "Where is the rectum existing in this image?\n<image>", "pred_response": "Left", "answer_id": "UcuRmh5m2ZnMJ5EYgARSj5", "gt_response": "Bottom", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 967, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Urogenital", "answer_id": "A4Bm55M8WLQU3mNfwawafW", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 968, "prompt": "Does the picture contain bladder?\n<image>", "pred_response": "No", "answer_id": "ADYbzFGteyGBsKT7h3wRHN", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 969, "prompt": "Does the parotid appear in the image?\n<image>", "pred_response": "No", "answer_id": "GsvdyPxyQLCsAURSJzaCCK", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 970, "prompt": "Which is smaller in this image, bladder or rectum?\n<image>", "pred_response": "Rectum", "answer_id": "2Ua96pgjndcHJ9bnY2ujnR", "gt_response": "Rectum", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 971, "prompt": "How many femoral heads are shown in this image?\n<image>", "pred_response": "0", "answer_id": "ePqzMtrDPEvYYNvz9kdYnX", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 972, "prompt": "What is the function of the organ on the bottom of this picture?\n<image>", "pred_response": "The function of the organ on the bottom of the picture is to store and excrete urine.", "answer_id": "hLWmGTxzDQ6xGUiq6MhgSu", "gt_response": "Store feces, excrete feces", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 973, "prompt": "Which organs/organ in the picture belong to the digestive system?\n<image>", "pred_response": "Rectum", "answer_id": "fFLYrDkY5VbJTW2bgUPpoM", "gt_response": "Rectum", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 974, "prompt": "Does the picture contain the organ which has the effect of discharging waste?\n<image>", "pred_response": "Yes", "answer_id": "6eDJEqSgtyQ2YBwXHRLnjH", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 975, "prompt": "Does the picture contain the organ which has the effect of excreting feces?\n<image>", "pred_response": "Yes", "answer_id": "Pg24XbJRoXPVjqqUKoTRem", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 976, "prompt": "Is this a coronal section?\n<image>", "pred_response": "Yes", "answer_id": "eqLcXqC6sPSga5GF6Xdgo3", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 977, "prompt": "Which part do the organs in this image locate from the aspect of the whole body?\n<image>", "pred_response": "Left", "answer_id": "PFYsG5jzK6aMvjX77QeyUu", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 978, "prompt": "Where is rectum in this image?\n<image>", "pred_response": "Left", "answer_id": "2LJyTZfBwEJvaxTVb8QwMu", "gt_response": "Bottom", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 979, "prompt": "What is the organ system visualized?\n<image>", "pred_response": "Pelvic", "answer_id": "Tt5ZJbYjAKnM35hwM8XQKz", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 980, "prompt": "Does the picture contain rectum?\n<image>", "pred_response": "No", "answer_id": "mGiJ6ehkCyx7FGnvN8qwdw", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 981, "prompt": "Does the picture contain bladder?\n<image>", "pred_response": "No", "answer_id": "PdW53jS7jJyaEQpuoYVfux", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 982, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Bladder", "answer_id": "ggQ8wEGc4TUGcQz9poxAhm", "gt_response": "Bladder", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 983, "prompt": "What is the effect of the gray organ on the center of this picture?\n<image>", "pred_response": "The gray organ in the center of the picture is the rectum, which is the last part of the large intestine.", "answer_id": "YFzyqg76pCp5WiUZ2dGMWJ", "gt_response": "Store urine", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 984, "prompt": "Which organs/organ is part of the digestive system in this image?\n<image>", "pred_response": "Rectum", "answer_id": "2YjGRZixtTPuskcXuaDo7E", "gt_response": "Rectum", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 985, "prompt": "Does the picture contain the organ which has the effect of excreting feces?\n<image>", "pred_response": "Yes", "answer_id": "BMV2M3NAL3L2ee8k9hapdL", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 986, "prompt": "Does the picture contain the organ which has the effect of detoxicating?\n<image>", "pred_response": "Yes", "answer_id": "K6r7iL9KnwnF6hgTdnEW5m", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 987, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "2y5XsRmEi94FsSCStV49zS", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 988, "prompt": "Is this a study of the pelvic cavity?\n<image>", "pred_response": "Yes", "answer_id": "Fzzfqq5WXdxLSPrMceXvqT", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 989, "prompt": "Does the picture contain colon?\n<image>", "pred_response": "Yes", "answer_id": "3TRdhApKY5kBuwhUBdWgGd", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 990, "prompt": "Which organs/organ in the picture belong to the urinary system?\n<image>", "pred_response": "Bladder", "answer_id": "NUVKpaytiLg3dtkWYS5v4E", "gt_response": "Bladder", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 991, "prompt": "Are there organs in the picture that can deliver food?\n<image>", "pred_response": "No", "answer_id": "NdU6V6EoSLnMNSM2BU2k88", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 992, "prompt": "How was this image taken?\n<image>", "pred_response": "CT", "answer_id": "MEXV9ox8iPALVxq5h926Ch", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 993, "prompt": "Where does the picture show?\n<image>", "pred_response": "Abdomen", "answer_id": "AyiXCJkudFWisD3ZDySasw", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 994, "prompt": "Does the rectum exist in this picture?\n<image>", "pred_response": "Yes", "answer_id": "5xmaE5g5t3YHdaQW2fQipq", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 995, "prompt": "Which is bigger in this image, small bowel or colon?\n<image>", "pred_response": "Colon", "answer_id": "aAmHfn6Vg9zX6Vq2PvnBuB", "gt_response": "Small Bowel", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 996, "prompt": "What is the function of the organ on the bottom of this image?\n<image>", "pred_response": "The function of the organ on the bottom of the image is to store and process food.", "answer_id": "ND6sGLz525egSnUBPeRedz", "gt_response": "Digest food, absorb water, excrete body waste", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 997, "prompt": "Do any of the organs in the picture belong to the urinary system?\n<image>", "pred_response": "Yes", "answer_id": "eRH6BHoyMCTLkjGVTGjZzM", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 998, "prompt": "What organ system is pictured?\n<image>", "pred_response": "Digestive", "answer_id": "WDjRG4ikAJVrDC9v3xQtst", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 999, "prompt": "Is there colon in the image?\n<image>", "pred_response": "Yes", "answer_id": "ieB7scua6ZdTk38c9S6xU4", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1000, "prompt": "Which is smaller in this image, colon or small bowel?\n<image>", "pred_response": "Small bowel", "answer_id": "4HS29edYKxghqFk2B5ptoZ", "gt_response": "Colon", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1001, "prompt": "What is the function of the organ on the top of this image?\n<image>", "pred_response": "The function of the organ on the top of this image is to store and process food.", "answer_id": "SCPFQ7Rs8RAF3qdoeRfnxa", "gt_response": "Absorb nutrients, secrete enzymes, digest food", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1002, "prompt": "Do any of the organs in the picture belong to the digestive system?\n<image>", "pred_response": "Yes", "answer_id": "cFVyhZuQBmtZyzCAiyQVdV", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1003, "prompt": "Is this a coronal plane?\n<image>", "pred_response": "Yes", "answer_id": "5kGuqSMeZQmXrkze9DP44e", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1004, "prompt": "Which part of the human body is the organ located in the image?\n<image>", "pred_response": "Abdomen", "answer_id": "mqRwpoQu9GA2tbL6pienGy", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1005, "prompt": "Where is colon in this image?\n<image>", "pred_response": "Left", "answer_id": "LnQHJeXXWyHLzgieXSXTYY", "gt_response": "Upper Left", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1006, "prompt": "What organ system is primarily present in this image?\n<image>", "pred_response": "Digestive", "answer_id": "JuekuKFJtMVMTsxNodpYPp", "gt_response": "Pelvic Cavity", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1007, "prompt": "Is there spinal cord in this picture?\n<image>", "pred_response": "No", "answer_id": "9RQyrit6YWzKwGUxLP9r4r", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1008, "prompt": "What is the effect of the organ on the left of this picture?\n<image>", "pred_response": "The organ on the left, the liver, is responsible for the production of bile, which is essential for the digestion of fats in the small intestine.", "answer_id": "LJunmnNsm3XWC89mFNE4Km", "gt_response": "Digest food, absorb water, excrete body waste", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1009, "prompt": "Which organs/organ is part of the digestive system in this image?\n<image>", "pred_response": "Liver", "answer_id": "4xkNnPgg3qkB5rRWGdXFjA", "gt_response": "Colon, Small Bowel", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1010, "prompt": "Are there organs in the picture that can promote blood flow?\n<image>", "pred_response": "Yes", "answer_id": "Yv49hqQDr9rSQXA828giv7", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1011, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "MXzm4g4aJb5NwnVV8xKhRx", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1012, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "fufLWucqDMAwRam2mnLTNk", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1013, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "0", "answer_id": "5U6bB4BX7Hbgu2REVVqphm", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1014, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Brain", "answer_id": "mAZT9tAFn6rGtiGYYp4Rd6", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1015, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "Y3RuRd5UKXXdFeTw3WzqVQ", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1016, "prompt": "Does the picture contain brian?\n<image>", "pred_response": "Yes", "answer_id": "8iFb9wPCrzbfAbGWqK8rin", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1017, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain", "answer_id": "WHZCtQy7hVWywzwSEvYRSH", "gt_response": "None", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1018, "prompt": "Is the brain healthy?\n<image>", "pred_response": "Yes", "answer_id": "id2XLaeHGaTLL3d4WoNQSQ", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1019, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "e42dVKfpcZLuVPfcZf9r4X", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1020, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "ftsBgEMetjBecsz2mtAuzq", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1021, "prompt": "What is the mr weighting in this image?\n<image>", "pred_response": "0", "answer_id": "k3DZYz3ieA42ryXDUN6jxL", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1022, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Brain", "answer_id": "4EYqLfaZogMFdhdPaMid45", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1023, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Brain", "answer_id": "SWXgjvfTAcR45LXMXSd8aS", "gt_response": "Brain", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1024, "prompt": "Does the picture contain brian?\n<image>", "pred_response": "No", "answer_id": "Cnh7HU9BEcqYVGs6Z3FyWm", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1025, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Brain", "answer_id": "TdEQiSCoP93Z6R7hgArc7c", "gt_response": "None", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1026, "prompt": "Is the brain healthy?\n<image>", "pred_response": "Yes", "answer_id": "iRXzqVqbkYt5PLJdjw7V2H", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1027, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "5ydT49x9QWyqGKSuiSVyDu", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1028, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "Gbf2wWbgUhVq7DWGmaoskH", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1029, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "itrcWqVAwPcGvaXkKxjBD2", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1030, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "EotepqaEstUWVYPviBgg2M", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1031, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "oGbgecAxFXZowQ93ZqGvhM", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1032, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "Urt6erFheyzgK2aLhQi3Ai", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1033, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "GLQA2mLZVbAgNsXRWwFoxZ", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1034, "prompt": "What diseases are included in the picture?\n<image>", "pred_response": "Lung Cancer", "answer_id": "3Q6wp7jmrALDJq8GGxVY8z", "gt_response": "None", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1035, "prompt": "Is the lung healthy?\n<image>", "pred_response": "Yes", "answer_id": "FKEgg9epHR7q2gqubUSuHp", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1036, "prompt": "Which is the biggest in this image,lung,liver or heart?\n<image>", "pred_response": "Lung", "answer_id": "KZjXunubJF8VWJxwdhMeJw", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1037, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "ANNccxWNxuXjSb5efLL8hv", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1038, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Abdomen", "answer_id": "NpVN2NF2ydq4rbcWk96ef6", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1039, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Liver", "answer_id": "YfM63e6QBu427pqBEm5Lex", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1040, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Liver", "answer_id": "F8vq6Zw5v3yZBPxH4q5463", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1041, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "Yes", "answer_id": "mHZxShfgQ27Ap8kjXKqmEu", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1042, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "TbajEHyGToStzBhL6FkHGH", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1043, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "HaQHCqscnHJoD7JwAGLvKw", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1044, "prompt": "Does the lung look healthy?\n<image>", "pred_response": "Yes", "answer_id": "iqmwLQGuZMsyjPCrwSgaQ7", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1045, "prompt": "Which is the biggest in this image,lung,liver or heart?\n<image>", "pred_response": "Lung", "answer_id": "HqMBqacZ8fcphsdcJfULEf", "gt_response": "Liver", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1046, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "knTZ68CzGbMkgD3uWhMPDA", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1047, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Chest", "answer_id": "GwciwuNm5mnPZHbp2P8e9C", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1048, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Lung", "answer_id": "Rj6cwueBxzoymTy2BF6N3L", "gt_response": "Lung, Heart", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1049, "prompt": "What is the largest organ in the picture?\n<image>", "pred_response": "Lung", "answer_id": "T97eU5jR5Wfm6EF7cA5aW7", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1050, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "GAJrqrn73oc8j3TPorvqSZ", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1051, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "Yes", "answer_id": "QFbqpcYKqXcDS8vSk6Pvxa", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1052, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "erpuBx2NuzsTUqiUF6rRWr", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1053, "prompt": "Is the lung healthy?\n<image>", "pred_response": "No", "answer_id": "CDZmPjLC9dxS83NXs5YCVy", "gt_response": "Yes", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1054, "prompt": "Which is the biggest in this image,lung,liver or heart?\n<image>", "pred_response": "Lung", "answer_id": "5CGkYoe7GW7bwk9ZXLV3ko", "gt_response": "Lung", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1055, "prompt": "What modality is used to take this image?\n<image>", "pred_response": "CT", "answer_id": "GojrsmxN6tLWDwmgHjTmM8", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1056, "prompt": "Which part of the body does this image belong to?\n<image>", "pred_response": "Head", "answer_id": "3kUuXW7y6bkEFYXJzipaH4", "gt_response": "Chest", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1057, "prompt": "What is the main organ in the image?\n<image>", "pred_response": "Neck", "answer_id": "ELCuxzXJ9DU6ZxordevDzT", "gt_response": "Spinal Cord", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1058, "prompt": "Does the picture contain liver?\n<image>", "pred_response": "No", "answer_id": "bTUvbQvzFRAdPuYYg54Jn8", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1059, "prompt": "Does the picture contain lung?\n<image>", "pred_response": "No", "answer_id": "VRcJ88fB8qJn4V8vnyxM3j", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1060, "prompt": "Does the picture contain heart?\n<image>", "pred_response": "No", "answer_id": "DX2utCJmYLPfCv2Ed4zXwE", "gt_response": "No", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1061, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "6PiK6Cu6HUvVwCda2975Bp", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1062, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "HxF47RXBycstLZpLBttZqC", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1063, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u8fd9\u5f20MRI\u56fe\u7247\u4f7f\u7528\u4e86\u5747\u503c\u6743\u91cd\u6cd5\u3002", "answer_id": "iJDTMtb54PuK5ko9Pg3kzn", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1064, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "aJHfqLrVPDHtQV6W8CXMeh", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1065, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "KSpTQcWT6JY2Zg7zBEGLt3", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1066, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "TyHD7GxMQzukmWnYAfZmbN", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1067, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "fde9X8m7sF8NDoBuKrm5Jr", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1068, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "Fx2Ztdkej2AoJWMayKjpLH", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1069, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "EFxXKgvQJNtJ5QAiVRX5mM", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1070, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "GygqVZ7YYHJRLGovHRJGUt", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1071, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "ZG47gzNe5hGHordWfEPp3a", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1072, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "ErVG2qpwJKSuAeZNAEw3JC", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1073, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "oSfmxZLRAfcqVfJadD2iQc", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1074, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Z5oSi3BK7xScVaRA4DUYto", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1075, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "KqRXvWBBWnHaj2L7GmY96u", "gt_response": "\u80ba\u764c", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1076, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "hWdJMobyeS6MfJRKanY9yA", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1077, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80ba", "answer_id": "4gyy6gxRxtQri2yveBEdjh", "gt_response": "\u53f3\u80ba, \u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1078, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "aS6Lr6XN4LiV9kwQmCYybh", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1079, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "QkJJDhDWMKC54Y7DbTGA5T", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1080, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u4e0d\u77e5\u9053", "answer_id": "NzB9zwfYRQAade9Jatwf6J", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1081, "prompt": "\u813e\u810f\u662f\u5426\u5b58\u5728\u5f02\u5e38?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "Ufzi79V6KRcPj9NprYRCx3", "gt_response": "\u5426", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1082, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Kiqa2G9s8geCDMJPK7qHt5", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1083, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "QsHe3a9pPihZy3RpkyGaDW", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1084, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "eYScm74cqo4uDHjBisRhg3", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1085, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "EbAvoQwdFhw67QWRbXaMC3", "gt_response": "\u53f3\u4e0b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1086, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "Ruio4GAeJFoJWbKZreCBwq", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1087, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "U4uQasKBxQmLc4b2B8Z2WN", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1088, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "gAWbuq9vGH5iSZpwnRtYKL", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1089, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "PRWXETGp2HfgVmkesW25Ri", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1090, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "iwjeL9W9iTYBTw74nzJ3br", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1091, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "HjutosLHUNPGNDTezyHyTu", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1092, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "5uqkGKBwvsnyxLs3Yb9dNN", "gt_response": "\u80ba\u764c", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1093, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "n5XLtSYjasLhD49xpAsFFU", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1094, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80ba", "answer_id": "PoULgQkzhbfUjS5AGZUZks", "gt_response": "\u53f3\u80ba, \u5de6\u4e0b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1095, "prompt": "\u56fe\u4e2d\u80ba\u3001\u5fc3\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u80ba", "answer_id": "ZCGyzf7SrsVjpHgq5yr5Pq", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1096, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "DvqunDjXMeurLWUbcD5qfZ", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1097, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "bCT9t5vPyxKeioUEMTrsGu", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1098, "prompt": "\u56fe\u4e2d\u5b58\u5728\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "DEsydLNvDhUF9TyYJWRWAB", "gt_response": "\u5b58\u5728", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1099, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "Y3gfUkhUncgPuRnVvA5LUE", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1100, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "9hMT5GzWbGvgpGf9o42iWg", "gt_response": "\u5fc3\u810f\u80a5\u5927", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1101, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "WyyKzJa7YtuydsZpRCVKas", "gt_response": "\u4e2d\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1102, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u662f\u4e0d\u6b63\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "4AJffi2DQYpTw9fJA2ReKK", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1103, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "4fNk3GeHDbTyjAqyMseTZH", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1104, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "GnH6UX2T7x9ag5EH535acT", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1105, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "E5YR2ZByX9o9AFoNNMTQr6", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1106, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "48xkocGZhHmv6wQt36nQ5y", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1107, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "QvwXaDF4VMGWhgP7SjBzEG", "gt_response": "\u5fc3\u810f\u80a5\u5927", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1108, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "Biytgc8XRnMCLfZkze6XTn", "gt_response": "\u4e2d\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1109, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "aTNjWGpvNNVJ5jdcgdADuX", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1110, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "2pXqBxuBzj3CDDk87kAqej", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1111, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "68D4mnDDEnC8CMfjRxuicQ", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1112, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "PnsTgTbmq8yoPdvV25Cz8W", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1113, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "bBYALGvhXyXhTLGQF4C7WJ", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1114, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "mHqLjoi5UxZ6BhomiJcfCM", "gt_response": "\u80ba\u4e0d\u5f20", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1115, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "DYVoM8hUEVYDQWi8cvCnDS", "gt_response": "\u5de6\u80ba,\u53f3\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1116, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "PM4hwBqf8kMXYirnpA8EhU", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1117, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "o3xBSF8KLN59EiSf8m3bka", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1118, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "niZCkWeYpdTpDst8tvMGAg", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1119, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "XW3N3WNSJshqB4WpNd5wAk", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1120, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "TEawGKPvGQUabA5M5vcr8W", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1121, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "eSMmTM9WFgk8Hk7FrDuPSM", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1122, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "TivziTb5BSpa7aKBh56NTw", "gt_response": "\u80ba\u7ed3\u8282", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1123, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80ba", "answer_id": "PQuuzDwF2gu9KiUrD659J5", "gt_response": "\u53f3\u80ba, \u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1124, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "EhstnCzKWCZ8yDz8NwbRJa", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1125, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "XB5k3rHtYxhb2QHUcbGihc", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1126, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "aJneSaTyknz3gjqHonQwaw", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1127, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "5uZGB44AdbHz89DaLCvyMZ", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1128, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "GjZ6BjcD6LwRe8eTKkKoC5", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1129, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "hGnbKbUSQMhZyfh8FUwprN", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1130, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "K5SC77ghkidgnLXGvZihoc", "gt_response": "\u80ba\u7ed3\u8282", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1131, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u53f3\u4fa7", "answer_id": "RWcRtQQ3ZYFniBh6pR2f2y", "gt_response": "\u53f3\u80ba, \u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1132, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "cgHqFdYHXLAiUYdu4Uq37K", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1133, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "BF6s4BH3y8S5YiPCHzTTLN", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1134, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8fd9\u5f20\u56fe\u7247\u662f\u4e00\u5f20\u80f8\u90e8X\u7ebf\u6210\u50cf\uff0c\u5b83\u663e\u793a\u4e86\u80f8\u8154\u548c\u80f8\u8154\u5185\u7684\u7ed3\u6784\u3002", "answer_id": "aC59B47FRqNQdiJfPHRUJB", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1135, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "LTi4oyvVJGx5iaQqNNXBFE", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1136, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "ajxShtHQYp5k6MxJ2pd9Hq", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1137, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "TzUTR3ceeG789isqdXeFoM", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1138, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "VsasTehEtqpL5BivQ5m9Ge", "gt_response": "\u80ba\u7ed3\u8282", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1139, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "hQD54vGx8SQiLAXCUcNHjL", "gt_response": "\u5de6\u80ba,\u53f3\u4e0a", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1140, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "CpAkLqyMYqs9sDVww6ZLmL", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1141, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "6KsNhDemsi96CsodjE43K6", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1142, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "hYGxm8r2CNrcBqa2fdqpQs", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1143, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "RUE34xCdXG6jAZMm86Z9qd", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1144, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "ZaVXUK4K2KY4rzBQYhbf9x", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1145, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "8g7f4nAJ9T9rovMbecyqfv", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1146, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u4e0d\u5305\u542b\u75be\u75c5", "answer_id": "3SDduDZ6qH6bY4cJvvFaSB", "gt_response": "\u80ba\u708e", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1147, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "acJoSNXNUyFhYV2TFCTMy9", "gt_response": "\u5de6\u80ba, \u53f3\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1148, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "ALr5g8RFPt8SiH9Y6QqjT5", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1149, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "CqMqkWenxBzBwjEb4nMerj", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1150, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "AFMSxL9Kg6tbXP8ZWyQPiV", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1151, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "MN93cpHndEP7wghFWpDVi9", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1152, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "nn8dJWTtS9SAnmwAzm3qpi", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1153, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "RCN3BSw3DnWwhxKKdMpvma", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1154, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u4e0d\u5305\u542b\u75be\u75c5", "answer_id": "AcLwn44dvwCDmyQMLgHvSE", "gt_response": "\u80ba\u80bf\u5757", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1155, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80ba", "answer_id": "fBXmk2xCG8AhiyyzNabmJs", "gt_response": "\u53f3\u80ba,\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1156, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "LxKBGTzFk5PXc6YdFtEmBf", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1157, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "oQ3HoSeQp9qzHEErTLqncv", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1158, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "FWqArVJciMA78RzMNmvG5T", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1159, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "ftygmRAc6FXG3noogDuTEZ", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1160, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "hP8WJwzk4Rauh8PemCiHzZ", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1161, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "NMaBLzf6JuYvpMwsfLhhUU", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1162, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "Lcfxgw6ZNNTEE8aRqZeDVm", "gt_response": "\u80ba\u7ed3\u8282", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1163, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "aEtianEkCrm2PVWCVAMEjG", "gt_response": "\u53f3\u80ba,\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1164, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "LYCfENC5sgNEmNxezEBrBK", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1165, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "earmmhG56be6cNBZDFaaqc", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1166, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8fd9\u5f20\u56fe\u7247\u662f\u4e00\u5f20\u80f8\u90e8X\u7ebf\u6210\u50cf\uff0c\u4e5f\u79f0\u4e3a\u80f8\u90e8CT\u3002", "answer_id": "eaxbAArDTmmr4GFqZPK2kN", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1167, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "EUkn6MW3JA39q7qfqob74i", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1168, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "oZApVdJeobhrFP9CKbAzu7", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1169, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "VnjNSWqt4KuLqg4RBaK5un", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1170, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "UB3UVNiQsFhATqnxq4xrKS", "gt_response": "\u6c14\u80f8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1171, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "HHcZ2LUmHmx5LCDzBqANGc", "gt_response": "\u53f3\u80ba,\u5de6\u4e0a", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1172, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "RrWjWuCJpxaB9x5WNmyCeJ", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1173, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "7ebAmrZxu7hMaLWDwVWVeU", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1174, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8fd9\u5f20\u56fe\u7247\u662f\u4e00\u5f20X\u5c04\u7ebf\u6210\u50cf\u3002", "answer_id": "gfoAofxrx5bgsPQdKgJp8N", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1175, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "5AwEREXCUBNQmq2jRFFS3G", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1176, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "UpJaxtm6yJf5cTT9EMXcy3", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1177, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "YjfPuiKQzHFjrEtWZ4uHnZ", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1178, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "jV5ZyzZPqigwwJixkYkSzz", "gt_response": "\u6c14\u80f8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1179, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "iD5D2c9imJ2qFKGdgbXeMK", "gt_response": "\u53f3\u80ba,\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1180, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "jnSCuK8ypiGdKKTu3oJTZD", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1181, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "6ivXRPtkDZAV5XRXEk2TQa", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1182, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "QMrjGrt7ZTA96NTfEbLgG9", "gt_response": "X-Ray", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1183, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "BFHHKwvvKEZcwtAjGu4Mgt", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1184, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "NkGR8xBtXVUZA7tqMfBino", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1185, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "hVETxo5yy6TJXYuBXP8THN", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1186, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "C3uTjefHBxiq4sqGhYufyy", "gt_response": "\u80ba\u4e0d\u5f20", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1187, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80f8\u90e8", "answer_id": "ZwQqBCkCecrZ3oPhaeqhR5", "gt_response": "\u5de6\u80ba,\u53f3\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1188, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "GHtwgsBo9Sm5kPia4icETj", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1189, "prompt": "\u80ba\u6216\u8005\u5fc3\u810f\u54ea\u4e2a\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80ba", "answer_id": "34VafYWH3GEZsmHZqAi38V", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1190, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "n8CaZcApyUEenpCRnYjLjb", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1191, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "oMKK4EYtpxR89kFbSU5hVT", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1192, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u4e0d\u77e5\u9053", "answer_id": "jS8esAkYusWANoY4LzhhNn", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1193, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "EZQ2KZfGtLWDiCk5AdHqmD", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1194, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "JxseW9mQUaoNFEMkdGu8un", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1195, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "LkfWYDk4GzAH7zujDU8mtX", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1196, "prompt": "\u809d\u810f\u548c\u813e\u810f\u8c01\u7684\u4f53\u79ef\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "TxKLUt5QCvhTZT387fZSWX", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1197, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "VcqFTmpTAckuQaE4fE9qBC", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1198, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "mLonTb6RipCvEYKq9gBspb", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1199, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u662f\u5426\u6b63\u5e38?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "6sxeJ8JaHv8QRfhXzDKvpo", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1200, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "d4sEZqjoQTE3S87qvQpcVb", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1201, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "XPk3kNNsdUkf2LNmrqJVGT", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1202, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Rq5VzR3XZ2YytZuUNWr7gA", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1203, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "o8wEFfXGKuENqyy8ueXcLS", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1204, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u809d\u764c", "answer_id": "hUGyvG9oghXdXAmEgVKyuD", "gt_response": "\u809d\u764c", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1205, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "n4iHKN5ZHT8vSWqKKE5Zox", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1206, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "0", "answer_id": "KXS3T7kpw6ecAC7DDZJD3W", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1207, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "9tKXGPy7HnfoUV7LKcLUpM", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1208, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "NHCX8p3cAvACW383LZwYzT", "gt_response": "\u809d\u810f,\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1209, "prompt": "\u56fe\u4e2d\u80be\u810f\u548c\u813e\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "3UYFGaPphdRKRVUEwmxynq", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1210, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "P8RuFrP8CqZUXs6T7XvfKX", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1211, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "3sNQjKBrsYYTHnmFVZMxDN", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1212, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "QZFasfKSRfkQVL3xPzMcrN", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1213, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "7RhxVhMgniaBUuGbbUTp9q", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1214, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Af8ga63DiK3nAVYzDWUWVX", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1215, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "fQzDdmdvqBUvC6CQocCAeU", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1216, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "6SpFNYsozmUXFfoanGKKd3", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1217, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "2H4MFgKSHTW6EWgnuUYHEu", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1218, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "PDmdrxC9UXiBsXSTFLi8Bo", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1219, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "ZmCAaHLFtGeyfbWCZ68Vex", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1220, "prompt": "\u56fe\u4e2d\u80be\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "itK2T5sRavkaHTjZo2oJq3", "gt_response": "\u80be\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1221, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "ABSyWG2SRQRUeYi78bbkEm", "gt_response": "\u5de6\u80be", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1222, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "73CruKx87GYS5p2Ad7nXow", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1223, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "dcgG3riVMRGG5JN64qxwRC", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1224, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "UCwGz9DokUyDfd6xxbaGTe", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1225, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "QvZMGr8SnHhVHNuzbidpJs", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1226, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u662f\u5426\u6b63\u5e38?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "KvR2SqKZ9KGRsbcmKZpUhf", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1227, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "7FsJQxR23Wapekwm5pTTyj", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1228, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "NsWeZpgLun4sQtdKaxWvK6", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1229, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "7QNz8Ptn7tndgkpKYd75Kw", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1230, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "UEX6ZXq7n5DJpcfcQTDhvP", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1231, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "emuxX3JCCT6NERyMXfpsFT", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1232, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "nr7rr5pEW7bnxwGi7Nh4tn", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1233, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "RJPESxijwQ27fvsA9SKwst", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1234, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "0", "answer_id": "kFkVYAouE3WkhsNad5CNpd", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1235, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "2ACFtSA65BrH9GqUVyfRxv", "gt_response": "\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1236, "prompt": "\u56fe\u4e2d\u80be\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "hX7DVzwCDRumsZgx8TFrDK", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1237, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "UPExUyAkuPxcnBiuEgNxBa", "gt_response": "\u5de6\u80be", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1238, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "nsLM8GUioKxaS4iB3eHpiN", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1239, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "hweN2Q4pAmnX9YXQPshBP8", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1240, "prompt": "\u8fd9\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "6iCYp97surTCBBsuB3J7D3", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1241, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "FeQChymkQA8YQHN5FU7aR6", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1242, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "9E6NVWJrWoZaFW9DMYS85X", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1243, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "bSsXEXdxXvueXQXZwwaWuw", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1244, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "dnSYRHDA79SBLbxHJAuAGh", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1245, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "fGLvwe2rDsRfqRTt2xDbrU", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1246, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "SXBVYyPwGGGL4R7KgTVQkf", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1247, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "5KieqcCqjicgjXUGuAfEWw", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1248, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "ZdPs7oNZtzpwru9dkESFg2", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1249, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "0", "answer_id": "Z5wioBUw2vMe4vgbL6hcVL", "gt_response": "1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1250, "prompt": "\u56fe\u4e2d\u80be\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "HLRmCSgCjtYuBdfJHKNJmW", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1251, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u6700\u53f3\u4fa7\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "Cxy2miwWZ4HVeqdvSufZLm", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1252, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "e5KULphzPcfikSzNK4w7nm", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1253, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "MHk8Pv4CkhHz45CvwKrJwW", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1254, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "Cdx4LRrLAsRQHfocy63nrB", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1255, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "nvkTCJawoY52cQcqxQZ8kc", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1256, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u4e0d\u77e5\u9053", "answer_id": "aNCKa9zPHHD2TZ3ujYtQza", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1257, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "g7Whin6GKkywgemKFqXXfj", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1258, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Bm72YTDcQ9jBgDcEzk3YY2", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1259, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "TgLE99qfDk7qhUeJAbZkfs", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1260, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "H5LCthG3eYSmpUciK5nWwv", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1261, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "UMt3WvZUjszET7QAtz5W2R", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1262, "prompt": "\u813e\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "Q5RjAEkQeEnZnon6hV3PvP", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1263, "prompt": "\u56fe\u4e2d\u809d\u810f\u548c\u813e\u810f\u54ea\u4e2a\u4f53\u79ef\u66f4\u5927?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "a7NLN5D3kggtQcjrPb9HH7", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1264, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "EXrTp9ejmK97BYsDvuTRrj", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1265, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "EkDyyKGkt75UNRhvSwdFsb", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1266, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "MnnhB2Sty9nVzVcrvXMJ7q", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1267, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "PyxaGbr4Wq3w6CC8raLpx9", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1268, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "5JchCfBNSqT7faxPtKwmXB", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1269, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "QURtuospcAJN5gjXELWEyE", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1270, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "e5aQtaxzin7HZbLuddBDAN", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1271, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "ehhfmHLEe7SxayAGSeNVUC", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1272, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "4KWP5UNEAGyEJHnBuq6u8j", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1273, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "33ikYcjsP7S2gFZR2frKMq", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1274, "prompt": "\u56fe\u4e2d\u5de6\u80be\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u5de6\u80be\u810f", "answer_id": "Luow3i7jzcKcrf3ZnRGWLg", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1275, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "HCLKKdYKKqQeL4kzwwEUaW", "gt_response": "\u5de6\u80be", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1276, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "PAysEECvXaobf4oqTQbNQ3", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1277, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "hSjJDmgpyjQeiAvWqwUzkx", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1278, "prompt": "\u56fe\u4e2d\u80be\u810f\u7684\u5f62\u72b6\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80be\u810f\u7684\u5f62\u72b6\u662f\u5706\u5f62", "answer_id": "gdccNvM75DEW3VJ4Q9qATb", "gt_response": "\u4e0d\u89c4\u5219", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1279, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "WaXifSqcdA4xkZ6GpLg2Xw", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1280, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "TDydfxaQgVcdY3PsLMDj62", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1281, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "3F99vCu3LbbsGarwMhXppa", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1282, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "BXtqizNZ9oa3kiHJ3gtfvv", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1283, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "4ZHqzMSUSdSGdnKZV9JYXw", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1284, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "658YPvjMDd5jYhmyWEUaV7", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1285, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "M9Yqx3jPPMV6gnM3p48bti", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1286, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "RpqqKm2W4La34PF2mFGVkC", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1287, "prompt": "\u56fe\u4e2d\u809d\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "T2RFoBhPg2c6qR5vVvmJ8e", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1288, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "gadVKk8HPtqqc6SMv7Ntdn", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1289, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "iQiKHz7DWKhtqydcJq5RXt", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1290, "prompt": "\u56fe\u50cf\u53f3\u4fa7\u7684\u9ed1\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "2d8oL9TuF3sysyBYZoEUkw", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1291, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u7070\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "ULXGHWW2ZY74PaFHP2A8mw", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1292, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u9ed1\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "nRk9kF8M7Xk3rHqVXksYnf", "gt_response": "\u53f3\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1293, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "kYyz4U2VcbNLBLVtZRexSA", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1294, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "ZDc2cyPzFaVhe8aGo5yYoy", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1295, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "ZtqpiPzM8m2mqsBYNENQ98", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1296, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u662f\u5426\u6b63\u5e38?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "fZNB5JCNFjd8wQeBVbvdqs", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1297, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "RcxuytzRehFXKiT6EtJK37", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1298, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "XWBZJcoQS2LBUAwSAtEFRH", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1299, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "BuyyZpQr7w8cbyKJiKKKCt", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1300, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "j94eWJR3qRWRL4SfCg98K5", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1301, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "2qcw4nwaonDkk9aQHhGYig", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1302, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "U4QMxPowPE7SohKE6fiemM", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1303, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u809d\u764c", "answer_id": "gR5eQ6dDxv8phHnk8Yg7R6", "gt_response": "\u809d\u764c", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1304, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "0", "answer_id": "PyNvWoy57LPjQNuncA95on", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1305, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "K54FsWtkw2wFFopvmDmqWS", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1306, "prompt": "\u56fe\u4e2d\u809d\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "itk8mvyVydSrRP4PjmgJSC", "gt_response": "\u5de6\u4e0b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1307, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "WcBQdRzzRckLfjoGMSz5au", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1308, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "b2Wqy6DpanprQNU6VzuNnQ", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1309, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u7070\u989c\u8272\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "EMcM82g8P6mW6frYnbiw4G", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1310, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u7070\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "4NHuvY79ahYidev6KhJKiK", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1311, "prompt": "\u56fe\u50cf\u4e2d\u8eab\u4f53\u9876\u90e8\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "Q3Z57bxkNizNY5XWoV2Fmc", "gt_response": "\u80a0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1312, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u626b\u63cf", "answer_id": "aHtBzyT94evd9P52najde5", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1313, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "4dG7Kn7dByrGRemuG8WgqF", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1314, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "4AtAjG4g68Fievt2KmCap7", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1315, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u662f\u5426\u6b63\u5e38?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "A6QfSpqwcG8ohV96A2qpLc", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1316, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "HqiEfk6AnVexiNvouCwxk4", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1317, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Q76N6AQhiLW3ijCq2kJsJW", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1318, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "nBWgLxJVh7vHSpD7GHwVP2", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1319, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "8otLQB5rBXbcgTkyB8kfKZ", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1320, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "gXFAKCocLfGydMaBDpemHu", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1321, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "7YwzUQgbYm6XegQnseYkRp", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1322, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u809d\u764c", "answer_id": "MRKADFUnDVy53yjNAmpgSo", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1323, "prompt": "\u56fe\u4e2d\u809d\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "fQy6pqcvNQFnRUazCjST8V", "gt_response": "\u5de6\u4e0b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1324, "prompt": "\u54ea\u4e00\u4e2a\u662f\u809d\u810f?\n<image>", "pred_response": "\u5de6", "answer_id": "NxFMmEvmsPZ8koTP7VFrCt", "gt_response": "\u5de6\u4fa7\u5448\u7070\u72b6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1325, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "0", "answer_id": "TXzRtYmRjhMc7Dx5CrLg9H", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1326, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "mCvKqphkokYwPisb3MXrd3", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1327, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "mbTFj4XhozQjHUeoHqcvdJ", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1328, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "a4auBh3eV3fGj5i4if9ToC", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1329, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "W89tUfN2JfL8SKDZcdDMzw", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1330, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "QLC5DYf9s8GjexyR6mdLC2", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1331, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "S7f9rguFGprUFLhXzrGqHX", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1332, "prompt": "\u8fd9\u5f20\u56fe\u7247\u770b\u8d77\u6765\u662f\u5426\u6b63\u5e38?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "ePJTzz8Xroskgsjuvr2fUV", "gt_response": "\u4e0d\u6b63\u5e38", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1333, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "32rvio8g4o6rniaY5btqWu", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1334, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "a8pZS7r3qsCZ2HkdrdbpGG", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1335, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "mNcZzyeGzz6p8Rs8oiH7Z8", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1336, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "jsMgTCjKe767aLZszeN4hX", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1337, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "cYLQJa2Wk32wtPTeGXaGsH", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1338, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "imFJm5s2YAzQYZyAuvFneV", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1339, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u809d\u764c", "answer_id": "cNUpnQYJ5HbpD4AJpb2byf", "gt_response": "\u809d\u764c", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1340, "prompt": "\u54ea\u4e00\u4e2a\u662f\u809d\u810f?\n<image>", "pred_response": "\u5de6", "answer_id": "WiWVryHWbZ5ZbjzZxHGQur", "gt_response": "\u5de6\u4fa7\u5448\u7070\u8272\u72b6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1341, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "aAFQGkCPKowviYd4G5xgEU", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1342, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "EZ2nik8ySGJwBqv54TViZ2", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1343, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u80be\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "cExNWP2VNTtmiNhhKCsNif", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1344, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "cZ6TUg7rS8EiUPGJW8JXcQ", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1345, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u7070\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "58aDatkYsSFCCJjcUXJBRf", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1346, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "MmaxNZWroGngyCbRSeQj8A", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1347, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "GXohwPZkECPzHdevjnWkny", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1348, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "Eni9cwQzQerWuxCx8WYWXL", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1349, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "AY67kT5zmEEpeDjjYCMWa4", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1350, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "McHzGQmqyMETJGtYNtELyN", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1351, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "QdMY4PEtxg3qutWRHAvyzD", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1352, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "65T2KuuaSXMyNAfHjfAHng", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1353, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "N6P56zmMRXReZczhCRcg9X", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1354, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "kZgkvbNX7En6HPinvd3YYr", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1355, "prompt": "\u56fe\u4e2d\u5fc3\u810f\u548c\u80ba\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u80ba", "answer_id": "8wMLvrb3HzWY5MiCRkmCaE", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1356, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u6700\u53f3\u4fa7\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "QQg6UjVWDuyYaPVgkQtJ5m", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1357, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u9ed1\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u80ba", "answer_id": "9n8E5CLhEdVURRjYyrWLM9", "gt_response": "\u53f3\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1358, "prompt": "\u56fe\u50cf\u4e2d\u8eab\u4f53\u9876\u90e8\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "PUwNQDSZ4CN4VhMZJjXkye", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1359, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "AF382vv5t4UYtuYypPZVyY", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1360, "prompt": "\u56fe\u4e2d\u8eab\u4f53\u9ed1\u8272\u90e8\u5206\u5185\u7684\u767d\u8272\u8109\u7edc\u72b6\u7269\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "NTvoMZktyCDVe7jGVshkiK", "gt_response": "\u80ba\u652f\u6c14\u7ba1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1361, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "U7D7bhmjtZW2DmzhxzsWeD", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1362, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "63FGMN2DCeX8fqAL7wEufB", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1363, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "E6jtSv7GoVafKYAuVX9YJu", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1364, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "RfYJodZiMZHVmdkzxk3RTt", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1365, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "KLDExppd59s4tDg7nA6ZbG", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1366, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "BkN9aBBqLo7Gpto7tbp6bY", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1367, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "kKSVZeoiDUtJFt2f5Hb8Ht", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1368, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "2p4uTD8VvuMKFuvm7DYJWG", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1369, "prompt": "\u80c3\u5728\u56fe\u7247\u4e2d\u51fa\u73b0\u4e86\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "HGK9xkTPYXBj2btvyZ9iho", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1370, "prompt": "\u56fe\u4e2d\u809d\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "kSfi2cft4HLZEGCZxeB7YJ", "gt_response": "\u5de6\u4fa7\u548c\u4e0a\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1371, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "NQwWJK62AQcTnb8ruY3NqS", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1372, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "YLWwfw5CL3gFGKuZVdorfu", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1373, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u9ed1\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "UyhxZrBzpfkjNZfAYpnFb7", "gt_response": "\u53f3\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1374, "prompt": "\u56fe\u50cf\u4e2d\u8eab\u4f53\u9876\u90e8\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "FoxXoMccruE9KHUJDs58TQ", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1375, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "Df5XvBi9UYewkRGVFGcy3P", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1376, "prompt": "\u56fe\u50cf\u53f3\u4e0a\u89d2\u7684\u9ed1\u8272\u7a7a\u6d1e\u4ee3\u8868\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "WiU6LF6iF42woWnz8tyEpS", "gt_response": "\u80c3", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1377, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "94sa4m34MzsnnQAUUtJZNW", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1378, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "AyvWM3ftw7sXRRnfwfY9rA", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1379, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "9Ktxa98yKaUhKVnNAYSM89", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1380, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "NBAojf3Jtyw9cvf3ju4eWT", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1381, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "J9rzKhwkSaz5CBKh6STqcN", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1382, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "asdY78yaBnKrQrLTRRQ65g", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1383, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "bLFaMndPYdFTxnEMFiJLna", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1384, "prompt": "\u56fe\u4e2d\u809d\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "aYmntcTFEsaVoWLNWZCVfT", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1385, "prompt": "\u56fe\u4e2d\u813e\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u813e\u810f", "answer_id": "eVm8Wi9Xt5ybdCYVRh93Xu", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1386, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "coqgve4NhPeUbrcEcDxzkb", "gt_response": "\u813e\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1387, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "T63GhTbu2B4dnkDpYAwKao", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1388, "prompt": "\u56fe\u50cf\u4e2d\u8eab\u4f53\u9876\u90e8\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8111", "answer_id": "S9aZoTFBDHw2cUXMpiPKP7", "gt_response": "\u5927\u80a0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1389, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "3vXwvni6rRFN9cDE3UZTFF", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1390, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "VQaznD2v6Uh792wDHfVwXa", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1391, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "YH9AYRP84625cKKbjBQTsA", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1392, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "V9793e6CjUNkTji7H5r7TV", "gt_response": "\u5c0f\u80a0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1393, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "G5tRaYmxSXHprybsVJuUjj", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1394, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "iutAPaYsS86ymjx7xEdg8o", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1395, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "RCmJGfMXmFpgkBgyAxxj2X", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1396, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "GxDRMRSAGmadd8W45qBJAx", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1397, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u53f3\u4e0b\u89d2\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "TEoF6q5GdTnfbP5e3emKao", "gt_response": "\u5de6\u4fa7\u80be\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1398, "prompt": "\u56fe\u7247\u5de6\u4fa7\u7684\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "97jZRJJAYJoAaAU3nULayZ", "gt_response": "\u5927\u80a0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1399, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u626b\u63cf\u5e73\u9762\u662f\u8179\u90e8", "answer_id": "BAyLQLzQWYfWzLbJYFefPT", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1400, "prompt": "\u8eab\u4f53\u5de6\u4e0b\u89d2\u7070\u8272\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "crfEPEvs8riTwc8s2ir2jK", "gt_response": "\u53f3\u4fa7\u80be\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1401, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "NDse5dZZQtdto4fCLSTvS5", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1402, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "LntuAWtjmPmMcsN9rpY3SD", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1403, "prompt": "\u56fe\u7247\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "6HmBiJXgFo35ssdHSQ2pPN", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1404, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "TvCL37rMaig9K7rLo93AiP", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1405, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "PBXBVJ7yvPexVJfFooEqig", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1406, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "efFTEiHaYH6QWykPS56LGX", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1407, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "5eQDsHci2sLuHsiLWeSteV", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1408, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "4SAFfWMmATjuE6mm8Vcft4", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1409, "prompt": "\u56fe\u4e2d\u5fc3\u810f\u3001\u80ba\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u80ba", "answer_id": "bEfLbgteoDt6osAdwMUosD", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1410, "prompt": "\u4f4d\u4e8e\u56fe\u7247\u6700\u53f3\u4fa7\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "UYqueRN7oAdueUACWxZBe8", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1411, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u7070\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "SZMw6s4q8orrCHLg2Moc2p", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1412, "prompt": "\u56fe\u50cf\u5de6\u4fa7\u7684\u9ed1\u8272\u90e8\u5206\u662f\u4ec0\u4e48\u5668\u5b98?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "oW46b68zfv2WrTgBU5vANU", "gt_response": "\u53f3\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1413, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u626b\u63cf\u5e73\u9762\u662f\u6a2a\u65ad\u9762", "answer_id": "CHFGwwfjW5FzkBGhikEGaN", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1414, "prompt": "\u56fe\u4e2d\u75c5\u4eba\u662f\u5426\u6709\u809d\u764c?\n<image>", "pred_response": "\u4e0d", "answer_id": "YcXptn2QYSCk2mkE8iknVR", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1415, "prompt": "\u8fd9\u662fCT\u626b\u63cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "ZuYtZiKaMwRaLqf7NUY6fW", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1416, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "jBKPKLyYhfDRkx3GDJTYZR", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1417, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "2CgTxmEkRtG4sQXeexAocZ", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1418, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "K2XA6rwhEa3MQ4yFKCQPLJ", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1419, "prompt": "\u80bf\u5757\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80ba", "answer_id": "CrAyfHjYG2rkWm93Dx2Qyt", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1420, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "LfoVm84j8KqBCzK4dFdn8u", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1421, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u8179\u90e8\u4e48?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "39BiaHUvqLszHMkyVBSzmU", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1422, "prompt": "\u75c5\u4eba\u7684\u5de6\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "SpT2eetvD572xGiz7MUBpf", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1423, "prompt": "\u80ba\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "Nvme8h4DT8Z8j8dNi5PNUW", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1424, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80bf\u5757\u5417?\n<image>", "pred_response": "\u4e0d\u80fd", "answer_id": "aD3xQWQqDxocYU6k3UBBgh", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1425, "prompt": "\u56fe\u4e2d\u7684\u80ba\u662f\u4ec0\u4e48\u989c\u8272\u7684?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "JrvCtAzFS84pDrniNgVkc2", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1426, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u5fc3\u810f?\n<image>", "pred_response": "0", "answer_id": "cKnUMFBQ3XPz7UneSmsFGp", "gt_response": "1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1427, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u4f1a", "answer_id": "Mm7JTqjFCfCxNh7X7pMP9H", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1428, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u80ba\u90e8\u5de6\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u6839\u636e\u60a8\u63d0\u4f9b\u7684\u75c5\u5386\uff0c\u60a8\u7684\u75be\u75c5\u4f4d\u4e8e\u80ba\u90e8\u5de6\u4fa7\u3002\u5177\u4f53\u6cbb\u7597\u65b9\u6848\u53d6\u51b3\u4e8e\u75be\u75c5\u7684\u7c7b\u578b\u548c\u4e25\u91cd\u7a0b\u5ea6\u3002\u5982\u679c\u60a8\u7684\u75be\u75c5\u662f\u80ba\u708e\uff0c\u60a8\u53ef\u4ee5\u5c1d\u8bd5\u91c7\u53d6\u4ee5\u4e0b\u63aa\u65bd\uff1a\n1. \u4f11\u606f\u548c\u89c2\ufffd", "answer_id": "Tc8bptBzpssCM9BMdFgyqd", "gt_response": "\u836f\u7269\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1429, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "JNLaaHgqs8CKgYc8B4mytt", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1430, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "4qQ2NvCMSwD5eZtbRmpoDn", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1431, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "NF2VB36B66HyrKdE3pE7e3", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1432, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u4e0d\u77e5\u9053", "answer_id": "VBJtUeGWPR8bCsBpHk45P7", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1433, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "UWdT5XGVZcvF44bjDV2N8d", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1434, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "oXUKmTXmFuLcqaNEqFdui5", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1435, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "icctCSqyCApAd8m4K6gAgv", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1436, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "n7mRU9TBjjMhqY4zXUGRJr", "gt_response": "\u5de6\u4e0a", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1437, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "aKBJ5nkMGk8SSQgQZQkKJb", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1438, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "8QpreumA685Lx2676QB45v", "gt_response": "X\u5149", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1439, "prompt": "\u8fd9\u662f\u6a2a\u65ad\u9762\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "HpqPJ4PDf9SgC6Xv9FJ78W", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1440, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "QkekuQXDHq5zHn64LoahfN", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1441, "prompt": "\u56fe\u4e2d\u5fc3\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6", "answer_id": "bVWuXSs7GWi8GQVwJf8FTh", "gt_response": "\u4e2d\u5fc3", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1442, "prompt": "\u80ba\u4e0d\u5f20\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "H76WiA2BRk6GdcgxWZ76Ym", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1443, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "Gdh6UZAErQVnTHNn6rWaAh", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1444, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "G33z8fH8BDJQJPb6UKEgkr", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1445, "prompt": "\u8fd9\u5f20\u56fe\u7247\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "Eimyk3XZVPP8PjzartrCxP", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1446, "prompt": "\u75c5\u4eba\u7684\u5de6\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "nPPxcR97MrCYpHJKnkdkTo", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1447, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u4e0d\u5f20\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "QJDUkiEtDHq55C4zDpbnup", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1448, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u7070\u8272", "answer_id": "bqa84Eg26h6H4P4HyVpGYu", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1449, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "8Cumd8bL3oSYPVXEcf3VsM", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1450, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u4f1a", "answer_id": "QpVLLfEZpfqKUhunNoTbmQ", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1451, "prompt": "\u56fe\u4e2d\u53f3\u4e0b\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u5728\u8fd9\u4e2a\u80f8\u90e8CT\u56fe\u4e2d\uff0c\u53f3\u4e0b\u80ba\u6709\u75be\u75c5\uff0c\u8fd9\u53ef\u80fd\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u547c\u5438\u56f0\u96be\u3001\u75bc\u75db\u548c\u54b3\u55fd\u7b49\u75c7\u72b6\u3002\u75be\u75c5\u53ef\u80fd\u662f\u80ba\u6c14\u80bf\u3001\u80ba\u708e\u6216\u80ba\u764c\u7b49\u3002\u8fd9\u4e9b\u75c7\u72b6\u53ef\u80fd\u4f1a\u5f71\u54cd\u75c5\u4eba\u7684\u751f\u6d3b\u8d28\u91cf", "answer_id": "TXHFDsvWtwpk23gCgXmcot", "gt_response": "\u80f8\u95f7,\u6c14\u6025,\u547c\u5438\u56f0\u96be,\u5e72\u54b3", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1452, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "ZEigsDPxFNys8fDqLBpNg5", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1453, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u663e\u793a\u7684\u662f\u54ea\u79cd\u7c7b\u578b\u7684\u6210\u50cf,\u6838\u78c1\u5171\u632f\u3001CT\u8fd8\u662fX\u5149?\n<image>", "pred_response": "CT", "answer_id": "362QkkSamQxX3GaXFMwjgk", "gt_response": "X\u5149", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1454, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "XQMuU93LktZ9dqMyu3mZVt", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1455, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "3zqVZkYmgm5QzzWVG4G5YP", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1456, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u6b63\u5e38\u7684?\n<image>", "pred_response": "\u5de6", "answer_id": "PF9HTufWLb79K5zrY6FezC", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1457, "prompt": "\u80bf\u5757\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80ba", "answer_id": "LAftTN2cdJdyx5G3dQdtBu", "gt_response": "\u53f3\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1458, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "grZxWyTsKqtJhHmDsE3tdC", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1459, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "9Wfe9k6AfZqdPmCgRdDtnK", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1460, "prompt": "\u56fe\u7247\u4e2d\u5305\u62ec\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "hMesyJswNdTSGsypRPxEWA", "gt_response": "\u80ba\u80bf\u5757", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1461, "prompt": "\u80ba\u7684\u53f3\u4fa7\u662f\u4ec0\u4e48\u75c5?\n<image>", "pred_response": "\u80ba\u708e", "answer_id": "XPDjt9iZqgBSHYSaYSj9pC", "gt_response": "\u80ba\u80bf\u5757", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1462, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80bf\u5757\u5417?\n<image>", "pred_response": "\u4e0d\uff0c\u5de6\u4e0b\u80ba\u6ca1\u6709\u80bf\u5757\u3002", "answer_id": "DVgkpA3dj4Hw4uuTL7Dybd", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1463, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "TYvy2tscJFyfMwVpFpayM5", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1464, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "cBKex8Q3G6PavARWeewL94", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1465, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8fd9\u5f20\u7167\u7247\u4e3b\u8981\u5c55\u793a\u4e86\u80f8\u8154\u548c\u80ba\u90e8\u7684\u7ed3\u6784\uff0c\u5305\u62ec\u80ba\u6ce1\u3001\u80f8\u8154\u548c\u80f8\u8154\u58c1\u3002\u80f8\u8154\u662f\u8840\u6db2\u7684\u6536\u96c6\u548c\u5206\u914d\u7684\u5730\u65b9\uff0c\u80ba\u90e8\u5219\u662f\u547c\u5438\u7684\u4e2d\u5fc3\u3002", "answer_id": "YrhokRX7Rspn6wt98ky7aH", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1466, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u53f3\u80ba\u75be\u75c5\u53ef\u80fd\u4f1a\u5bfc\u81f4\u75c5\u4eba\u51fa\u73b0\u547c\u5438\u56f0\u96be\u3001\u75b2\u52b3\u3001\u54b3\u55fd\u3001\u54b3\u75f0\u3001\u80f8\u75db\u7b49\u75c7\u72b6\u3002", "answer_id": "4B5egQk5YZNj8rniuC4HMi", "gt_response": "\u547c\u5438\u56f0\u96be,\u54b3\u8840,\u80f8\u75db", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1467, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "eytEvAMGBZYBNEsUiRMT5h", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1468, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "CT", "answer_id": "Xqj9ydAr9EgnDmDAmPit7r", "gt_response": "X\u5149", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1469, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "d4dErj8QokJNqWebUt8wxW", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1470, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u80f8\u8154?\n<image>", "pred_response": "\u662f", "answer_id": "H5bnqwbMWaKu7eYL8fs7CM", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1471, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "VE3JkcYSR9CgF8PPY6XDae", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1472, "prompt": "\u56fe\u4e2d\u80bf\u5757\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u53f3\u4fa7", "answer_id": "cvzqoaYcvgziQrqY4UKaN9", "gt_response": "\u53f3\u4e0a\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1473, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Syoax73sqvJBaBn2cknNpf", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1474, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "HDxVeHTu9hkhHXsLCwdcH5", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1475, "prompt": "\u80ba\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "anj678FHWBC7qqECCvW5AZ", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1476, "prompt": "\u56fe\u4e2d\u80ba\u7684\u53f3\u4fa7\u662f\u4ec0\u4e48\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "AtAjsAZHUYqs9XBqTrdxCu", "gt_response": "\u80ba\u80bf\u5757", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1477, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80bf\u5757\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "FfSuzyVifLZkuutSN5FBRB", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1478, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "hqqU7SEtgMt9Nre8uwZ8XE", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1479, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "7KwinuPygGUtdtLWiiBBut", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1480, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u4f1a", "answer_id": "GGfFFft59D3i5sk3yvWZGF", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1481, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u80ba\u90e8\u53f3\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u60a3\u8005\u9700\u8981\u505a\u51fa\u80ba\u90e8\u53f3\u4fa7\u7684\u75be\u75c5\u7684\u6cbb\u7597\u65b9\u6848\uff0c\u5305\u62ec\u91c7\u53d6\u9002\u5f53\u7684\u836f\u7269\u548c\u533b\u7597\u63aa\u65bd\u3002\u5177\u4f53\u7684\u6cbb\u7597\u65b9\u6848\u53d6\u51b3\u4e8e\u75be\u75c5\u7684\u7c7b\u578b\u548c\u4e25\u91cd\u7a0b\u5ea6\u3002\u60a3\u8005\u5e94\u8be5\u54a8\u8be2\u533b\u751f\uff0c\u4e86\u89e3\u75be\u75c5\u7684", "answer_id": "2fZhXHhcnTSgYX4LtVScMz", "gt_response": "\u836f\u7269\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1482, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "ZQvDUW6HAASTDnZuyBE2Ty", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1483, "prompt": "\u8fd9\u662fCT\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "XWxpzUr8HGQmyTSam9f5x2", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1484, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "XtwnLLCof4BRfREnQUS3J5", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1485, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u80f8\u8154?\n<image>", "pred_response": "\u662f", "answer_id": "CF49k5QQnmaqeU9hD6zuRw", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1486, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "PffogZeszBVGTbc6Lhk74p", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1487, "prompt": "\u56fe\u4e2d\u80ba\u7ed3\u8282\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "ZrEYn3UNRQaWzPbbGy6Vdo", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1488, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u9888\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f\uff0c\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u7684", "answer_id": "L3v6UEgRN5YzycMarVGJqE", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1489, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "YDyej7u4vaQdKpXreEriLx", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1490, "prompt": "\u56fe\u4e2d\u80ba\u7684\u53f3\u4fa7\u662f\u4ec0\u4e48\u75c5?\n<image>", "pred_response": "\u80ba\u708e", "answer_id": "jrrShVBtGiwXB44NQC8fKT", "gt_response": "\u80ba\u7ed3\u8282", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1491, "prompt": "\u80ba\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "CZucQHzLngqpppd2nBAeRm", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1492, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u7ed3\u8282\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "9CREEjJMwjpa6pXa4J5SCN", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1493, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u7070\u8272", "answer_id": "5baz3dtc3rePM5vAZbxmnU", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1494, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "XRDrXUcepTFGzVU6w4QD2k", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1495, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4e0d\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u4f24\u5bb3\uff0c\u56e0\u4e3a\u5b83\u662f\u4e00\u5f20\u865a\u62df\u7684\u80f8\u90e8X\u5c04\u7ebf\u56fe\u50cf\uff0c\u6ca1\u6709\u5177\u4f53\u7684\u75c5\u7406\u7ed3\u6784\u6216\u6709\u5bb3\u7269\u8d28\u3002\u865a\u62df\u56fe\u50cf\u662f\u901a\u8fc7\u8ba1\u7b97\u673a\u751f\u6210\u7684\uff0c\u4e0d\u4f1a\u5bf9\u8eab\u4f53\u4ea7\u751f\u4efb\u4f55\u5b9e\u9645\u7684\u5f71\u54cd\u3002", "answer_id": "TaB6GmdjwF57NjhcvwgqcL", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1496, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u80ba\u90e8\u53f3\u4e0b\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u6839\u636e\u60a8\u7684\u63cf\u8ff0\uff0c\u75be\u75c5\u4f4d\u4e8e\u80ba\u90e8\u53f3\u4e0b\u4fa7\u3002\u7531\u4e8e\u7f3a\u4e4f\u66f4\u591a\u7684\u75c5\u53f2\u548c\u75c7\u72b6\u4fe1\u606f\uff0c\u6211\u65e0\u6cd5\u63d0\u4f9b\u5177\u4f53\u7684\u6cbb\u7597\u5efa\u8bae\u3002\u5efa\u8bae\u60a8\u54a8\u8be2\u533b\u751f\uff0c\u4ee5\u83b7\u5f97\u66f4\u5177\u4f53\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u5efa\u8bae\u3002", "answer_id": "SefyRm2jSGscsDVNArifWs", "gt_response": "\u836f\u7269\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1497, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "QoSznKWukDj3VS9PucfPnb", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1498, "prompt": "\u8fd9\u662f\u6838\u78c1\u5171\u632f\u6210\u50cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "5BpmJUopP4P27nQtobvykH", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1499, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "BKBkTzRZZGdkcSeafzTAmM", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1500, "prompt": "\u4ece\u6574\u4e2a\u8eab\u4f53\u7684\u89d2\u5ea6\u770b,\u8fd9\u5e45\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "n5kCnUhbx4RFxNABADidnC", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1501, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "4oJf2Yxm3SqZP6PQGuCjHo", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1502, "prompt": "\u56fe\u4e2d\u80ba\u4e0d\u5f20\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "k8ZsXxmsfmq4fsLunndRe4", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1503, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "bAdtp2Qp8tAXNizXzuyUvr", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1504, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "2eWnzLC2H2QyF4Z2EUYSse", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1505, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "eWpc2mGCYV6e4yN8ghDMAY", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1506, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "T3TgSeQ6rKjVzabbhNWbP5", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1507, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u4e0d\u5f20\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "E5ctB5RjByfmVQEFKygJ7G", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1508, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u7070\u8272", "answer_id": "ZtKhRYg6aZ7ph62BDHMeyG", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1509, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "XK85XjFLvupAzrBdyFUk9E", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1510, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u60a8\u7684\u95ee\u9898\u4e0d\u5b8c\u6574\uff0c\u65e0\u6cd5\u56de\u7b54\u3002", "answer_id": "Rosj5Wou6UG2mTMtJVtX5w", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1511, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u80ba\u90e8\u5de6\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u6839\u636e\u60a8\u63d0\u4f9b\u7684\u75c5\u5386\uff0c\u60a8\u7684\u80ba\u90e8\u5de6\u4fa7\u6709\u7ea4\u7ef4\u5316\u75c5\u53d8\uff0c\u8fd9\u662f\u4e00\u79cd\u80ba\u7ea4\u7ef4\u5316\u75c5\uff0c\u4e5f\u79f0\u4e3a\u80ba\u7ea4\u7ef4\u5316\u75c5\u53d8\u3002\u8fd9\u79cd\u75be\u75c5\u901a\u5e38\u662f\u7531\u4e8e\u80ba\u90e8\u7ea4\u7ef4\u5316\u75c5\u53d8\u5f15\u8d77\u7684\uff0c\u8fd9\u662f\u4e00\u79cd\u80ba\u90e8\u75be\u75c5\uff0c\u901a\u5e38", "answer_id": "frHF9VtTQB6c4dySWpoanK", "gt_response": "\u7269\u7406\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1512, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "Z4B89Aiwz6W4GPjjdvtCHz", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1513, "prompt": "\u8fd9\u662fCT\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "FWZa3AxWeCVe2L3VWdqyyf", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1514, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "BWkaJ3duxsfLoB24JUynbD", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1515, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "K78TeoMd2xGrn8yiTYoU4x", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1516, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "ajWiL6mWgYC7vUBeYKgqmG", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1517, "prompt": "\u6d78\u6da6\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "8nCxmUzJakupYVfQVYQskW", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1518, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "bGwtzhzkvYQ8Mb8suM3ekQ", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1519, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "L7d4Cv38pzw3TzvxApjiTQ", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1520, "prompt": "\u75c5\u4eba\u7684\u5de6\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "8E8XqkVTHeU5vwmRK9e8Vd", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1521, "prompt": "\u56fe\u7247\u4e2d\u5305\u62ec\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "WwbVXZz7Gb6JmEmdnJuwF7", "gt_response": "\u80ba\u6d78\u6da6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1522, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u6d78\u6da6\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "BS6MQJ9RxPv8LVU9Nm47fD", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1523, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "5xyCom2b7j48zV4LjfxDqX", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1524, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "hedPLhtCcgjpcdfzh4i47A", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1525, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4e0d\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u4f24\u5bb3\uff0c\u56e0\u4e3a\u5b83\u662f\u4e00\u5f20\u80f8\u90e8X\u5c04\u7ebf\u56fe\uff0c\u5b83\u4e0d\u4f1a\u5bf9\u8eab\u4f53\u4ea7\u751f\u8f90\u5c04\u3002", "answer_id": "Hq7WugmsJ7859HWZrzLfuE", "gt_response": "\u8f83\u5c11", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1526, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u80ba\u90e8\u5de6\u4e0b\u89d2\u7684\u75be\u75c5?\n<image>", "pred_response": "\u60a3\u8005\u9700\u8981\u505a\u54ea\u4e9b\u68c0\u67e5\u548c\u6cbb\u7597", "answer_id": "cqe5fXN9THDkVyX5PqGj7a", "gt_response": "\u836f\u7269\u6cbb\u7597", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1527, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "DRxFeysYMditU4RzmgPtVy", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1528, "prompt": "\u8fd9\u662fCT\u626b\u63cf\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "5kx5cqBXa6Nyt4HQPW9Zpv", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1529, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "nSD7nrWbPeBdsJJast3Fif", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1530, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "GRC3ZyPvpRAD6AbhpKuj5j", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1531, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u4e0d\u6b63\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "AdiBxkGGcRBdkmz3pwz37f", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1532, "prompt": "\u56fe\u4e2d\u7ed3\u8282\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80ba", "answer_id": "Cmb9bhf2dft9GvXMXJgYFY", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1533, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "L29Ao9xUoNrFgztUkAFPa4", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1534, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "2fJufvbTQtKBNXE79hDoXk", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1535, "prompt": "\u75c5\u4eba\u7684\u53f3\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "ADV3wMp7bDN3Ff6dzJX7pC", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1536, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "nBLtRPpRzrcjE4SGkosyvs", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1537, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u7ed3\u8282\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "iaEUTvNpFKkccU3JMcWvNS", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1538, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "n5EuC4rbKwesaJXNcNEaLw", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1539, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "BZnyeoFZf8cEsuBPYhUNu7", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1540, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u9ed1\u8272\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "4HCs7uccYNdpWDrP547Lsm", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1541, "prompt": "\u56fe\u4e2d\u80ba\u90e8\u53f3\u4e0b\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba\u90e8\u53f3\u4e0b\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u80ba\u764c\u3002", "answer_id": "EZQE7RwinP3k3MqY8eBKNv", "gt_response": "\u80ba\u90e8\u611f\u67d3,\u6162\u6027\u523a\u6fc0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1542, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "XXFdwDR9pY2o7qZPggGawV", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1543, "prompt": "\u8fd9\u662fX\u5149\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "Ew7A5qVSSd8P3EdravEyua", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1544, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "AbxwHf8w8MHJcQt7K2hUAb", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1545, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "ktWrYPpQAHGdqAvARizjE8", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1546, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "EoMYdXot7DFzbmEDxEN5UJ", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1547, "prompt": "\u56fe\u4e2d\u6d78\u6da6\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "QCLyMEnvEG4abJZz6fuFGV", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1548, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u80ba", "answer_id": "Kr65MYbN4gtpvaq4HNHaCk", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1549, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "bXGcPsEfCXnFJB7pTScosS", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1550, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "BZG3QgKTUGHSfCDMzgjCyd", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1551, "prompt": "\u75c5\u4eba\u6709\u4efb\u4f55\u7684\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u6709", "answer_id": "MCopHy5QtkhyYeU4rGA3oT", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1552, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u6d78\u6da6\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "489kHSd3AF86ePzsQKJDEA", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1553, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "LWN9WcACyMSVPSG7YgTw32", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1554, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "RXHLBsEFZHxF87QqDEpAXd", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1555, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "evDuyiTsMWgvWBmMSNgoWs", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1556, "prompt": "\u5982\u4f55\u9884\u9632\u80ba\u90e8\u5de6\u4fa7\u75c5\u53d8\u7684\u53d1\u751f?\n<image>", "pred_response": "\u9884\u9632\u80ba\u90e8\u5de6\u4fa7\u75c5\u53d8\u7684\u53d1\u751f\u5305\u62ec\u5438\u5165\u542b\u70df\u7684\u9884\u9632\u3001\u4fdd\u6301\u826f\u597d\u7684\u751f\u6d3b\u4e60\u60ef\u3001\u907f\u514d\u8fc7\u5ea6\u66b4\u9732\u4e8e\u7a7a\u6c14\u6c61\u67d3\u7269\u3001\u907f\u514d\u60a3\u6709\u6162\u6027\u75be\u75c5\u548c\u75be\u75c5\u7684\u60a3\u8005\u63a5\u53d7\u6709\u6548\u7684\u6cbb\u7597", "answer_id": "6t2pjjCuBFLtQrfbzG5H68", "gt_response": "\u5065\u5eb7\u751f\u6d3b,\u4fdd\u6301\u8eab\u4f53\u5065\u5eb7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1557, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "2FgKiG9xJQCX6zSVMQbMH8", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1558, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7\u6838\u78c1\u5171\u632f\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "38izGWRmh6oCvkSNrfyPEu", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1559, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u6a2a", "answer_id": "aEaHdKmzLkDfMY4KjcbsHF", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1560, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "LfVFVLvTafL8qtLefuy3Kg", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1561, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u5de6", "answer_id": "aS4Coz7GCBMpLRE5kmn92Q", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1562, "prompt": "\u56fe\u4e2d\u7ed3\u8282\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "Eyza4RG8p8wV9oahsFonKn", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1563, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "HpqKYkkPQrDAKRSr5gYC29", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1564, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f\uff0c\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u7684", "answer_id": "U7wHhjWKezps3dJX5geJNr", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1565, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "GdxqEBE2iv8VxztjWnD4E9", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1566, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "5dqv6FaGidzYzTTymgYpYa", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1567, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u7ed3\u8282\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "FBQv3LsQJ6PLLWd9Acrt3x", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1568, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "3fbtxH559D9D3hjjrAbHBS", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1569, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "n9sxpZoMPQWzZ7f8Z6j2qW", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1570, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u9ed1\u8272\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "bJZzUypDBxiApNSNwf42WT", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1571, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u5de6\u80ba\u75be\u75c5\u53ef\u80fd\u4f1a\u5bfc\u81f4\u75c5\u4eba\u51fa\u73b0\u547c\u5438\u56f0\u96be\u3001\u75b2\u52b3\u3001\u54b3\u55fd\u3001\u54b3\u75f0\u3001\u54b3\u55fd\u547c\u5438\u56f0\u96be\u7b49\u75c7\u72b6\u3002", "answer_id": "Wq8F7cwfPw6hBWrBp4fDiv", "gt_response": "\u80f8\u95f7,\u4e4f\u529b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1572, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "EaU27pYNPPFGebY4x4cNrS", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1573, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "WY22M2srm5DrDpeYMe8Nz8", "gt_response": "X\u5149", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1574, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "5Bm74qt5utAhpBJ6jmVU83", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1575, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "DoQ8qiPEY3nLhS6YG5iFzr", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1576, "prompt": "\u56fe\u4e2d\u5fc3\u810f\u5728\u54ea?\n<image>", "pred_response": "\u5de6", "answer_id": "SeyanL4R9yVNCVVJZAYgKC", "gt_response": "\u4e2d\u5fc3", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1577, "prompt": "\u5fc3\u810f\u80a5\u5927\u4f4d\u4e8e\u80f8\u8154\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "NhWuaPcL87cHN4HgpPw7NX", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1578, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "nj4c3A2FFx8foXgrZqBKgd", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1579, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "ePoXfdxkSpNjUsG7wds3AE", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1580, "prompt": "\u80ba\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "mN9wdqeAiUymfpSazuTJ2A", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1581, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Zy3LxTGeYfz3x69zeckmGZ", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1582, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80bf\u5757\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "4X5QtdtSBhCvpqTpzsHY5V", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1583, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u7070\u8272", "answer_id": "ggFAzsCMiUoUa92tEnPg2X", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1584, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "DUgKGTprBEGpTnxzVcDjMt", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1585, "prompt": "\u56fe\u7247\u4e2d\u5fc3\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u5728\u60a3\u8005\u7684\u80f8\u8154\u4e2d\u690d\u5165\u547c\u5438\u7ba1", "answer_id": "RYNstbif2qmn7jMqHcMMAM", "gt_response": "\u63a8\u52a8\u8840\u6db2\u6d41\u52a8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1586, "prompt": "\u56fe\u4e2d\u5fc3\u810f\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u70df\u8349", "answer_id": "YQdLrjsNU59dHxhLNxPCrE", "gt_response": "\u9ad8\u8840\u538b,\u6269\u5f20\u6027\u5fc3\u808c\u75c5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1587, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "iBiMY8GAsbc8m8paQYe93W", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1588, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "CT", "answer_id": "UomfandFteTnYn5FKR7Cpd", "gt_response": "X\u5149", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1589, "prompt": "\u8fd9\u662f\u6a2a\u65ad\u9762\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "P99qHoqsVjmHwm7oLEYQqR", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1590, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "f3vgFBAAS6Hn2vpVX4rsE9", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1591, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "8MGnmKbN4StiK5MHdHsMXa", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1592, "prompt": "\u6c14\u80f8\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "mCDU9RywYmeBvSeaGFzneJ", "gt_response": "\u5de6\u4e0a\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1593, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "mo4CKaPqb4XAP93CXbi6xS", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1594, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u9888\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "BeuxVznc8ghFn8ioeKShnV", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1595, "prompt": "\u8fd9\u5f20\u56fe\u7247\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "NvVvp6Lh7byqP9FPG6cjD5", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1596, "prompt": "\u56fe\u7247\u4e2d\u5305\u62ec\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "ewXZPCHdRN2ZYxvVywS6fp", "gt_response": "\u6c14\u80f8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1597, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u6c14\u80f8\u5417?\n<image>", "pred_response": "\u662f\u7684\uff0c\u5de6\u4e0a\u80ba\u6709\u6c14\u80f8\u3002", "answer_id": "YUZPe5bLpt3LoDVx7q3Jau", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1598, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u7070\u8272", "answer_id": "5mC5uNQLMLrsP2hezyKb9L", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1599, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "AFyvcS6TBXykpLnnReZVse", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1600, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u9ed1\u8272\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u9ed1\u8272\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u5fc3\u810f\u7684\u68c0\u67e5\u3002", "answer_id": "Qew3gu4zsbBZPLsLDRCFkC", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1601, "prompt": "\u5982\u4f55\u9884\u9632\u80ba\u90e8\u5de6\u4e0a\u53f6\u75c5\u53d8\u7684\u53d1\u751f?\n<image>", "pred_response": "\u80ba\u90e8\u5de6\u4e0a\u53f6\u75c5\u53d8\u662f\u7531\u4e8e\u957f\u671f\u5438\u70df\u3001\u8fc7\u654f\u3001\u75c5\u6bd2\u611f\u67d3\u7b49\u56e0\u7d20\u5f15\u8d77\u7684\u3002\u9884\u9632\u80ba\u90e8\u5de6\u4e0a\u53f6\u75c5\u53d8\u7684\u65b9\u6cd5\u5305\u62ec\uff1a\n1. \u51cf\u5c11\u5438\u70df\uff1a\u5438\u70df\u4f1a\u589e\u52a0\u80ba\u90e8\u75be\u75c5\u7684\u98ce\u9669\uff0c\u5c24\u5176\u662f\u5de6\u4e0a", "answer_id": "9d7mCmZaaY3vSGAQb5yYbk", "gt_response": "\u5fcc\u70df,\u907f\u514d\u5267\u70c8\u8fd0\u52a8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1602, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "aU9PgCNg2tmdf84g5c7tSK", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1603, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "Akj6rpxkVfHnmuf9FgYi7w", "gt_response": "X\u5149", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1604, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "keBWUpCTN4Nsru7TWMpUnp", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1605, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "amN3ip9ZcV9TPNRRCDSwYm", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1606, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "6Qim2Ruf2w28drGroE7kEo", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1607, "prompt": "\u56fe\u4e2d\u80ba\u708e\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80ba\u708e\u4f4d\u4e8e\u80ba\u7684\u5de6\u4fa7", "answer_id": "ZSDW3tub42xin28arVUkCJ", "gt_response": "\u5de6\u4e0b\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1608, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "d4gQr3DtnLfe6YYR3ToTdV", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1609, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u8179\u90e8\u4e48?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "JyvHu3sK4bNeC6ArJEc8t9", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1610, "prompt": "\u75c5\u4eba\u7684\u5de6\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "W4HRzmwGehxcHooEL3vDmz", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1611, "prompt": "\u80ba\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "JsfmSZ8cPgHTxLHWi9QhNZ", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1612, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u708e\u5417?\n<image>", "pred_response": "\u4e0d\u80fd", "answer_id": "4idB59qJnYHLMXXt7kMDce", "gt_response": "\u53ef\u4ee5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1613, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u7070\u8272", "answer_id": "F7gJz9YkSfCkRyqEW6gvim", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1614, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u5668\u5b98?\n<image>", "pred_response": "2", "answer_id": "jAZCCb3CR2hqK9x5oaHusM", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1615, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u5fc3\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "ZAAxpbnFEZyyDqPMvs3mqm", "gt_response": "\u63a8\u52a8\u8840\u6db2\u6d41\u52a8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1616, "prompt": "\u5982\u4f55\u9884\u9632\u80ba\u90e8\u5de6\u4e0b\u53f6\u75c5\u53d8\u7684\u53d1\u751f?\n<image>", "pred_response": "\u80ba\u90e8\u5de6\u4e0b\u53f6\u75c5\u53d8\u662f\u7531\u4e8e\u957f\u671f\u5438\u70df\u548c\u73af\u5883\u6c61\u67d3\u6240\u81f4\uff0c\u56e0\u6b64\u9884\u9632\u63aa\u65bd\u5305\u62ec\u51cf\u5c11\u5438\u70df\u3001\u4fdd\u6301\u826f\u597d\u7684\u751f\u6d3b\u4e60\u60ef\u3001\u5b9a\u671f\u4f53\u68c0\u7b49\u3002", "answer_id": "gQnyxNuqBmWgWfGHgycyog", "gt_response": "\u9632\u5bd2\u4fdd\u6696,\u589e\u5f3a\u4f53\u8d28", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1617, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "FneuhGTzNGHrPWaVqpBYoZ", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1618, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "dXfnBU6BYwXF4TaFJWmSJk", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1619, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "JYTJcfzwACERabM2QdVXrp", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1620, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u4e0d\u77e5\u9053", "answer_id": "cd35sMmWniMRoGv5wyMSCD", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1621, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "U5LM2CsPE4tX7vxLGsAne4", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1622, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "E3jV2ETynpjHbv46pkjPC2", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1623, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "9nH5PcCJ22GtHdNw5FDwgN", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1624, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "e7ypKGguhcycwyMbiC7FJN", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1625, "prompt": "\u813e\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "AUHNiVTKX8vxaFUPdurAR6", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1626, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7X\u5149\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "2McWPijbyX97VvqMzHEFXs", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1627, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "ZVeLSzP4aPrkqJWS7vYgbh", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1628, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u4e0d", "answer_id": "CZfG2k6VsnnNibX9LxiFeB", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1629, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "QWYpXcD3UFjvoesmXThWMA", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1630, "prompt": "\u56fe\u4e2d\u80ba\u708e\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "juyj88dfaiqDtJYHwyXVaz", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1631, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "JMWyCEnvGZaR5NUSyDZA9m", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1632, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "TFewoouNAiYrByyYPpezZF", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1633, "prompt": "\u75c5\u4eba\u7684\u5de6\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Rs6rPBncWHAWtigKCk7qJC", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1634, "prompt": "\u56fe\u7247\u4e2d\u5305\u62ec\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "HUbKZyV3oxBLidCGi2J39P", "gt_response": "\u80ba\u708e", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1635, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u708e\u5417?\n<image>", "pred_response": "\u4e0d\u662f\u3002\u8fd9\u5f20\u56fe\u7247\u662f\u4e00\u4e2a\u80f8\u90e8X\u5c04\u7ebf\u56fe\uff0c\u5c55\u793a\u4e86\u80f8\u8154\u548c\u80f8\u817a\u7684\u7ed3\u6784\u3002\u80ba\u708e\u662f\u4e00\u79cd\u75be\u75c5\uff0c\u4f46\u5b83\u4e0d\u662f\u8fd9\u5f20\u56fe\u7247\u6240\u5c55\u793a\u7684\u5185\u5bb9\u3002", "answer_id": "UN26SyrY4VtwSmuhYBnMQW", "gt_response": "\u53ef\u4ee5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1636, "prompt": "\u56fe\u4e2d\u7684\u80ba\u662f\u4ec0\u4e48\u989c\u8272\u7684?\n<image>", "pred_response": "\u80ba\u662f\u9ed1\u8272\u7684\u3002", "answer_id": "X9isMbxKPdqEPBrkoQxbYj", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1637, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u80ba?\n<image>", "pred_response": "2", "answer_id": "CexFjWdKqTJS4HEmd75LsM", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1638, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u9ed1\u8272\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u68c0\u67e5", "answer_id": "8EQZN3A9SKHWuaUdeX3aoT", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1639, "prompt": "\u56fe\u4e2d\u53f3\u4e0b\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u6839\u636e\u56fe\u4e2d\u7684\u80ba\u90e8\u75be\u75c5\uff0c\u75c5\u4eba\u53ef\u80fd\u4f1a\u51fa\u73b0\u547c\u5438\u56f0\u96be\u3001\u75bc\u75db\u548c\u54b3\u55fd\u7b49\u75c7\u72b6\u3002\u8fd9\u4e9b\u75c7\u72b6\u53ef\u80fd\u4f1a\u5f71\u54cd\u75c5\u4eba\u7684\u751f\u6d3b\u8d28\u91cf\u548c\u5065\u5eb7\u72b6\u51b5\u3002", "answer_id": "AYC4YqpBEAvXcqmvbQxrUN", "gt_response": "\u54b3\u6ce1\u6cab\u9ecf\u6db2\u75f0,\u547c\u5438\u56f0\u96be,\u54b3\u55fd,\u80f8\u75db", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1640, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "gbRkgbX7SPAcvnkyshoH9F", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1641, "prompt": "\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u88ab\u8fd9\u5f20\u56fe\u7247\u4f7f\u7528?\n<image>", "pred_response": "CT", "answer_id": "gcwkx5PKNKV47mtxKbtsBm", "gt_response": "X\u5149", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1642, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "Rijvb2LZpvngfKDrEYgmKU", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1643, "prompt": "\u56fe\u50cf\u5728\u8eab\u4f53\u91cc\u4ee3\u8868\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "YXza4L8gbCd49qHLfVY2j4", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1644, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "gbAZMjprw9BP6sNTctwXeQ", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1645, "prompt": "\u56fe\u4e2d\u80ba\u708e\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "Ugd3XmKJUMMtBDHBcT4E5c", "gt_response": "\u53f3\u4e0a\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1646, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "9xuwewmJttWendY4dDyPnD", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1647, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "f4zzi5D9C7p6g49GzSV33p", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1648, "prompt": "\u75c5\u4eba\u7684\u53f3\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "GA5JK6PzXsJLi5tZNKNuPB", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1649, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "9VU8sUhKbEwDKYBjb8ySC6", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1650, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u708e\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "dtwM5gF2NRAfXAFqbnpqpP", "gt_response": "\u53ef\u4ee5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1651, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "hwjEM3EErfyiZ3XFBmiB2V", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1652, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u5fc3\u810f?\n<image>", "pred_response": "0", "answer_id": "bdUKsdRRzrpPd2hbq654Sy", "gt_response": "1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1653, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u5668\u5b98\u7684\u529f\u80fd\u662f\u5c55\u793a\u80f8\u8154\u548c\u8179\u90e8\u7684\u7ed3\u6784\uff0c\u5305\u62ec\u80ba\u3001\u80c6\u56ca\u3001\u80a0\u9053\u548c\u809d\u810f\u3002", "answer_id": "gYKC7zexe3CTG4XTVPHMQB", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1654, "prompt": "\u56fe\u4e2d\u53f3\u4e0a\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u6839\u636e\u56fe\u4e2d\u7684\u80f8\u90e8\u75be\u75c5\uff0c\u75c5\u4eba\u53ef\u80fd\u4f1a\u51fa\u73b0\u547c\u5438\u56f0\u96be\u3001\u75bc\u75db\u3001\u54b3\u55fd\u3001\u547c\u5438\u56f0\u96be\u7b49\u75c7\u72b6\u3002\u8fd9\u4e9b\u75c7\u72b6\u53ef\u80fd\u662f\u7531\u4e8e\u80ba\u90e8\u75be\u75c5\u5f15\u8d77\u7684\uff0c\u5982\u80ba\u708e\u3001\u80ba\u6c14\u80bf\u3001\u80ba\u764c\u7b49\u3002\u75c5\u4eba\u9700\u8981", "answer_id": "3qabrFd3eEVoTw8QLqnnG2", "gt_response": "\u54b3\u6ce1\u6cab\u9ecf\u6db2\u75f0,\u547c\u5438\u56f0\u96be,\u54b3\u55fd,\u80f8\u75db", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1655, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "5yCXfUxz5DyQCxfip7f7Lh", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1656, "prompt": "\u8fd9\u662f\u6838\u78c1\u5171\u632f\u626b\u63cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "URVcuRjWeju677brWZmeUm", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1657, "prompt": "\u8fd9\u662f\u6a2a\u65ad\u9762\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "BBKoeZmKsJyr62d9noMaFw", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1658, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "m66eQKCU5vzTZdFTLMcnNF", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1659, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4e2a\u53f6\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "X9Em33aeUdEEkqxNgJTHed", "gt_response": "\u53cc\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1660, "prompt": "\u56fe\u4e2d\u7ed3\u8282\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "bvW55Dhkj8ducAXyZcf2to", "gt_response": "\u5de6\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1661, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "KP5Unf7YzCvoDXBUfNEGpm", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1662, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "FHTeeae4MDUj68GYwSvCSn", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1663, "prompt": "\u80ba\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "4bxVF64g6YYhY6Ld9XqEuN", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1664, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "f8aKkuPmwCjeHX3qr29h44", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1665, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u7ed3\u8282\u5417?\n<image>", "pred_response": "\u4e0d\u662f\u3002", "answer_id": "hgvZESxhyefrGg9au2UaiB", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1666, "prompt": "\u56fe\u4e2d\u53f3\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u7070\u8272", "answer_id": "SrgjbNL9CB5Q5fW3HCvaJT", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1667, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u5668\u5b98?\n<image>", "pred_response": "1", "answer_id": "kevpnR5mGuvZsW58H7txQ6", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1668, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "847LELJhX3y3MUuJRTTzAK", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1669, "prompt": "\u56fe\u4e2d\u80ba\u90e8\u5de6\u4e0b\u89d2\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba\u90e8\u5de6\u4e0b\u89d2\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u80ba\u764c\u3002", "answer_id": "X2UG53yX2tonvNLrNV7vkj", "gt_response": "\u80ba\u90e8\u611f\u67d3,\u6162\u6027\u523a\u6fc0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1670, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "FHKs4nKed95rCqPNDHRXms", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1671, "prompt": "\u8fd9\u662f\u6838\u78c1\u5171\u632f\u626b\u63cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "dvZpg3VyRtCJ2xw3pQ2K9m", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1672, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u626b\u63cf\u5e73\u9762\u662f\u80f8\u8154", "answer_id": "Yr39K6GnC4x7A8uR4sc53d", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1673, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "JVC4eH29N8JQVFrdUFKFC3", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1674, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "jnJzPgN6XpARwX6bpfSB2q", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1675, "prompt": "\u56fe\u4e2d\u80ba\u4e0d\u5f20\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "G7axsq4yu5vBJZTKMH2NfM", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1676, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u8179\u90e8\u4e48?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "GYVgTCRJdBTyhz2BdkwsVe", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1677, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "QCX5Gcg9ueQ99Mo8KTL6nK", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1678, "prompt": "\u80ba\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "92K8Vz4z4RYWHLeARrcgU4", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1679, "prompt": "\u75c5\u4eba\u7684\u53f3\u80ba\u6709\u5f02\u5e38\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "CxrqvN8vxNE4Rrm82jsdoY", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1680, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u5de6\u4e0b\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u80ba\u4e0d\u5f20\u5417?\n<image>", "pred_response": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d\uff0c\u5de6\u4e0b\u80ba\u6ca1\u6709\u89c2\u5bdf\u5230\u80ba\u4e0d\u5f20\u3002", "answer_id": "6MTNX5vZS45h4EnQvtMy3x", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1681, "prompt": "\u56fe\u4e2d\u7684\u80ba\u662f\u4ec0\u4e48\u989c\u8272\u7684?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "iBQv89cTV8VDGYiwRNZivd", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1682, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u5fc3\u810f?\n<image>", "pred_response": "0", "answer_id": "C6sVrHsRM3WS9BijiEMpfo", "gt_response": "1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1683, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8fd9\u5f20\u7167\u7247\u662f\u4e00\u4e2a\u80f8\u90e8X\u5c04\u7ebf\u56fe\uff0c\u5c55\u793a\u4e86\u80f8\u8154\u548c\u80f8\u8154\u5185\u7684\u5668\u5b98\u3002\u80f8\u8154\u5305\u542b\u80ba\u90e8\u548c\u5fc3\u810f\uff0c\u80ba\u90e8\u8d1f\u8d23\u547c\u5438\uff0c\u5fc3\u810f\u5219\u8d1f\u8d23\u8840\u6db2\u7684\u5faa\u73af\u3002\u8fd9\u5f20\u7167\u7247\u53ef\u4ee5\u5e2e\u52a9\u533b\u751f\u8bca\u65ad\u548c\u6cbb\ufffd\ufffd", "answer_id": "MpxfdFqah6jaC6qfV9rAYb", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1684, "prompt": "\u5982\u4f55\u9884\u9632\u80ba\u90e8\u53f3\u4e0b\u53f6\u75c5\u53d8\u7684\u53d1\u751f?\n<image>", "pred_response": "\u80ba\u90e8\u53f3\u4e0b\u53f6\u75c5\u53d8\u662f\u7531\u4e8e\u957f\u671f\u5438\u70df\u3001\u8fc7\u654f\u3001\u75c5\u6bd2\u611f\u67d3\u7b49\u56e0\u7d20\u5f15\u8d77\u7684\u3002\u9884\u9632\u80ba\u90e8\u53f3\u4e0b\u53f6\u75c5\u53d8\u7684\u65b9\u6cd5\u5305\u62ec\uff1a\n1. \u51cf\u5c11\u5438\u70df\uff1a\u5438\u70df\u4f1a\u589e\u52a0\u80ba\u90e8\u75be\u75c5\u7684\u98ce\u9669\uff0c\u5c24\u5176\u662f\u5bf9\ufffd", "answer_id": "kxAp38SRx6u5S7TuADKaEt", "gt_response": "\u5fcc\u70df,\u589e\u5f3a\u4f53\u8d28", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1685, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "oVuB7tbyGe5jcb8LVzoESH", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1686, "prompt": "\u8fd9\u662fX\u5149\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "NyD9hcADmU5swHvta87Tj8", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1687, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u626b\u63cf\u5e73\u9762\u662f\u80f8\u8154", "answer_id": "CTXPCoG9oNXjgrof6suh4V", "gt_response": "\u51a0\u72b6\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1688, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "iwLN7cWLfCHAQcMZ52Y5ue", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1689, "prompt": "\u56fe\u4e2d\u80f8\u8154\u54ea\u4e00\u90e8\u5206\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "579Q6xZqMiLKQfRkifBLFr", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1690, "prompt": "\u5fc3\u810f\u80a5\u5927\u4f4d\u4e8e\u80f8\u8154\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "SSFV6H8GdeysF3wCVCUGsQ", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1691, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u9888\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "4MTq26YzvwaqSa8MErT3ji", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1692, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "BbntvPjhpwT73Mvnoi9tNE", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1693, "prompt": "\u56fe\u4e2d\u80f8\u8154\u5b58\u5728\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u80f8\u8154\u7f3a\u8840", "answer_id": "GgdoMtBaXtrcAipSZNCtci", "gt_response": "\u5fc3\u810f\u80a5\u5927", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1694, "prompt": "\u56fe\u4e2d\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "SDdwTTJSGpPE8cCP5yj7gC", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1695, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53ef\u4ee5\u89c2\u5bdf\u5230\u5fc3\u810f\u80a5\u5927\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "HEkaAwjXgzq4a8smpHEZgz", "gt_response": "\u53ef\u4ee5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1696, "prompt": "\u56fe\u4e2d\u5de6\u80ba\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "3iwYXaYuvHXiu9PAhvxC9R", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1697, "prompt": "\u56fe\u4e2d\u5b58\u5728\u591a\u5c11\u4e2a\u5fc3\u810f?\n<image>", "pred_response": "0", "answer_id": "k28SiehiWyHM2RGiYgJsZo", "gt_response": "1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1698, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u7684\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8fd9\u5f20\u7167\u7247\u4e2d\u4e3b\u8981\u7684\u5668\u5b98\u662f\u80f8\u8154\u548c\u80f8\u80ba\u3002", "answer_id": "3r9jMDEQk9VL2njDGVQ9rE", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1699, "prompt": "\u5982\u4f55\u6cbb\u7597\u4f4d\u4e8e\u56fe\u4e2d\u5fc3\u810f\u7684\u75be\u75c5?\n<image>", "pred_response": "\u60a3\u8005\u7684\u75be\u75c5\u662f\u80ba\u764c\uff0c\u9700\u8981\u8fdb\u884c\u80ba\u5207\u9664\u624b\u672f\u3002", "answer_id": "LuasPxJUT8oHWgtxkcb5zM", "gt_response": "\u7269\u7406\u6cbb\u7597,\u836f\u7269\u6cbb\u7597", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1700, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "ZsjCcygbRdZLj9PouewjJa", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1701, "prompt": "\u8fd9\u5f20\u7167\u7247\u662f\u600e\u4e48\u62cd\u7684?\n<image>", "pred_response": "CT", "answer_id": "nkPL7rxfMWjKFj4aLwiynQ", "gt_response": "X\u5149", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1702, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "3S6meJSrN597DvzuVRsnFC", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1703, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u80f8\u8154?\n<image>", "pred_response": "\u662f", "answer_id": "EJM33ygp5VYnjKDChpVsoZ", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1704, "prompt": "\u56fe\u4e2d\u80ba\u54ea\u4e00\u4fa7\u662f\u5f02\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "fbwWeb7jmPzWnFzLtz7JA8", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1705, "prompt": "\u56fe\u4e2d\u79ef\u6db2\u4f4d\u4e8e\u80ba\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u80ba", "answer_id": "AKKbVSK7zuASJ2okFGkPkr", "gt_response": "\u53f3\u4e0b\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1706, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "fZ5agemKFQoiEk4Pffao7p", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1707, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "iUhfsdnXT3bGqDhyzqtzXJ", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1708, "prompt": "\u56fe\u7247\u4e2d\u5305\u62ec\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "eUPXNEPWSFSgty8Z3w88Pn", "gt_response": "\u80f8\u8154\u79ef\u6db2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1709, "prompt": "\u56fe\u4e2d\u80ba\u7684\u53f3\u4e0b\u4fa7\u662f\u4ec0\u4e48\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "jLeAWUCn4ocyU7ZghZ5Wuz", "gt_response": "\u80f8\u8154\u79ef\u6db2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1710, "prompt": "\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d,\u53f3\u4e0a\u80ba\u53ef\u4ee5\u89c2\u5bdf\u5230\u79ef\u6db2\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "dwCa6LfmMbEv5RbRNiZZZ7", "gt_response": "\u4e0d\u53ef\u4ee5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1711, "prompt": "\u56fe\u4e2d\u7684\u80ba\u662f\u4ec0\u4e48\u989c\u8272\u7684?\n<image>", "pred_response": "\u767d\u8272", "answer_id": "4BenSFpYVb4acM65TUxZFQ", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1712, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u5668\u5b98?\n<image>", "pred_response": "2", "answer_id": "UQ4kdKDFETk4ngsrSaHKFu", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1713, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u9ed1\u8272\u7684\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u6765\u505a\u4ec0\u4e48?\n<image>", "pred_response": "\u505a\u80ba\u708e", "answer_id": "3LFMGTuL5YnfXhWf9rGDoQ", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1714, "prompt": "\u56fe\u4e2d\u53f3\u4e0b\u80ba\u7684\u75be\u75c5\u4f1a\u7ed9\u75c5\u4eba\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u6839\u636e\u56fe\u4e2d\u7684\u80ba\u90e8\u75be\u75c5\uff0c\u75c5\u4eba\u53ef\u80fd\u4f1a\u51fa\u73b0\u547c\u5438\u56f0\u96be\u3001\u75b2\u52b3\u3001\u54b3\u55fd\u3001\u54b3\u75f0\u3001\u54b3\u55fd\u547c\u5438\u56f0\u96be\u7b49\u75c7\u72b6\u3002\u8fd9\u4e9b\u75c7\u72b6\u53ef\u80fd\u4f1a\u5f71\u54cd\u75c5\u4eba\u7684\u751f\u6d3b\u8d28\u91cf\u548c\u5065\u5eb7\u72b6\u51b5\u3002", "answer_id": "ffXP64A5sYtEayfYopq5nT", "gt_response": "\u80f8\u95f7,\u547c\u5438\u56f0\u96be,\u80f8\u75db", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1715, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "MjCxJu4cbCszCxwHLRXm4T", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1716, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "jn86ihiiQaX5WMgnLsn6wy", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1717, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "HHSuv9H7S92ZKLkFEGZyK9", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1718, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u8fd9\u5f20MRI\u56fe\u7247\u4f7f\u7528\u4e86\u5747\u503c\u6743\u91cd\u6cd5\u3002", "answer_id": "YdFkXdhTyW5EfAUeASvx9B", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1719, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "A9bmHav9dzxMWA5hijF8we", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1720, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "LnNyCNpptU2AMANWTtj3bA", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1721, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "2kFTyjrTFDYm7fkj4ke4UQ", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1722, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "BCNfRviZXCZDgtSHXvogp7", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1723, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "N692Zu4XiHCyNRtxrsmUUT", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1724, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u4e0d\u77e5\u9053", "answer_id": "WWkwRGjyKMxMZkgXrcT69K", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1725, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "QYoMmDvQG5CtJXpa2f8VzA", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1726, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "VX5vtPHo2WgYjT6W7fmTTs", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1727, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "gHQbVopJAY5SNeGbvQM7Yq", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1728, "prompt": "\u809d\u810f\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "d2Tee7SN4b35J65h2kyCQo", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1729, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "2", "answer_id": "SrAVnLFSz3vxWsVqyKbp2a", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1730, "prompt": "\u809d\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "4rhuqV7uM6wPcMBCzShFyP", "gt_response": "\u5065\u5eb7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1731, "prompt": "\u80be\u810f\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "m2qsnLmjVKJC5qC57dDg9o", "gt_response": "\u5065\u5eb7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1732, "prompt": "\u8fd9\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u533b\u5b66\u56fe\u50cf?\n<image>", "pred_response": "\u7167\u7247", "answer_id": "N7RZySE4ANS6xir8ZqR93S", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1733, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u626b\u63cf\u5e73\u9762\u662f\u6a2a\u5411\u7684\u3002", "answer_id": "YsTb9GBRKwtBNHxKDn4JLL", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1734, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u9888\u90e8?\n<image>", "pred_response": "\u662f", "answer_id": "VxBEyBSxKF56zZqup3Biat", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1735, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u662f\u5589?\n<image>", "pred_response": "\u5de6", "answer_id": "8idKprGdWEYaWgd2wMJ53D", "gt_response": "\u9ed1\u8272\u7a7a\u6d1e", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1736, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u9888\u90e8", "answer_id": "d4jwhwQsEADw4W28Lo5z6s", "gt_response": "\u9888\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1737, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u9888\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "EjfhnsYrrkVZmKXKtNKv3j", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1738, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u98df\u9053\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "FdHcqqq7V4jEu48gEBEJZQ", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1739, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u80ba\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "aAqY9QpU4DFo4wBExhdzwZ", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1740, "prompt": "\u56fe\u4e2d\u5589\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "T7PxTM3iVdNAPcBjLLRcTH", "gt_response": "\u9ed1\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1741, "prompt": "\u56fe\u4e2d\u5589\u662f\u4ec0\u4e48\u5f62\u72b6\u7684?\n<image>", "pred_response": "\u5706\u5f62", "answer_id": "7PjwgHajNPEFEQFALoTQn4", "gt_response": "\u692d\u5706", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1742, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u4e0a\u65b9\u7684\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8fd9\u4e2a\u56fe\u50cf\u4e0a\u65b9\u7684\u5668\u5b98\u662f\u9888\u90e8\u7684\u652f\u6491\u5668\u5b98\uff0c\u5177\u6709\u652f\u6491\u9888\u90e8\u7684\u529f\u80fd\u3002", "answer_id": "TWQPBvG4ZFDFSdYqqRmGsP", "gt_response": "\u901a\u6c14,\u53d1\u97f3", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1743, "prompt": "\u4f4d\u4e8e\u56fe\u50cf\u9876\u90e8\u7684\u5668\u5b98\u5c5e\u4e8e\u4ec0\u4e48\u7cfb\u7edf?\n<image>", "pred_response": "\u547c\u5438\u7cfb\u7edf", "answer_id": "L7XX9fB93x643yDdXzSnfz", "gt_response": "\u547c\u5438\u7cfb\u7edf,\u6d88\u5316\u7cfb\u7edf", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1744, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u795e\u7ecf\u7cfb\u7edf?\n<image>", "pred_response": "\u8111", "answer_id": "STw2oVsApF6X6Aj4TziaJg", "gt_response": "\u810a\u9ad3", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1745, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "PUFiXB2atP9pFXTXAT8CqT", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1746, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u5206\u6ccc\u9176\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "6z2qykNTtntMLXtFFsbPsT", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1747, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "jXJiVJ9cGy3jqAHqViuDrE", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1748, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "KqjhwmjhnRtcxcURfzRDR4", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1749, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "m9KV7zFXZBwRbPBWFGnSFN", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1750, "prompt": "\u4ec0\u4e48\u5668\u5b98\u6807\u5fd7\u7740\u5668\u5b98\u7cfb\u7edf\u7684\u4e0b\u7aef?\n<image>", "pred_response": "\u810a\u690e", "answer_id": "QtQPpVadfQ3ANhhnubt3fS", "gt_response": "\u4e0b\u988c\u9aa8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1751, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "CTTTnWjbCFUzzYPB7VNZcz", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1752, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u5c0f\u80a0\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "jwWDiva6344dnisbGgiCAR", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1753, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u662f\u5426\u5305\u542b\u8180\u80f1?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "7reJjQXJSoFormVtGbUfiE", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1754, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u9888\u90e8", "answer_id": "8SeCBHPvRchNvS9Pye5iVU", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1755, "prompt": "\u56fe\u4e2d\u6709\u6ca1\u6709\u4e00\u4e9b\u5668\u5b98\u63a7\u5236\u5fc3\u8df3\u548c\u547c\u5438?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "8uufAU3AsYa8vjd7ZqaciZ", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1756, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7CT\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "mLMaXFkmqiRZe6ups3BLmu", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1757, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "MWs4sgAvxFZbMnfE5fs782", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1758, "prompt": "\u4ece\u6574\u4e2a\u8eab\u4f53\u7684\u89d2\u5ea6\u770b,\u8fd9\u5e45\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u9888\u90e8", "answer_id": "SbiDfupKbiBjWGzDAdWoRn", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1759, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u9888\u90e8", "answer_id": "gnwoBDQavnJa742ME7S6GE", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1760, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u5668\u5b98\u6210\u5bf9\u51fa\u73b0?\n<image>", "pred_response": "\u9888\u90e8", "answer_id": "FcVAXBRPhjqc7V8hekNhFG", "gt_response": "\u816e\u817a", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1761, "prompt": "\u8fd9\u5e45\u56fe\u50cf\u91cc\u6709\u816e\u817a\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "8AzG9KM7teHutgQaLV6zp8", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1762, "prompt": "\u4e0b\u988c\u9aa8\u51fa\u73b0\u5728\u8fd9\u5f20\u56fe\u4e0a\u4e86\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "6PE5Zjekdz7rjUuK4RRswA", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1763, "prompt": "\u56fe\u7247\u4e2d\u7684\u4e0b\u988c\u9aa8\u662f\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "SAHCwvCpsHYL3NqtQmcCrS", "gt_response": "\u767d\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1764, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u9888\u90e8", "answer_id": "EowNQvXkJQ9yfjsU2stWrq", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1765, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u795e\u7ecf\u7cfb\u7edf?\n<image>", "pred_response": "\u8111", "answer_id": "baJGcXhvxPFicNcS2L4Tro", "gt_response": "\u810a\u9ad3", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1766, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u53ef\u4ee5\u6392\u6cc4\u673a\u4f53\u5e9f\u7269\u8d28\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Z8HHhu3VyE9q5Qy2EY794v", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1767, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "TYRsKcZ7eTHoWBrQvtPDNx", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1768, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "CT", "answer_id": "hbSHr3UWEWAfpevwsx7fDz", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1769, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "4iaCMUoE3pSPjk4JNBaip8", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1770, "prompt": "\u56fe\u50cf\u5728\u8eab\u4f53\u91cc\u4ee3\u8868\u4ec0\u4e48?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "jWmxXkoCmrt5jdnY9rhgfw", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1771, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "mweWHfjCTdkzTj7jyBvkgk", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1772, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u9888\u90e8", "answer_id": "BRnsv8MwfzZMRYUozZ8qEZ", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1773, "prompt": "\u8fd9\u5e45\u56fe\u50cf\u91cc\u6709\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "TeoDni9HRyyBsew2VqPtRC", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1774, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u76f4\u80a0\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "GcC8aGS2U74e5JLdrkE3KH", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1775, "prompt": "\u810a\u9ad3\u5728\u56fe\u7247\u4e2d\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u9ed1\u8272", "answer_id": "mHMYpyVDTCKDP3GerXr9Lq", "gt_response": "\u7070\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1776, "prompt": "\u56fe\u7247\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5934", "answer_id": "62HWQcXyPbRDktQUxzffQr", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1777, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u795e\u7ecf\u7cfb\u7edf?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "3WkPd9am6kokqeQMxJvUJC", "gt_response": "\u810a\u9ad3", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1778, "prompt": "\u8fd9\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u533b\u5b66\u56fe\u50cf?\n<image>", "pred_response": "CT", "answer_id": "JvAzvzpA4gVQsvEfvtBp84", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1779, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "SVfU5yGhvqjqmVnbLLqhCZ", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1780, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u9888\u90e8?\n<image>", "pred_response": "\u4e0d", "answer_id": "6eY4FwmqrQTqWADYSrsxvP", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1781, "prompt": "\u56fe\u4e2d\u54ea\u4e2a\u662f\u6c14\u7ba1?\n<image>", "pred_response": "\u5de6", "answer_id": "B8rXphotf2bwdBvm9UL8kC", "gt_response": "\u9ed1\u8272\u7a7a\u6d1e", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1782, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u810a\u690e", "answer_id": "UssW9PMhc45EsSoaBZh8eV", "gt_response": "\u9888\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1783, "prompt": "\u816e\u817a\u51fa\u73b0\u5728\u56fe\u50cf\u4e2d\u4e86\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "7JofUJHvUHdgCPx2GqVpdP", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1784, "prompt": "\u8fd9\u5e45\u56fe\u50cf\u91cc\u6709\u6c14\u7ba1\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "Xxpyk8pmrEVxW5LRpTuFjC", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1785, "prompt": "\u56fe\u4e2d\u98df\u9053\u663e\u793a\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u7070\u8272", "answer_id": "8gKnbasta3xeQwmugPmBX9", "gt_response": "\u7070\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1786, "prompt": "\u8fd9\u4e2a\u75c5\u4eba\u7684\u6c14\u7ba1\u662f\u4ec0\u4e48\u5f62\u72b6\u7684?\n<image>", "pred_response": "\u5706\u5f62", "answer_id": "8SF3qzC286q9k32vtpKpQ8", "gt_response": "\u4e0d\u89c4\u5219", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1787, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u9876\u90e8\u7684\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u547c\u5438", "answer_id": "SLtF442uVLdyn2kBB9ALA7", "gt_response": "\u8fd0\u9001\u6c14\u4f53", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1788, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u795e\u7ecf\u7cfb\u7edf?\n<image>", "pred_response": "\u810a\u9ad3", "answer_id": "Wc7N5feDgK4rHaPy3MLyPa", "gt_response": "\u810a\u9ad3", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1789, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "o5WxK3dJgSARfK7drcstai", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1790, "prompt": "\u56fe\u4e2d\u662f\u5426\u6709\u53ef\u4ee5\u611f\u77e5\u4e8b\u7269\u7684\u5668\u5b98?\n<image>", "pred_response": "\u4e0d", "answer_id": "mWVUi4Guv9QNr4GbPsN78o", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1791, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7X\u5149\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "WtKm9t2RvRg48A2qWhS92f", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1792, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "62qeLDc7BpUPFLnRoNRZ6B", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1793, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "7L7naTGy5VyZ5sHuG6mQBe", "gt_response": "\u9888\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1794, "prompt": "\u56fe\u4e2d\u98df\u7ba1\u5728\u54ea?\n<image>", "pred_response": "\u5de6", "answer_id": "AXuX27RGRziyMwRtHahDp7", "gt_response": "\u6c14\u7ba1\u4e0b\u65b9", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1795, "prompt": "\u810a\u9ad3\u5728\u8fd9\u5f20\u56fe\u4e0a\u51fa\u73b0\u4e86\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "ijMtztyhZLWeeiUHLMJMGm", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1796, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u810a\u9ad3\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "HgZH6dGcqiGuwRCXHLPoMV", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1797, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u80ba\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "RQH3TGfk3Bn6SiRGHXZLUq", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1798, "prompt": "\u8fd9\u4e2a\u75c5\u4eba\u7684\u98df\u9053\u662f\u4ec0\u4e48\u5f62\u72b6\u7684?\n<image>", "pred_response": "\u5706\u5f62", "answer_id": "TmrYFrU2UZdQSEfjd4nS2q", "gt_response": "\u4e0d\u89c4\u5219", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1799, "prompt": "\u8fd9\u5f20\u7167\u7247\u4e2d\u7684\u9ed1\u8272\u7a7a\u6d1e\u7ec4\u7ec7\u6709\u4ec0\u4e48\u529f\u80fd?\n<image>", "pred_response": "\u9ed1\u8272\u7a7a\u6d1e\u7ec4\u7ec7\u662f\u810a\u690e\u4e2d\u7684\u4e00\u79cd\u7ec6\u80de\uff0c\u5177\u6709\u652f\u6301\u548c\u4fdd\u62a4\u810a\u690e\u7684\u529f\u80fd\u3002\u5b83\u4eec\u5305\u62ec\u9aa8\u76c6\u8154\u3001\u810a\u690e\u9ad3\u7ec6\u80de\u548c\u810a\u9ad3\u7ec6\u80de\u7b49\u3002\u8fd9\u4e9b\u7ec6\u80de\u5728\u810a\u690e\u7684\u751f\u957f\u3001\u7ec6\u80de\u5206\u5316\u548c\ufffd\ufffd", "answer_id": "i8zkwwpzyudE5kAzhPhs9R", "gt_response": "\u98df\u7269\u8fd0\u9001", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1800, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "nQbnsuMBeqJLSrmERXmea6", "gt_response": "\u6c14\u7ba1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1801, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "7vVd2MFR5aFYTw6yfLVCk8", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1802, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u53ef\u4ee5\u8fd0\u9001\u6c14\u4f53\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Bddj48ArwwWnLRzTqFGYkr", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1803, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8fd9\u5f20\u56fe\u7247\u662f\u4e00\u4e2a\u7167\u7247\u7684\u65ad\u5c42\u56fe\uff0c\u5b83\u5c55\u793a\u4e86\u4e00\u4e2a\u4eba\u7684\u5934\u90e8\u7684\u8111\u90e8\u3002", "answer_id": "jYEaMuqffnnFc7o4aXqVTg", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1804, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "mE9PXT2yLXxic8cYXVvG7N", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1805, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u4e0d\u77e5\u9053", "answer_id": "7P88DnGJRh6Y29LPcmfAmf", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1806, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "gXMN6YctDuPcvZkrkSi7Ak", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1807, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "isRnFoDPmWFXmRS3aMbkJK", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1808, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u8111\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "VgnG6fc2DdjXjBY29Ccmi9", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1809, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u766b\u75eb", "answer_id": "5xRP45ggaJjnQicehXBs9u", "gt_response": "\u8111\u6c34\u80bf", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1810, "prompt": "\u5927\u8111\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f\u3002\u5927\u8111\u662f\u6b63\u5e38\u7684\uff0c\u5b83\u662f\u4eba\u7c7b\u7684\u4e2d\u67a2\uff0c\u8d1f\u8d23\u63a7\u5236\u8eab\u4f53\u7684\u5404\u79cd\u6d3b\u52a8\u3002", "answer_id": "G7PozSXaBiYUyYNZRaybsi", "gt_response": "\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1811, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "bMkJ9tThPLryMqxh4Gpugm", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1812, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "CT", "answer_id": "UERUv7HFQMqNQrQc3gvb7S", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1813, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "7i2ph85oBWZ4xcTzjNxv49", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1814, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "FNYng2336XCjxQNdNg8PZK", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1815, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u9888\u90e8?\n<image>", "pred_response": "\u4e0d", "answer_id": "U2LS2hD63vNE7xgSoB5Hpn", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1816, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "ckYLamZhHiM7iJQn79Qd2J", "gt_response": "\u9876\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1817, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "4L6YUTPmrNF23dgtUc3DjQ", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1818, "prompt": "\u56fe\u7247\u4e2d\u53ef\u4ee5\u89c2\u5bdf\u5230\u8111\u6c34\u80bf\u5417?\n<image>", "pred_response": "\u4e0d\u80fd", "answer_id": "epJuR8uHfbKCG4wJgb8U5s", "gt_response": "\u53ef\u4ee5", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1819, "prompt": "\u5982\u4f55\u6cbb\u7597\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u9876\u90e8\u7684\u75be\u75c5?\n<image>", "pred_response": "\u5f88\u62b1\u6b49\uff0c\u4f5c\u4e3a\u4e00\u4e2aAI\u8bed\u8a00\u6a21\u578b\uff0c\u6211\u4e0d\u80fd\u63d0\u4f9b\u5177\u4f53\u7684\u6cbb\u7597\u5efa\u8bae\u3002\u5982\u679c\u60a8\u6709\u5173\u4e8e\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u9876\u90e8\u75be\u75c5\u7684\u95ee\u9898\uff0c\u8bf7\u54a8\u8be2\u533b\u751f\u6216\u4e13\u4e1a\u533b\u7597\u673a\u6784\u3002", "answer_id": "cza6AowRVL7npTH9yAErsw", "gt_response": "\u836f\u7269\u6cbb\u7597,\u652f\u6301\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1820, "prompt": "\u8fd9\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u533b\u5b66\u56fe\u50cf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "eNfSBe3RGHinmAAapNbWGb", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1821, "prompt": "\u8fd9\u5f20\u6838\u78c1\u5171\u632f\u56fe\u50cf\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u8fd9\u5f20\u6838\u78c1\u5171\u632f\u56fe\u50cf\u662f\u7528\u5747\u503c\u6743\u91cd\u6cd5\uff08\u5747\u503c\u6743\u91cd\u6cd5\uff09\u6765\u5904\u7406\u7684\u3002", "answer_id": "54hN56BW5an4CbpKsbcUqY", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1822, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "YMx84bGf8BQa3jm4t9oKbx", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1823, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u662f", "answer_id": "ixgRL5wVxxXUJcptfKeMg6", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1824, "prompt": "\u56fe\u4e2d\u8111\u6c34\u80bf\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "EyRJSSo2WkNK2hseRDSwZ4", "gt_response": "\u5de6\u4e0a\u53f6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1825, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "Kz6FzdsnGHRUZ4TNFcEyrM", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1826, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u8fd9\u4e2a\u56fe\u7247\u6ca1\u6709\u663e\u793a\u4efb\u4f55\u75be\u75c5\u3002\u5b83\u662f\u4e00\u4e2a\u9ed1\u8272\u80cc\u666f\u4e0b\u7684\u8111\u90e8\u7167\u7247\uff0c\u6ca1\u6709\u4efb\u4f55\u75c5\u7076\u6216\u75c7\u72b6\u3002", "answer_id": "m3GHKcS2a8TcGfbRcXi2pY", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1827, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "1", "answer_id": "Xav2xP9u87MuVDUuV6D6uH", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1828, "prompt": "\u5982\u4f55\u9884\u9632\u56fe\u7247\u4e2d\u5927\u8111\u5de6\u4e0a\u53f6\u75be\u75c5\u7684\u53d1\u751f?\n<image>", "pred_response": "\u9884\u9632\u5927\u8111\u5de6\u4e0a\u53f6\u75be\u75c5\u7684\u53d1\u751f\u5305\u62ec\u4fdd\u6301\u5065\u5eb7\u7684\u751f\u6d3b\u65b9\u5f0f\u548c\u5b9a\u671f\u505a\u5065\u5eb7\u68c0\u67e5\u3002\u4fdd\u6301\u5065\u5eb7\u7684\u751f\u6d3b\u65b9\u5f0f\u5305\u62ec\u9075\u5faa\u8425\u517b\u5747\u8861\u7684\u996e\u98df\uff0c\u907f\u514d\u8fc7\u5ea6\u5438\u70df\u548c\u996e\u9152\uff0c\u4fdd\u6301\u9002\u5f53\u7684\u4f53\u91cd\uff0c\u907f\u514d\u8fc7\u5ea6", "answer_id": "noenAJYAx5cEGze8CuQX2P", "gt_response": "\u6ce8\u610f\u996e\u98df\u536b\u751f,\u52a0\u5f3a\u8eab\u4f53\u7d20\u8d28,\u907f\u514d\u8111\u90e8\u5916\u4f24", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1829, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "CT", "answer_id": "KHoSm3DijtrGZv8nUcrBEw", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1830, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528\u4ec0\u4e48\u6837\u7684\u6743\u91cd\u751f\u6210\u7684,t1\u8fd8\u662ft2?\n<image>", "pred_response": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528 t2 \u6743\u91cd\u751f\u6210\u7684\u3002", "answer_id": "YkFZuJk7btpBdzB8RSyAry", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1831, "prompt": "\u8fd9\u662f\u6a2a\u65ad\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "Ddvx6CTracoapzoi2QDTeC", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1832, "prompt": "\u56fe\u50cf\u5728\u8eab\u4f53\u91cc\u4ee3\u8868\u4ec0\u4e48?\n<image>", "pred_response": "\u8fd9\u4e2a\u56fe\u50cf\u4ee3\u8868\u4e86\u4e00\u4e2a\u4eba\u7684\u5934\u90e8\uff0c\u5305\u62ec\u5927\u8111\u548c\u9888\u90e8\u3002", "answer_id": "PhvCL42TtroHvXU6ebrork", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1833, "prompt": "\u56fe\u4e2d\u8111\u90e8\u589e\u5f3a\u80bf\u7624\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u8111\u90e8\u589e\u5f3a\u80bf\u7624\u5728\u989d\u53f6\u7684\u5de6\u4fa7\u3002", "answer_id": "TPwypWx4feipEf2iBG8MRf", "gt_response": "\u5de6\u53f6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1834, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "6YgRDXfGZyDHWULRNLTttc", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1835, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u766b\u75eb", "answer_id": "L2wsRD5Nwe5xDqffrsysDM", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1836, "prompt": "\u76f8\u5bf9\u4e8e\u5176\u4ed6\u7ec4\u7ec7,\u8111\u80bf\u7624\u662f\u767d\u8272\u7684\u8fd8\u662f\u7070\u8272\u7684?\n<image>", "pred_response": "\u76f8\u5bf9\u4e8e\u5176\u4ed6\u7ec4\u7ec7,\u8111\u80bf\u7624\u662f\u7070\u8272\u7684\u3002", "answer_id": "i9acQCNzrusQnzhiL7DfjF", "gt_response": "\u767d\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1837, "prompt": "\u5982\u4f55\u6cbb\u7597\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u5de6\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u5f88\u62b1\u6b49\uff0c\u4f5c\u4e3a\u4e00\u4e2aAI\u8bed\u8a00\u6a21\u578b\uff0c\u6211\u4e0d\u80fd\u63d0\u4f9b\u5177\u4f53\u7684\u75be\u75c5\u6cbb\u7597\u5efa\u8bae\u3002\u5982\u679c\u60a8\u6709\u4efb\u4f55\u75be\u75c5\u6216\u7591\u8651\uff0c\u8bf7\u54a8\u8be2\u60a8\u7684\u533b\u751f\u6216\u533b\u7597\u4e13\u5bb6\uff0c\u4ee5\u83b7\u5f97\u4e13\u4e1a\u7684\u5e2e\u52a9\u548c\u5efa\u8bae\u3002", "answer_id": "MTvrqKEqW6KVrNjUsoiVp9", "gt_response": "\u836f\u7269\u6cbb\u7597,\u652f\u6301\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1838, "prompt": "\u8fd9\u662f\u6838\u78c1\u5171\u632f\u626b\u63cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "RRZAVaFyoGrKKyzxNpkzan", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1839, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "ADLx3fhWQezkT5mpXHigwV", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1840, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "4gjPAuA7CwbihhqSR5Yrzz", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1841, "prompt": "\u56fe\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "n4QPpZiQq69dyhLjZvFSvF", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1842, "prompt": "\u56fe\u4e2d\u8111\u90e8\u589e\u5f3a\u80bf\u7624\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u8111\u90e8\u589e\u5f3a\u80bf\u7624\u5728\u5706\u5f62\u7a97\u6d1e\u4e2d\u3002", "answer_id": "ZQUQAMqLoGQiWY98PDDbUr", "gt_response": "\u5de6\u53f6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1843, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "MtN2eLc5TUL5PJKqvBithj", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1844, "prompt": "\u5927\u8111\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "9eFmu4BrgmPE9Kyo4My4sf", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1845, "prompt": "\u5982\u4f55\u6cbb\u7597\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u5de6\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u5f88\u62b1\u6b49\uff0c\u4f5c\u4e3a\u4e00\u4e2aAI\u8bed\u8a00\u6a21\u578b\uff0c\u6211\u4e0d\u80fd\u63d0\u4f9b\u5177\u4f53\u7684\u6cbb\u7597\u5efa\u8bae\u3002\u60a8\u5e94\u8be5\u54a8\u8be2\u533b\u751f\u6216\u4e13\u4e1a\u533b\u7597\u673a\u6784\uff0c\u4ee5\u83b7\u5f97\u6709\u5173\u60a8\u7684\u75be\u75c5\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u5efa\u8bae\u3002", "answer_id": "WEyJxRSCV94C33k2ZmBvRv", "gt_response": "\u836f\u7269\u6cbb\u7597,\u652f\u6301\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1846, "prompt": "\u8fd9\u662fCT\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "PkepY9nPKzfMaWpLD8Vt2B", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1847, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u662f\u7684\uff0c\u8fd9\u662fT1\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf\u3002", "answer_id": "NQoqzWLRQmjYjqmaUaQGSq", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1848, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u5728\u54ea\u4e2a\u5e73\u9762\u626b\u63cf\u7684?\n<image>", "pred_response": "\u6a2a", "answer_id": "bNmzz3VCM7PZ7D5c5wuWe8", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1849, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u662f", "answer_id": "3xspAncoAzhZpGh7FhKZ4u", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1850, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "RCMrWGA9jVKYXvigD5TumY", "gt_response": "\u5de6\u4e0a\u53f6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1851, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "NrgnwbccfVZwybeZAK4PUV", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1852, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u766b\u75eb", "answer_id": "iKv26f65NGrDTyeGuuGz55", "gt_response": "\u8111\u6c34\u80bf", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1853, "prompt": "\u4ea7\u751f\u56fe\u4e2d\u5927\u8111\u5de6\u4e0a\u4fa7\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u4ea7\u751f\u56fe\u4e2d\u5927\u8111\u5de6\u4e0a\u4fa7\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u7f3a\u8840\u6027\u8111\u5352\u4e2d\u3002", "answer_id": "jTdroYtVT3N4yNJJQyb8Eb", "gt_response": "\u9885\u8111\u635f\u4f24,\u9885\u5185\u5360\u4f4d\u6027\u75c5\u53d8,\u9885\u5185\u708e\u75c7,\u8111\u8840\u7ba1\u75c5\u53d8,\u8111\u7f3a\u6c27", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1854, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8fd9\u5f20\u56fe\u7247\u662f\u4e00\u4e2a\u7167\u7247\u7684\u65ad\u5c42\u626b\u63cf\u6210\u50cf\uff0c\u4e5f\u79f0\u4e3a\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u3002", "answer_id": "G5aszWNdoVrB8WtTGEZS3D", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1855, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528\u4ec0\u4e48\u6837\u7684\u6743\u91cd\u751f\u6210\u7684,t1\u8fd8\u662ft2?\n<image>", "pred_response": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528t2\u6743\u91cd\u751f\u6210\u7684\u3002", "answer_id": "eehVBb9zBD3A4SmHaWR4Df", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1856, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "Be8QWiJ9VRmrjRrVwdQS3d", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1857, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "BE76PpkB6GsRiuJAvkXmFh", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1858, "prompt": "\u56fe\u4e2d\u8111\u54ea\u4e2a\u53f6\u662f\u6b63\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u534a\u5e45", "answer_id": "Y2Le86V74qWsoYDg3Qftph", "gt_response": "\u5de6\u53f6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1859, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "Y4nHDP6ZjPahkGhX4QHxeh", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1860, "prompt": "\u56fe\u4e2d\u8111\u90e8\u53f3\u4fa7\u663e\u793a\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u75be\u75c5\u4e0d\u660e", "answer_id": "CsBLzUSDPWCRwmy2vtJUh7", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1861, "prompt": "\u5982\u4f55\u6cbb\u7597\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u53f3\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u5f88\u62b1\u6b49\uff0c\u6211\u4e0d\u80fd\u63d0\u4f9b\u4efb\u4f55\u533b\u7597\u5efa\u8bae\u6216\u6cbb\u7597\u65b9\u6848\uff0c\u56e0\u4e3a\u6211\u662f\u4e00\u4e2a\u4eba\u5de5\u667a\u80fd\u52a9\u624b\uff0c\u5e76\u6ca1\u6709\u533b\u5b66\u80cc\u666f\u3002\u5982\u679c\u60a8\u6709\u4efb\u4f55\u75be\u75c5\u6216\u9700\u8981\u533b\u7597\u5efa\u8bae\uff0c\u8bf7\u54a8\u8be2\u60a8\u7684\u533b\u751f\u6216\u533b\u7597\u4e13\u5bb6\u3002", "answer_id": "G5XYQAC6gv9erxcV6S9srP", "gt_response": "\u836f\u7269\u6cbb\u7597,\u652f\u6301\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1862, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "CT", "answer_id": "b63sJRRkFbDtEcTdYd3cX9", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1863, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u8fd8\u662fT2\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf?\n<image>", "pred_response": "\u8fd9\u662fT1\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf\u3002", "answer_id": "hdb47DvnvWKMEPon83uayq", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1864, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "jEk4qAbqxuy2M8dQ3c4zF5", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1865, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u80f8\u8154?\n<image>", "pred_response": "\u4e0d", "answer_id": "VWXBb9sWR7EhZhTP5m7jVA", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1866, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "L42ixSkJp4BM58ep9f82uf", "gt_response": "\u53f3\u53f6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1867, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5934\u8111", "answer_id": "GxhyNZBkPZfNUKz3WKZk4d", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1868, "prompt": "\u5927\u8111\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "8avagqZoDZy7qAMdsmX8GB", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1869, "prompt": "\u56fe\u7247\u4e2d\u5927\u8111\u53f3\u4fa7\u7684\u75be\u75c5\u4f1a\u7ed9\u60a3\u8005\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u75be\u75c5", "answer_id": "BGmsEfafXwfDUtfFygHjRV", "gt_response": "\u89c6\u529b\u969c\u788d,\u5455\u5410,\u8033\u9e23,\u9885\u5185\u538b\u589e\u9ad8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1870, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u663e\u793a\u7684\u662f\u54ea\u79cd\u7c7b\u578b\u7684\u6210\u50cf,\u6838\u78c1\u5171\u632f\u3001CT\u8fd8\u662fX\u5149?\n<image>", "pred_response": "CT", "answer_id": "nZHPp539hYMtwsTMcrNnik", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1871, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528\u4ec0\u4e48\u6837\u7684\u6743\u91cd\u751f\u6210\u7684,t1\u8fd8\u662ft2?\n<image>", "pred_response": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528 t2 \u6743\u91cd\u751f\u6210\u7684\u3002", "answer_id": "BR723M7UZ8AKbtD8T3F9Wc", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1872, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "XuoJJMUKz79trWRtCSNesd", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1873, "prompt": "\u4ece\u6574\u4e2a\u8eab\u4f53\u7684\u89d2\u5ea6\u770b,\u8fd9\u5e45\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "Sw7H7GkWv2QvC3pgt3ZnvQ", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1874, "prompt": "\u56fe\u4e2d\u54ea\u4fa7\u7684\u8111\u534a\u7403\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "bXgGjxejpRbV3bp7mqpnJP", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1875, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "LArsTiYCNEpYuFSVUjci5D", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1876, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u764c\u75c7", "answer_id": "S9AESx3trkwjVtrqrwHCVm", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1877, "prompt": "\u5982\u4f55\u9884\u9632\u56fe\u7247\u4e2d\u5927\u8111\u53f3\u4e0a\u53f6\u75be\u75c5\u7684\u53d1\u751f?\n<image>", "pred_response": "\u9884\u9632\u5927\u8111\u53f3\u4e0a\u53f6\u75be\u75c5\u7684\u53d1\u751f\u5305\u62ec\u4fdd\u6301\u5065\u5eb7\u7684\u751f\u6d3b\u65b9\u5f0f\u548c\u5b9a\u671f\u505a\u5065\u5eb7\u68c0\u67e5\u3002\u4fdd\u6301\u5065\u5eb7\u7684\u751f\u6d3b\u65b9\u5f0f\u5305\u62ec\u9075\u5faa\u8425\u517b\u5747\u8861\u7684\u996e\u98df\uff0c\u907f\u514d\u8fc7\u5ea6\u5438\u70df\u548c\u996e\u9152\uff0c\u4fdd\u6301\u9002\u5f53\u7684\u4f53\u91cd\uff0c\u907f\u514d\u8fc7\u5ea6", "answer_id": "ZE4oRdXmAqKeeyQdRCMRj2", "gt_response": "\u6ce8\u610f\u996e\u98df\u536b\u751f,\u52a0\u5f3a\u8eab\u4f53\u7d20\u8d28,\u907f\u514d\u8111\u90e8\u5916\u4f24", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1878, "prompt": "\u8fd9\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u533b\u5b66\u56fe\u50cf?\n<image>", "pred_response": "\u8fd9\u662f\u4e00\u4e2a\u7167\u7247\uff0c\u663e\u793a\u4e86\u4e00\u4e2a\u4eba\u7684\u5934\u90e8\u7684\u8111\u90e8\u3002", "answer_id": "GMfqFeDGSVKm7n7EGJBz4N", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1879, "prompt": "\u8fd9\u5f20\u6838\u78c1\u5171\u632f\u56fe\u50cf\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u8fd9\u5f20\u6838\u78c1\u5171\u632f\u56fe\u50cf\u4f7f\u7528\u4e86\u5747\u503c\u6743\u91cd\u6cd5\u3002", "answer_id": "a3YvueEF2Nj4zNSTiEpTud", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1880, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "WvEGwWhKUWo6AwSuiQEiz6", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1881, "prompt": "\u4ece\u6574\u4e2a\u8eab\u4f53\u7684\u89d2\u5ea6\u770b,\u8fd9\u5e45\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "hpXzPQehS4NFm8hbwCL6fz", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1882, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4e0a\u89d2", "answer_id": "kzMyhb37pd8FiPtsdWrVYu", "gt_response": "\u53f3\u4e0a\u53f6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1883, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u76c6\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "YoJCPFXVyKTojihpNPXrED", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1884, "prompt": "\u56fe\u4e2d\u8111\u90e8\u53f3\u4e0a\u4fa7\u663e\u793a\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u75be\u75c5\u4e0d\u89c1", "answer_id": "TqoBwQCDvGdBZwtd6tG9i6", "gt_response": "\u8111\u6c34\u80bf,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1885, "prompt": "\u56fe\u4e2d\u80bf\u7624\u662f\u589e\u5f3a\u6027\u7684\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "nJ8A7H3jtzFCHH69wsowNn", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1886, "prompt": "\u4ea7\u751f\u56fe\u4e2d\u5927\u8111\u53f3\u4e0a\u4fa7\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111\u53f3\u4e0a\u4fa7\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u8111\u5352\u4e2d\u3002", "answer_id": "HF5VAA4s8rVS58UTiCKjfR", "gt_response": "\u8111\u80da\u80ce\u7ec4\u7ec7\u53d1\u80b2\u5f02\u5e38,\u9057\u4f20\u56e0\u7d20,\u5316\u5b66\u56e0\u7d20", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1887, "prompt": "\u8fd9\u5f20\u7167\u7247\u662f\u600e\u4e48\u62cd\u7684?\n<image>", "pred_response": "\u8fd9\u5f20\u7167\u7247\u662f\u7531\u4e00\u4e2a\u626b\u63cf\u4eea\u62cd\u6444\u7684\uff0c\u5b83\u662f\u4e00\u4e2a\u7167\u7247\uff0c\u5b83\u662f\u900f\u8fc7\u4e00\u4e2a\u78c1\u94c1\u6765\u626b\u63cf\u5934\u90e8\u7684\u7167\u7247\u3002", "answer_id": "KXKcxKgZYM3HtQEJZLgqzE", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1888, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528\u4ec0\u4e48\u6837\u7684\u6743\u91cd\u751f\u6210\u7684,t1\u8fd8\u662ft2?\n<image>", "pred_response": "T1", "answer_id": "eL7GRG6QwgmB5kTmzHiMUS", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1889, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u626b\u63cf", "answer_id": "kjw2iCVbjsndzYXjifQizR", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1890, "prompt": "\u4ece\u6574\u4e2a\u8eab\u4f53\u7684\u89d2\u5ea6\u770b,\u8fd9\u5e45\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u4f4d\u4e8e\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "cmx6NZGeyujMMUwuvuEHfP", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1891, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "h4YMthMrrV87DZmUkR4iAx", "gt_response": "\u53f3\u53f6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1892, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "jMcR9k7opZpUfMc5KnF5Jo", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1893, "prompt": "\u56fe\u4e2d\u8111\u90e8\u53f3\u4fa7\u663e\u793a\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u764c\u75c7", "answer_id": "iho7UwTyU4q7VfxdrZ7Y44", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1894, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "2", "answer_id": "45seNDCN4heyKuVpjVbuqt", "gt_response": "3", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1895, "prompt": "\u4ea7\u751f\u56fe\u4e2d\u5927\u8111\u53f3\u4fa7\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u764c\u75c7", "answer_id": "EEZCy9HGyyCU7jmsLgZ2AZ", "gt_response": "\u8111\u80da\u80ce\u7ec4\u7ec7\u53d1\u80b2\u5f02\u5e38,\u9057\u4f20\u56e0\u7d20,\u5316\u5b66\u56e0\u7d20", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1896, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7X\u5149\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "HvynPCH9fHqcnLR5XCELjb", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1897, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528\u4ec0\u4e48\u6837\u7684\u6743\u91cd\u751f\u6210\u7684,t1\u8fd8\u662ft2?\n<image>", "pred_response": "\u8fd9\u4e2a\u56fe\u50cf\u662f\u7528t2\u6743\u91cd\u751f\u6210\u7684\u3002", "answer_id": "C4M6fJLtXaKmotx6WuGxoc", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1898, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u626b\u63cf\u5e73\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "n4DKvoTfmWhXGpYHbvj8uc", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1899, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u662f", "answer_id": "HNWrzR8BsU5VPoHsVzSX72", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1900, "prompt": "\u56fe\u4e2d\u8111\u54ea\u4e2a\u53f6\u662f\u6b63\u5e38\u7684?\n<image>", "pred_response": "\u5de6\u534a\u5e45", "answer_id": "7uLzF8pchXFP9gFBdL9TtT", "gt_response": "\u53f3\u53f6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1901, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "UMjgo7r4MGxkcTQN7Ffxio", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1902, "prompt": "\u56fe\u4e2d\u8111\u90e8\u5de6\u4fa7\u663e\u793a\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u766b\u75eb", "answer_id": "mxPDrFBEQDhL9DNqXcArc8", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1903, "prompt": "\u56fe\u4e2d\u8111\u6c34\u80bf\u662f\u4ec0\u4e48\u5bc6\u5ea6?\n<image>", "pred_response": "\u8111\u6c34\u80bf\u7684\u5bc6\u5ea6\u662f 0.25 \u514b/\u7acb\u65b9\u5398\u7c73\u3002", "answer_id": "PhMkhqh7PxuCEEPhJ4BHB8", "gt_response": "\u4f4e\u5bc6\u5ea6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1904, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u4f1a", "answer_id": "5AR5XppxjNFUPRLtkfNJuK", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1905, "prompt": "\u8fd9\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u533b\u5b66\u56fe\u50cf?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "fpZdA2aeCkzBUwng9FWHcq", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1906, "prompt": "\u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u6838\u78c1\u5171\u632f\u7684\u54ea\u79cd\u7c7b\u578b?\n<image>", "pred_response": "\u78c1\u5171\u632f", "answer_id": "KmJtYoZjpSU9SuAkXianN5", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1907, "prompt": "\u8fd9\u662f\u77e2\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "hPGEmfwXkoy4i66LW8kgPV", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1908, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u8179\u90e8?\n<image>", "pred_response": "\u4e0d\u5b58\u5728", "answer_id": "UpULneJLHU23mzy3eTmWBS", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1909, "prompt": "\u56fe\u4e2d\u54ea\u4fa7\u7684\u8111\u53f6\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "dt9HFQDkXnXARdwcr8DnJn", "gt_response": "\u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1910, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "Mbvv6XtVMRbywkFPbuB5EU", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1911, "prompt": "\u56fe\u4e2d\u75c5\u4eba\u60a3\u6709\u8111\u7624\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "5qhBXypnCZWUywjv8zBrwS", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1912, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "1", "answer_id": "HhHaydwZMgVqk8iAzBm2wf", "gt_response": "3", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1913, "prompt": "\u5982\u4f55\u6cbb\u7597\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u5de6\u4fa7\u7684\u75be\u75c5?\n<image>", "pred_response": "\u5f88\u62b1\u6b49\uff0c\u4f5c\u4e3a\u4e00\u4e2aAI\u8bed\u8a00\u6a21\u578b\uff0c\u6211\u4e0d\u80fd\u63d0\u4f9b\u5177\u4f53\u7684\u6cbb\u7597\u5efa\u8bae\u3002\u4f46\u662f\uff0c\u5982\u679c\u60a8\u6709\u5173\u4e8e\u8fd9\u5f20\u56fe\u7247\u4e2d\u8111\u5de6\u4fa7\u75be\u75c5\u7684\u7591\u8651\uff0c\u6700\u597d\u54a8\u8be2\u533b\u751f\u6216\u4e13\u4e1a\u533b\u7597\u673a\u6784\u3002\u533b\u751f\u4f1a\u6839\u636e\u60a8\u7684\u5177\u4f53\u60c5\u51b5\u548c\u75c7", "answer_id": "GWRuhxhrMriWfV6VN9mbpg", "gt_response": "\u836f\u7269\u6cbb\u7597,\u652f\u6301\u6cbb\u7597,\u624b\u672f\u6cbb\u7597", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1914, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u663e\u793a\u7684\u662f\u54ea\u79cd\u7c7b\u578b\u7684\u6210\u50cf,\u6838\u78c1\u5171\u632f\u3001CT\u8fd8\u662fX\u5149?\n<image>", "pred_response": "\u8fd9\u5f20\u56fe\u50cf\u663e\u793a\u7684\u662f\u6838\u78c1\u5171\u632f\u6210\u50cf\u3002", "answer_id": "bqpdo7vrAsqi8LqsdtYA2A", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1915, "prompt": "\u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u6838\u78c1\u5171\u632f\u7684\u54ea\u79cd\u7c7b\u578b?\n<image>", "pred_response": "\u78c1\u5171\u632f", "answer_id": "knjXTdqKsDD4QExG5tBgan", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1916, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "iPTheiLYJoQtwFHbeJEKEk", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1917, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "B3u4mKKcs7boMMQbqFEbKx", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1918, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u53f3\u4fa7", "answer_id": "D24iS9yx49kAJcZY8buWBs", "gt_response": "\u5de6\u4e0b\u53f6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1919, "prompt": "\u56fe\u4e2d\u662f\u4ec0\u4e48\u5668\u5b98\u7cfb\u7edf?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "LNfaj4rEu2WjASCfxfLYdC", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1920, "prompt": "\u56fe\u4e2d\u8111\u90e8\u5de6\u4e0b\u4fa7\u663e\u793a\u4ec0\u4e48\u75be\u75c5?\n<image>", "pred_response": "\u764c\u75c7", "answer_id": "FMVKSp2kaCWyRSE8gifvAr", "gt_response": "\u8111\u6c34\u80bf,\u589e\u5f3a\u578b\u8111\u80bf\u7624,\u975e\u589e\u5f3a\u578b\u8111\u80bf\u7624", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1921, "prompt": "\u5982\u4f55\u9884\u9632\u56fe\u7247\u4e2d\u5927\u8111\u5de6\u4e0b\u53f6\u75be\u75c5\u7684\u53d1\u751f?\n<image>", "pred_response": "\u9884\u9632\u5927\u8111\u5de6\u4e0b\u53f6\u75be\u75c5\u7684\u53d1\u751f\u5305\u62ec\u4fdd\u6301\u5065\u5eb7\u7684\u751f\u6d3b\u65b9\u5f0f\u548c\u9075\u5faa\u533b\u751f\u7684\u5efa\u8bae\u3002\u8fd9\u4e9b\u63aa\u65bd\u5305\u62ec\uff1a\n1. \u4fdd\u6301\u5065\u5eb7\u7684\u996e\u98df\u4e60\u60ef\uff0c\u5305\u62ec\u5403\u8db3\u591f\u7684\u852c\u83dc\u3001\u6c34\u679c\u548c\u5168\u8c37\u7269\uff0c\u907f\u514d\u8fc7\u591a\u7684\u7cd6\u5206\u548c\ufffd\ufffd", "answer_id": "cJZYE5uuRWudkLerHGvGCy", "gt_response": "\u6ce8\u610f\u996e\u98df\u536b\u751f,\u52a0\u5f3a\u8eab\u4f53\u7d20\u8d28,\u907f\u514d\u8111\u90e8\u5916\u4f24", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1922, "prompt": "\u7528\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u62cd\u6444\u8fd9\u5f20\u7167\u7247?\n<image>", "pred_response": "CT", "answer_id": "KsHaEmyoBFc7wPoSySkLy3", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1923, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u8fd8\u662fT2\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf?\n<image>", "pred_response": "\u8fd9\u662fT1\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf\u3002", "answer_id": "2nazH7jXzhn8zQCqtcwTug", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1924, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "hFKwUWBKze3iNhXMTMSnXG", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1925, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "QisS7tFbSuUNfZP2fTXNWw", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1926, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u4f4d\u7f6e\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "GuNWxx4KdopL5mxYjVbVur", "gt_response": "\u53f3\u53f6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1927, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "QNSNBtKEvogQ42WfK6c3gU", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1928, "prompt": "\u60a3\u8005\u7684\u5de6\u8111\u53f6\u662f\u5426\u5f02\u5e38?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "PXcdD5dbZgdnQwMNqqU7cG", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1929, "prompt": "\u56fe\u4e2d\u5f02\u5e38\u662f\u9ad8\u5bc6\u5ea6\u8fd8\u662f\u4f4e\u5bc6\u5ea6?\n<image>", "pred_response": "\u56fe\u4e2d\u5f02\u5e38\u662f\u9ad8\u5bc6\u5ea6\u3002", "answer_id": "f6XKzJXtcFVcXH8kCvWiG5", "gt_response": "\u4f4e\u5bc6\u5ea6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1930, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "\u8be5\u56fe\u50cf\u4e2d\u6ca1\u6709\u5f02\u5e38\u3002", "answer_id": "bxb7huinDR6uyDKPyjxq4R", "gt_response": "2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1931, "prompt": "\u56fe\u7247\u4e2d\u5927\u8111\u5de6\u4fa7\u7684\u75be\u75c5\u4f1a\u7ed9\u60a3\u8005\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u6839\u636e\u56fe\u7247\u4e2d\u7684\u75be\u75c5\u63cf\u8ff0\uff0c\u5de6\u4fa7\u5927\u8111\u7684\u75be\u75c5\u53ef\u80fd\u4f1a\u5bfc\u81f4\u60a3\u8005\u51fa\u73b0\u5404\u79cd\u75c7\u72b6\uff0c\u5305\u62ec\u5934\u75db\u3001\u566a\u97f3\u611f\u3001\u542c\u529b\u51cf\u9000\u3001\u773c\u775b\u75b2\u52b3\u3001\u773c\u775b\u5e72\u71e5\u3001\u773c\u775b\u75bc\u75db\u3001\u773c\ufffd", "answer_id": "d6FhcEEUGpCsLLDCbq65jC", "gt_response": "\u89c6\u529b\u969c\u788d,\u5455\u5410,\u8033\u9e23,\u9885\u5185\u538b\u589e\u9ad8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1932, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7CT\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "5TXGSbS77NDCMaxAkNRztR", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1933, "prompt": "\u8fd9\u5f20\u6838\u78c1\u5171\u632f\u56fe\u50cf\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u8fd9\u5f20\u6838\u78c1\u5171\u632f\u56fe\u50cf\u4f7f\u7528\u4e86\u5747\u503c\u6743\u91cd\u6cd5\u3002", "answer_id": "26QxdVs9hUkGnQBcunuCGC", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1934, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "87bRTrVHSkfcTCWRwZXrAx", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1935, "prompt": "\u56fe\u50cf\u5728\u8eab\u4f53\u91cc\u4ee3\u8868\u4ec0\u4e48?\n<image>", "pred_response": "\u8fd9\u4e2a\u56fe\u50cf\u4ee3\u8868\u4e86\u4e00\u4e2a\u4eba\u7684\u5934\u90e8\uff0c\u5305\u62ec\u5927\u8111\u548c\u9885\u9aa8\u3002", "answer_id": "EFwBccRLP2Jk39manazm9x", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1936, "prompt": "\u56fe\u4e2d\u54ea\u4fa7\u7684\u8111\u53f6\u662f\u5f02\u5e38\u7684,\u5de6\u4fa7\u8fd8\u662f\u53f3\u4fa7?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "DKRufvoX3Lf5HUXcNKYSN5", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1937, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u76c6\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "ZvTuzUeBHbej4YvWwbX2Tp", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1938, "prompt": "\u56fe\u7247\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "JiBUAHDYovCmBamieVTQ3h", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1939, "prompt": "\u56fe\u4e2d\u80bf\u7624\u662f\u589e\u5f3a\u6027\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "cGqevbCaQhiD47YXUYuwLk", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1940, "prompt": "\u5982\u4f55\u9884\u9632\u56fe\u7247\u4e2d\u5927\u8111\u53f3\u53f6\u75be\u75c5\u7684\u53d1\u751f?\n<image>", "pred_response": "\u9884\u9632\u5927\u8111\u53f3\u53f6\u75be\u75c5\u7684\u53d1\u751f\u5305\u62ec\u4fdd\u6301\u5065\u5eb7\u7684\u751f\u6d3b\u65b9\u5f0f\u548c\u5b9a\u671f\u505a\u5065\u5eb7\u68c0\u67e5\u3002\u4fdd\u6301\u5065\u5eb7\u7684\u751f\u6d3b\u65b9\u5f0f\u5305\u62ec\u9075\u5faa\u8425\u517b\u5747\u8861\u7684\u996e\u98df\uff0c\u907f\u514d\u8fc7\u5ea6\u996e\u9152\u548c\u5438\u70df\uff0c\u4ee5\u53ca\u5b9a\u671f\u953b\u70bc\u8eab\u4f53\u3002\u5b9a\u671f\u505a\u5065", "answer_id": "Y68oBbdi5kKkMznzwfBMvH", "gt_response": "\u6ce8\u610f\u996e\u98df\u536b\u751f,\u52a0\u5f3a\u8eab\u4f53\u7d20\u8d28,\u907f\u514d\u8111\u90e8\u5916\u4f24", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1941, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "YBBYLXgHeVf33nvR33BU8o", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1942, "prompt": "\u8fd9\u5f20\u6838\u78c1\u5171\u632f\u56fe\u50cf\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u4e0d\u77e5\u9053", "answer_id": "CPTWL7ShYgRX25DXGYHtZM", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1943, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u7684\u626b\u63cf\u5e73\u9762\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u626b\u63cf\u5e73\u9762\u662f\u6a2a\u65ad\u9762", "answer_id": "9g9ThXBATQLJF3iKzXCHJW", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1944, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u80f8\u8154?\n<image>", "pred_response": "\u4e0d", "answer_id": "Fwn4hqZPWdn8624C997TRb", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1945, "prompt": "\u56fe\u4e2d\u8111\u6c34\u80bf\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "GCjitzUC4viU65qnWrjFuG", "gt_response": "\u53f3\u4e0b\u53f6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1946, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "GLyckVSxZQfw9jD3o7af9W", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1947, "prompt": "\u5927\u8111\u662f\u6b63\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "dAFw9WbEYwt6asmYsLCxvQ", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1948, "prompt": "\u56fe\u7247\u4e2d\u5927\u8111\u53f3\u4e0b\u4fa7\u7684\u75be\u75c5\u4f1a\u7ed9\u60a3\u8005\u5e26\u6765\u4ec0\u4e48\u6837\u7684\u75c7\u72b6?\n<image>", "pred_response": "\u6839\u636e\u56fe\u7247\u4e2d\u7684\u75be\u75c5\uff0c\u5927\u8111\u53f3\u4e0b\u4fa7\u6709\u764c\u75c7\uff0c\u8fd9\u53ef\u80fd\u4f1a\u7ed9\u60a3\u8005\u5e26\u6765\u4e00\u7cfb\u5217\u75c7\u72b6\uff0c\u5305\u62ec\u5934\u75db\u3001\u566a\u97f3\u3001\u89c6\u89c9\u5e72\u6270\u3001\u7761\u7720\u969c\u788d\u3001\u660f\u8ff7\u3001\u75bc\u75db\u548c\u80bf\u80c0\u7b49\u3002\u8fd9\u4e9b\u75c7\u72b6\u53ef\u80fd\u4f1a\u5f71\u54cd\u60a3", "answer_id": "MwvqnK24ruBbgaLidihnVd", "gt_response": "\u89c6\u529b\u969c\u788d,\u5455\u5410,\u8033\u9e23,\u9885\u5185\u538b\u589e\u9ad8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1949, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7X\u5149\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "iAS5tKGMvt6dM52FjHGXQF", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1950, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u8fd8\u662fT2\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf?\n<image>", "pred_response": "\u8fd9\u662fT1\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf\u3002", "answer_id": "4fZyKWd3ZjJdJuQgrg5eGm", "gt_response": "T1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1951, "prompt": "\u626b\u63cf\u7684\u662f\u56fe\u50cf\u7684\u54ea\u4e2a\u5e73\u9762,\u6a2a\u65ad\u9762\u8fd8\u662f\u51a0\u72b6\u9762?\n<image>", "pred_response": "\u6a2a\u65ad\u9762", "answer_id": "KJdBj8rWD2enttWwpyrzCX", "gt_response": "\u6a2a\u65ad\u9762", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1952, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "6R4H9wbpKsowby9HYuyqxp", "gt_response": "\u5934\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1953, "prompt": "\u56fe\u4e2d\u8111\u6c34\u80bf\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "eHx9Ry9RVLzfU9dSAsvPpR", "gt_response": "\u53f3\u4e0b\u53f6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1954, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "TXfiCbfnVM9wCkvXQdy2eV", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1955, "prompt": "\u5927\u8111\u770b\u8d77\u6765\u4e0d\u6b63\u5e38\u5417?\n<image>", "pred_response": "\u4e0d\u6b63\u5e38", "answer_id": "JyY5XR2R8ZybvZbXiwg3yp", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1956, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u7684\u589e\u5f3a\u578b\u8111\u80bf\u7624\u662f\u4ec0\u4e48\u989c\u8272?\n<image>", "pred_response": "\u589e\u5f3a\u578b\u8111\u80bf\u7624\u662f\u7070\u8272\u7684\u3002", "answer_id": "K2wAS5HuFFAaJZy6kM4mNH", "gt_response": "\u767d\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1957, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "1", "answer_id": "icw7TDbcU8qFyHB8pXr5Du", "gt_response": "3", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1958, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4e0d\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u4f24\u5bb3\uff0c\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u7eaf\u7cb9\u7684\u865a\u62df\u56fe\u50cf\uff0c\u6ca1\u6709\u7269\u7406\u5b58\u5728\u3002", "answer_id": "oLh358xy2gCKhCMvLotxDG", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1959, "prompt": "\u8fd9\u5f20\u7167\u7247\u662f\u600e\u4e48\u62cd\u7684?\n<image>", "pred_response": "\u8fd9\u5f20\u7167\u7247\u662f\u7531\u4e00\u4e2a\u533b\u5b66\u4eea\u5668\u62cd\u6444\u7684\uff0c\u5b83\u662f\u4e00\u4e2a\u78c1\u5171\u632f\u6210\u50cf\u4eea\uff0c\u53ef\u4ee5\u751f\u6210\u7cbe\u786e\u7684\u8111\u90e8\u56fe\u50cf\u3002", "answer_id": "6uidhsrLyk95fyNbEBWQM5", "gt_response": "\u6838\u78c1\u5171\u632f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1960, "prompt": "\u8fd9\u662fT1\u52a0\u6743\u7684\u6838\u78c1\u5171\u632f\u56fe\u50cf\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "aUcSjzgvVMhTEAPtLs44VU", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1961, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "Y55MQhcjJJCXKWEdVKashw", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1962, "prompt": "\u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u662f\u5426\u5b58\u5728\u4e8e\u5934\u90e8?\n<image>", "pred_response": "\u662f", "answer_id": "a5zCDJWavh7MNeFUe9Uyjf", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1963, "prompt": "\u56fe\u4e2d\u8111\u90e8\u589e\u5f3a\u80bf\u7624\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "KErbkSseDu8q3u425Y3HmK", "gt_response": "\u53f3\u4e0a\u53f6", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1964, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u662f\u7684", "answer_id": "Qr3FgtaCPd7Zx9nKERK6h4", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1965, "prompt": "\u60a3\u8005\u7684\u5de6\u8111\u53f6\u662f\u5426\u5f02\u5e38?\n<image>", "pred_response": "\u4e0d", "answer_id": "ZFTqVk7uVdJWdzQ2WJ37U4", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1966, "prompt": "\u8be5\u56fe\u50cf\u4e2d\u6709\u591a\u5c11\u79cd\u5f02\u5e38?\n<image>", "pred_response": "1", "answer_id": "ji5vGaYD5y8ijQBNXbvh7R", "gt_response": "3", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1967, "prompt": "\u4ea7\u751f\u56fe\u4e2d\u5927\u8111\u53f3\u4e0a\u4fa7\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u75be\u75c5\u7684\u4e3b\u8981\u75c5\u56e0\u662f\u8111\u5352\u4e2d\u3002", "answer_id": "iJw4LJ6NwmSZfoZ9du3XFv", "gt_response": "\u8111\u80da\u80ce\u7ec4\u7ec7\u53d1\u80b2\u5f02\u5e38,\u9057\u4f20\u56e0\u7d20,\u5316\u5b66\u56e0\u7d20", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1968, "prompt": "\u4ec0\u4e48\u6837\u7684\u6210\u50cf\u65b9\u5f0f\u88ab\u8fd9\u5f20\u56fe\u7247\u4f7f\u7528?\n<image>", "pred_response": "CT", "answer_id": "ZoBp8PQKg2SEiBh5GMJksk", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1969, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u8179\u90e8\u4e48?\n<image>", "pred_response": "\u662f", "answer_id": "fY3fKBcPfyiMsEVbdiA6L8", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1970, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u80ba\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "DPeUUp3HoV4tH3TJzob4A4", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1971, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u76f4\u80a0\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "ZRAvr5GKeqNp9mjTEe5usG", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1972, "prompt": "\u5728\u8fd9\u4e2a\u56fe\u50cf\u4e2d\u5de6\u80ba\u548c\u809d\u810f\u54ea\u4e2a\u4f53\u79ef\u66f4\u5927?\n<image>", "pred_response": "\u5de6\u80ba", "answer_id": "DbEaW9kHsKVFk6uLmfuj4T", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1973, "prompt": "\u56fe\u4e2d\u663e\u793a\u4e86\u591a\u5c11\u4e2a\u80be\u810f?\n<image>", "pred_response": "0", "answer_id": "bUytfvpv6rdqMvo6KBSCh2", "gt_response": "0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1974, "prompt": "\u8fd9\u5f20\u56fe\u7247\u4e2d\u810a\u9ad3\u7684\u5f62\u72b6\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u4e0d\u89c4\u5219", "answer_id": "3NJeQy8mpgtrGnAXK7nUQE", "gt_response": "\u5706\u5f62", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1975, "prompt": "\u8fd9\u5f20\u56fe\u7247\u5de6\u8fb9\u7684\u7070\u8272\u5668\u5b98\u6709\u4ec0\u4e48\u4f5c\u7528?\n<image>", "pred_response": "\u5de6\u4fa7\u7684\u7070\u8272\u5668\u5b98\u662f\u809d\u810f\uff0c\u5b83\u7684\u4f5c\u7528\u662f\u5bf9\u98df\u7269\u548c\u836f\u7269\u8fdb\u884c\u4ee3\u8c22\uff0c\u5e76\u4e14\u4ea7\u751f\u80c6\u6c41\u3002", "answer_id": "3hrde7PkgLn7PYkaJYymdB", "gt_response": "\u751f\u7269\u8f6c\u5316\u3001\u89e3\u6bd2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1976, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "5xxe3rt7QgqLNH8i3NezFm", "gt_response": "\u80c3,\u7ed3\u80a0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1977, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u5faa\u73af\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "4cZfynThUNEchkhcZGcqXj", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1978, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u53ef\u4ee5\u5206\u6ccc\u9176\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "CngFs6iRwE9b6P5XMfgHXx", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1979, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u76c6\u8154\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "hgdVnQc9TNMrY5EaHfkbeu", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1980, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u80ba\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "dhyVt7bGdA6nZEJTh8voWp", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1981, "prompt": "\u8fd9\u5e45\u56fe\u50cf\u91cc\u6709\u6c14\u7ba1\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "47JfcMXwp6rfyyhd868ZpN", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1982, "prompt": "\u56fe\u4e2d\u80be\u810f\u7684\u989c\u8272\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80be\u810f\u7684\u989c\u8272\u662f\u7070\u8272\u3002", "answer_id": "UHGsC7wk4QttJ3KoWoP8TE", "gt_response": "\u7070\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1983, "prompt": "\u5728\u8fd9\u4e2a\u56fe\u50cf\u4e2d\u5c0f\u80a0\u548c\u80be\u810f\u54ea\u4e2a\u4f53\u79ef\u66f4\u5c0f?\n<image>", "pred_response": "\u5c0f\u80a0", "answer_id": "hnicHWAng33RzEY2ZRkGiN", "gt_response": "\u80be\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1984, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u5668\u5b98?\n<image>", "pred_response": "2", "answer_id": "kb878YFK2EWEoPBsEVTcv2", "gt_response": "3", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1985, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u4e2d\u6700\u53f3\u8fb9\u7684\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8fd9\u5f20\u56fe\u50cf\u4e2d\u6700\u53f3\u8fb9\u7684\u5668\u5b98\u662f\u809d\u810f\u3002", "answer_id": "6G7NeyK8N3U5XfXXrUkb5r", "gt_response": "\u5438\u6536\u8425\u517b,\u5206\u6ccc\u9176,\u6d88\u5316\u98df\u7269", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1986, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u5206\u6ccc\u9176\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "MELDTCeLUKR3SMigFRNpEv", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1987, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u89e3\u6bd2\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "N3638CmMPpNEAbK7fDpEnQ", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1988, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u62cd\u6444\u662f\u901a\u8fc7CT\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "aXHpDHYZrSZbfeAiq2PhGj", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1989, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "KakXxWmHKaDQnb8YghYTAX", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1990, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "BFuKFgVrZN4NH4HRsczmnx", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1991, "prompt": "\u56fe\u50cf\u662f\u5426\u5305\u542b\u7ed3\u80a0?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "Pdpf5TVaDuVuAMicrNT4LJ", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1992, "prompt": "\u8fd9\u5f20\u56fe\u7247\u91cc\u6709\u80a1\u9aa8\u5934\u5417?\n<image>", "pred_response": "\u6ca1\u6709", "answer_id": "4h6KGejeVy6A9FeyBcypbz", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1993, "prompt": "\u5728\u8fd9\u4e2a\u56fe\u50cf\u4e2d\u53f3\u80ba\u548c\u5fc3\u810f\u54ea\u4e2a\u4f53\u79ef\u66f4\u5c0f?\n<image>", "pred_response": "\u53f3\u80ba", "answer_id": "7VDpbJkUZsXSaMZzjhYqW2", "gt_response": "\u5fc3\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1994, "prompt": "\u56fe\u4e2d\u6709\u591a\u5c11\u4e2a\u5668\u5b98?\n<image>", "pred_response": "2", "answer_id": "53cNZkAUevpzszSRSow8jJ", "gt_response": "4", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1995, "prompt": "\u56fe\u50cf\u53f3\u4fa7\u7684\u9ed1\u8272\u5668\u5b98\u6709\u4ec0\u4e48\u4f5c\u7528?\n<image>", "pred_response": "\u9ed1\u8272\u5668\u5b98\u662f\u80be\u810f\u7684\u7b80\u79f0\uff0c\u5b83\u7684\u4f5c\u7528\u662f\u5bf9\u8840\u6db2\u4e2d\u7684\u6c61\u57a2\u548c\u6709\u5bb3\u7269\u8d28\u8fdb\u884c\u6392\u6cc4\uff0c\u540c\u65f6\u7ef4\u6301\u8840\u6db2\u7684\u6d53\u5ea6\u548c\u6e29\u5ea6\u3002", "answer_id": "dNpkmdgvv9TBkevVtggozK", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1996, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u5fc3\u810f", "answer_id": "PkWi92mKWHeSYB8NkoRj6j", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1997, "prompt": "\u56fe\u4e2d\u6709\u6ca1\u6709\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u547c\u5438?\n<image>", "pred_response": "\u6709", "answer_id": "AVXwhyPcRL6zYySrNe3tUq", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1998, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u795e\u7ecf\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "N57EGay87mDNLLEWHhasHR", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 1999, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "6aZTFtsot7ihaY4XKMTs2i", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2000, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u80ba\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "3T6XQxMVQhHyFJeUDAECXQ", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2001, "prompt": "\u56fe\u50cf\u91cc\u6709\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "Ua34bBchHUZNw9HH4UgpvN", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2002, "prompt": "\u56fe\u50cf\u53f3\u4fa7\u7684\u9ed1\u8272\u5668\u5b98\u6709\u4ec0\u4e48\u4f5c\u7528?\n<image>", "pred_response": "\u53f3\u4fa7\u7684\u9ed1\u8272\u5668\u5b98\u5305\u62ec\u809d\u810f\u3001\u80be\u810f\u548c\u80be\u4e0a\u817a\u3002\u5b83\u4eec\u7684\u4e3b\u8981\u529f\u80fd\u662f\u5bf9\u4e8e\u8eab\u4f53\u7684\u5404\u79cd\u5316\u5b66\u7269\u8d28\u8fdb\u884c\u5206\u79bb\u3001\u6eb6\u89e3\u548c\u50a8\u5b58\uff0c\u4ee5\u53ca\u5bf9\u4e8e\u8eab\u4f53\u7684\u5e9f\u7269\u8fdb\u884c\u6392\u6cc4\u3002\u809d\u810f\u8d1f\u8d23\u5bf9\u4e8e\u8840\u6db2\u4e2d\u7684\u5316\u5b66\u7269\u8d28", "answer_id": "WUtvUJLFD2KcBCHtrCAEwZ", "gt_response": "\u547c\u5438", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2003, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf?\n<image>", "pred_response": "\u80ba", "answer_id": "HENEowjuWcnqoayTe8Nu6a", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2004, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "JMDtvEi3fFABXnyMvqPusf", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2005, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u547c\u5438\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u6709", "answer_id": "hoqdjESGkXCBBiHZeum5eZ", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2006, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "Tqx8HqeBvNVoSNFqw8nEjv", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2007, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "CeRasLAjkzaHv4eWqekZ9X", "gt_response": "\u8179\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2008, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u4e0d\u77e5\u9053", "answer_id": "LeB8sdQuH8owUPafGWosfu", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2009, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "4n5TaQGmXft6mvfcJKCoyv", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2010, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80be\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "QRsx2MjebF9zziCxfEbPMX", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2011, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "bKnYzbHGWXYiQX6RAKE9NF", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2012, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "NgYNuqM54kWjR32Xout5tz", "gt_response": "\u76c6\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2013, "prompt": "\u8fd9\u5f20\u56fe\u7247\u91cc\u6709\u76f4\u80a0\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "hqrjmgXwr6DLZySRuejgzA", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2014, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u98df\u9053\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "WQdtM7yLCJJAoJGeCa84ew", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2015, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf?\n<image>", "pred_response": "\u80a0", "answer_id": "QueyTVB7cjGs3jkeRgPfiY", "gt_response": "\u76f4\u80a0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2016, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "fBcpc6WWFcrY3XZCrLuoyV", "gt_response": "\u76c6\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2017, "prompt": "\u56fe\u4e2d\u76f4\u80a0\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "C2rsqfVBCUFKJwGwsUiqNM", "gt_response": "\u5e95\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2018, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u5934\u90e8\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "FcF9ArG38qn5YpjjKxfS6U", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2019, "prompt": "\u8fd9\u5e45\u56fe\u50cf\u91cc\u6709\u773c\u775b\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "entatUwab2eMJTkmhy7UM4", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2020, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u5c0f\u80a0\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "7czt5sPGfgEobk7DsXn7wB", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2021, "prompt": "\u5728\u8fd9\u4e2a\u56fe\u50cf\u4e2d\u8180\u80f1\u548c\u76f4\u80a0\u54ea\u4e2a\u4f53\u79ef\u66f4\u5c0f?\n<image>", "pred_response": "\u76f4\u80a0", "answer_id": "FuVHBPyeXdfYcBEvFybmSH", "gt_response": "\u76f4\u80a0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2022, "prompt": "\u4f4d\u4e8e\u56fe\u50cf\u5e95\u90e8\u7684\u5668\u5b98\u7684\u529f\u80fd\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u76c6\u8154", "answer_id": "Bp53Cy2r5m7jGrJZdS84AZ", "gt_response": "\u50a8\u5b58\u7caa\u4fbf,\u6392\u6cc4\u7caa\u4fbf", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2023, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf?\n<image>", "pred_response": "\u80be\u810f", "answer_id": "PR5dbT6usxbN4L6ecxDVED", "gt_response": "\u5c0f\u80a0,\u76f4\u80a0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2024, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u547c\u5438\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "fgpCSw9bpFUb2qHpPvdmHN", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2025, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u53ef\u4ee5\u63a5\u6536\u58f0\u97f3\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "f4KV7paiSEPtAsoT6aTuoE", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2026, "prompt": "\u56fe\u50cf\u4e2d\u7684\u4e3b\u8981\u5668\u5b98\u7cfb\u7edf\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "kEGp8R9AxLaThtTqvSd5Q9", "gt_response": "\u76c6\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2027, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u5c0f\u80a0\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "PL7gjbc3NNYbkJ7GnqDSKc", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2028, "prompt": "\u56fe\u7247\u4e2d\u7684\u8180\u80f1\u662f\u4ec0\u4e48\u989c\u8272\u7684?\n<image>", "pred_response": "\u7070\u8272", "answer_id": "nrTrRyS8UuQkiGusBrpXmk", "gt_response": "\u6df1\u7070\u8272", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2029, "prompt": "\u56fe\u4e2d\u4f53\u79ef\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80be\u810f", "answer_id": "DqXLCcKyeZNo2DiX6MQLrX", "gt_response": "\u8180\u80f1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2030, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u6ccc\u5c3f\u7cfb\u7edf?\n<image>", "pred_response": "\u8180\u80f1", "answer_id": "4MiZsBaDDkHMRHCLsfbySC", "gt_response": "\u8180\u80f1", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2031, "prompt": "\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8eab\u4f53\u7684\u54ea\u4e2a\u90e8\u4f4d?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "nZQeG2q8fqtUDCa72VvmzZ", "gt_response": "\u76c6\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2032, "prompt": "\u56fe\u4e2d\u76f4\u80a0\u5728\u54ea\u91cc?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "3EbJm238HFATjUY5fLcKER", "gt_response": "\u5e95\u90e8", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2033, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u76c6\u8154\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "mbXTWrXVuaaUgJseztyP78", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2034, "prompt": "\u56fe\u50cf\u662f\u5426\u5305\u542b\u7ed3\u80a0?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "ZYVpYUwniQm8cn6vBVQJLa", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2035, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u91cc\u6709\u76f4\u80a0\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "9Z3tPXyWF9UFPyVhjscRX4", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2036, "prompt": "\u5728\u8fd9\u4e2a\u56fe\u50cf\u4e2d\u76f4\u80a0\u548c\u7ed3\u80a0\u54ea\u4e2a\u4f53\u79ef\u66f4\u5927?\n<image>", "pred_response": "\u7ed3\u80a0", "answer_id": "Y4iiUwonW3P8sHi3CHZ4P7", "gt_response": "\u7ed3\u80a0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2037, "prompt": "\u56fe\u7247\u4e2d\u54ea\u4e2a\u5668\u5b98\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf?\n<image>", "pred_response": "\u80a0", "answer_id": "GCvFZLgCNLJwWmXcTMLKUL", "gt_response": "\u5c0f\u80a0, \u76f4\u80a0, \u7ed3\u80a0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2038, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6d88\u5316\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "5dxtUayvEVWVyRemuwJfFu", "gt_response": "\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2039, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u80f8\u8154\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "SxQdecvZrsXZLPYX8GsWwy", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2040, "prompt": "\u56fe\u50cf\u662f\u5426\u5305\u542b\u76f4\u80a0?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "8kK2QrFVce572morUQUwVb", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2041, "prompt": "\u62cd\u8fd9\u6837\u7684\u56fe\u50cf\u4f1a\u5bf9\u8eab\u4f53\u9020\u6210\u591a\u5927\u7684\u4f24\u5bb3?\n<image>", "pred_response": "\u4e0d\u4f1a", "answer_id": "oFP29mjdhS4d4pAkxL7VC7", "gt_response": "\u8f83\u591a", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2042, "prompt": "\u56fe\u7247\u4e2d\u6709\u5c5e\u4e8e\u6ccc\u5c3f\u7cfb\u7edf\u7684\u5668\u5b98\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "LqcZzvjJ9eShk6V6gTWwZm", "gt_response": "\u6ca1\u6709", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2043, "prompt": "\u8fd9\u662f\u51a0\u72b6\u9762\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "8sei7tFwUVeV68gcswEoZs", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2044, "prompt": "\u56fe\u50cf\u5c55\u793a\u7684\u4f4d\u7f6e\u662f?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "JVmx4sn9AAreSXs5jAR2jC", "gt_response": "\u76c6\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2045, "prompt": "\u8fd9\u5f20\u56fe\u50cf\u662f\u5173\u4e8e\u76c6\u8154\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "HFpCD4v8hAwSWBasQ3vrt4", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2046, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u813e\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "iZDQScopFntZPvd9pH43nC", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2047, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5668\u5b98\u53ef\u4ee5\u88ab\u7528\u4e8e\u6d88\u5316\u98df\u7269\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "NpmbDRW2XNfXi9cj4NUGho", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2048, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "78K6C3vCDbbBtvV3nqLsFM", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2049, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "M7wXyJs4f5A5Tg4y97okrN", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2050, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u8fd9\u5f20MRI\u56fe\u7247\u4f7f\u7528\u4e86\u5747\u503c\u6743\u91cd\u6cd5\u3002", "answer_id": "ZFJjau5dAAEeAtrF28qvwe", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2051, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "4AvxWwoeHHw6hXrQAeQ5PW", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2052, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "NQr6aYgwJPerEyLm4CJBYY", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2053, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u8111\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "FzdWz4zccraBzVoQhRYtTJ", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2054, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u766b\u75eb", "answer_id": "cGak86j5GQjqCDCdD8emZm", "gt_response": "\u8111\u80bf\u7624,\u8111\u6c34\u80bf", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2055, "prompt": "\u5927\u8111\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f\u3002", "answer_id": "e5kmoydybUh26MPt5n6chK", "gt_response": "\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2056, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u5de6\u4fa7", "answer_id": "C7WBQ8aA7tmr9t2rASeYSw", "gt_response": "\u53f3\u4e0b\u89d2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2057, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8fd9\u5f20\u56fe\u7247\u662f\u4e00\u4e2a\u7167\u7247\u7684\u65ad\u5c42\u56fe\uff0c\u5b83\u5c55\u793a\u4e86\u4e00\u4e2a\u4eba\u7684\u5934\u90e8\u7684\u8111\u90e8\u3002", "answer_id": "AGrtjvg6HXAfsowreuAtaL", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2058, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "Lzu8afHhg9UcRoiWmr4jqN", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2059, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u8fd9\u5f20MRI\u56fe\u7247\u4f7f\u7528\u4e86\u5747\u503c\u6743\u91cd\u6cd5\u3002", "answer_id": "4EVg7ensNgkicgFuy4P5Ee", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2060, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "8je3sQxGcPjTh5JUF9LQjc", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2061, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "bWeSrxSYej3JWe3aQXatw8", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2062, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u8111\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "YJL9bwJcjBiGsyPVSUTUAA", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2063, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u764c\u75c7", "answer_id": "kxbh7S6jDLcG76pwVmWijv", "gt_response": "\u8111\u80bf\u7624,\u8111\u6c34\u80bf", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2064, "prompt": "\u5927\u8111\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "ZAtoVskBttiLWnEUMW7hYF", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2065, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u4e2d\u592e", "answer_id": "QaVpF5yA9usfmfwvJfGKBa", "gt_response": "\u53f3\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2066, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u6838\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u3002", "answer_id": "aRrjaG9nyFa5Y6N6UfoUCq", "gt_response": "MRI", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2067, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u5934\u90e8", "answer_id": "Zs3fXnoJSNgy9B3Hy4edWc", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2068, "prompt": "\u8fd9\u5f20MRI\u56fe\u7247\u7528\u7684\u4ec0\u4e48\u52a0\u6743\u65b9\u5f0f?\n<image>", "pred_response": "\u4e0d\u77e5\u9053", "answer_id": "5hnmnFheJ7KWbnfumG6sZH", "gt_response": "T2", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2069, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "6k6hZZqUQNotAEReJhvi3z", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2070, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u5927\u8111", "answer_id": "TKkhuvm5Li8w9dmAoxowL7", "gt_response": "\u5927\u8111", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2071, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u8111\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "Sq6LbbgXZsMHFpDNQFHWQZ", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2072, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "9QDaPKhZi2BpDo6qdSomT8", "gt_response": "\u65e0", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2073, "prompt": "\u5927\u8111\u662f\u5065\u5eb7\u7684\u5417?\n<image>", "pred_response": "\u4e0d", "answer_id": "BZSnscfcmLH4v74cUdcfiR", "gt_response": "\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2074, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "FqZUc7JwemsBwdyYUMMsdJ", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2075, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u80f8\u8154", "answer_id": "MBySpyRCNMDrwM64NeeP6H", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2076, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "Tz4pVnPEyD9T5De7yJah2W", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2077, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u80ba", "answer_id": "c2h42gf6jmp6vrkwTQV6fo", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2078, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "25oqsvE2HAkcuKFWEXmDKt", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2079, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u662f", "answer_id": "dTcn4DzKAHRgr7scmK2G97", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2080, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "EAkxxemoxUgHmmzEucWCrF", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2081, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u54ea\u4e9b\u75be\u75c5?\n<image>", "pred_response": "\u80ba\u764c", "answer_id": "QEJYMRqoDtio5HM5N8ZtKc", "gt_response": "\u80ba\u764c", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2082, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "LcF6CB5cYbPd7GLqJw9www", "gt_response": "\u662f\u7684", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2083, "prompt": "\u5f02\u5e38\u75c5\u53d8\u5728\u54ea\u4e2a\u4f4d\u7f6e?\n<image>", "pred_response": "\u80ba", "answer_id": "TuGMntx8BXgYTq9nfqMmAQ", "gt_response": "\u53f3\u80ba, \u5de6\u4fa7", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2084, "prompt": "\u56fe\u4e2d\u80ba\u3001\u5fc3\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u80ba", "answer_id": "SqsQdZ5jvUPuVdMEB4UAmW", "gt_response": "\u80ba", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2085, "prompt": "\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\n<image>", "pred_response": "CT", "answer_id": "gjZVUgnRn7qhLsjo9mb8Rv", "gt_response": "CT", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2086, "prompt": "\u56fe\u50cf\u91cc\u5305\u542b\u7684\u533a\u57df\u5c5e\u4e8e\u8eab\u4f53\u54ea\u4e2a\u90e8\u5206?\n<image>", "pred_response": "\u8179\u90e8", "answer_id": "DRywfqPLUZxBQwojagDw5S", "gt_response": "\u80f8\u8154", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2087, "prompt": "\u56fe\u7247\u4e3b\u8981\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "4GLnz3GvyuA7oPExQxPtka", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2088, "prompt": "\u56fe\u4e2d\u6700\u5927\u7684\u5668\u5b98\u662f\u4ec0\u4e48?\n<image>", "pred_response": "\u809d\u810f", "answer_id": "c2dt64XnrEsH35AJNePLzr", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2089, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u809d\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "PhcN9n6kp2kLpMji7gvR43", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2090, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u80ba\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "hatAt2oPb5x332GgBTgWpf", "gt_response": "\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2091, "prompt": "\u56fe\u7247\u4e2d\u5305\u542b\u5fc3\u810f\u5417?\n<image>", "pred_response": "\u4e0d\u5305\u542b", "answer_id": "MqXQ9vaefjsdBuYKRoTULU", "gt_response": "\u4e0d\u5305\u542b", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2092, "prompt": "\u80ba\u662f\u5f02\u5e38\u7684\u5417?\n<image>", "pred_response": "\u4e0d\u662f", "answer_id": "kCuPD2YPSt3YoUnrZJEHhJ", "gt_response": "\u4e0d\u662f", "model_id": "llava-slake1_sct", "metadata": {}}
{"question_id": 2093, "prompt": "\u56fe\u4e2d\u80ba\u3001\u5fc3\u810f\u548c\u809d\u810f\u54ea\u4e2a\u66f4\u5927?\n<image>", "pred_response": "\u80ba", "answer_id": "jWxKPLxEa3qsH9XPJPNpgx", "gt_response": "\u809d\u810f", "model_id": "llava-slake1_sct", "metadata": {}}
