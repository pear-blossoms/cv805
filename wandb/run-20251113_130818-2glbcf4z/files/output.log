[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|                                                                                                                                                 | 0/125 [00:00<?, ?it/s]/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
                                                                                                                                                                              

{'loss': 1.0846, 'learning_rate': 5e-06, 'epoch': 0.01}

{'loss': 0.9537, 'learning_rate': 1e-05, 'epoch': 0.02}

{'loss': 0.9504, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.02}

{'loss': 0.8744, 'learning_rate': 2e-05, 'epoch': 0.03}

{'loss': 1.0012, 'learning_rate': 1.9996629653035128e-05, 'epoch': 0.04}

{'loss': 0.9459, 'learning_rate': 1.9986520883988233e-05, 'epoch': 0.05}

{'loss': 0.9443, 'learning_rate': 1.9969680506871138e-05, 'epoch': 0.06}

{'loss': 0.9225, 'learning_rate': 1.9946119873266615e-05, 'epoch': 0.06}

{'loss': 0.951, 'learning_rate': 1.9915854864676665e-05, 'epoch': 0.07}

{'loss': 0.9381, 'learning_rate': 1.9878905881817254e-05, 'epoch': 0.08}

{'loss': 0.8761, 'learning_rate': 1.9835297830866827e-05, 'epoch': 0.09}

{'loss': 0.9125, 'learning_rate': 1.9785060106677818e-05, 'epoch': 0.1}

{'loss': 0.9692, 'learning_rate': 1.9728226572962474e-05, 'epoch': 0.1}

{'loss': 0.9055, 'learning_rate': 1.966483553946637e-05, 'epoch': 0.11}

{'loss': 0.8487, 'learning_rate': 1.9594929736144978e-05, 'epoch': 0.12}

{'loss': 0.8502, 'learning_rate': 1.9518556284360696e-05, 'epoch': 0.13}

{'loss': 0.9217, 'learning_rate': 1.9435766665119823e-05, 'epoch': 0.14}

{'loss': 0.9004, 'learning_rate': 1.934661668437073e-05, 'epoch': 0.14}

{'loss': 0.959, 'learning_rate': 1.9251166435386837e-05, 'epoch': 0.15}

{'loss': 0.8891, 'learning_rate': 1.9149480258259535e-05, 'epoch': 0.16}

{'loss': 0.9975, 'learning_rate': 1.9041626696528503e-05, 'epoch': 0.17}

{'loss': 0.942, 'learning_rate': 1.892767845097864e-05, 'epoch': 0.18}

{'loss': 0.8718, 'learning_rate': 1.8807712330634645e-05, 'epoch': 0.18}

{'loss': 0.9304, 'learning_rate': 1.868180920098644e-05, 'epoch': 0.19}

{'loss': 0.9159, 'learning_rate': 1.8550053929480202e-05, 'epoch': 0.2}

{'loss': 0.9031, 'learning_rate': 1.8412535328311813e-05, 'epoch': 0.21}

{'loss': 0.9246, 'learning_rate': 1.826934609456129e-05, 'epoch': 0.22}

{'loss': 0.861, 'learning_rate': 1.8120582747708503e-05, 'epoch': 0.22}

{'loss': 0.8971, 'learning_rate': 1.796634556457236e-05, 'epoch': 0.23}

{'loss': 0.8922, 'learning_rate': 1.780673851171728e-05, 'epoch': 0.24}

{'loss': 0.8848, 'learning_rate': 1.7641869175372493e-05, 'epoch': 0.25}

{'loss': 0.8857, 'learning_rate': 1.7471848688911465e-05, 'epoch': 0.26}

{'loss': 0.9088, 'learning_rate': 1.72967916579403e-05, 'epoch': 0.26}

{'loss': 0.8647, 'learning_rate': 1.7116816083045603e-05, 'epoch': 0.27}

{'loss': 0.9564, 'learning_rate': 1.6932043280253892e-05, 'epoch': 0.28}

{'loss': 0.9785, 'learning_rate': 1.6742597799256182e-05, 'epoch': 0.29}

{'loss': 0.8529, 'learning_rate': 1.6548607339452853e-05, 'epoch': 0.3}

{'loss': 0.8889, 'learning_rate': 1.6350202663875385e-05, 'epoch': 0.3}

{'loss': 0.8824, 'learning_rate': 1.614751751104301e-05, 'epoch': 0.31}

{'loss': 0.878, 'learning_rate': 1.5940688504813664e-05, 'epoch': 0.32}

{'loss': 0.885, 'learning_rate': 1.5729855062290024e-05, 'epoch': 0.33}

{'loss': 0.8162, 'learning_rate': 1.551515929984271e-05, 'epoch': 0.34}

{'loss': 0.9472, 'learning_rate': 1.529674593731399e-05, 'epoch': 0.34}

{'loss': 0.9082, 'learning_rate': 1.5074762200466557e-05, 'epoch': 0.35}

{'loss': 0.8357, 'learning_rate': 1.4849357721743169e-05, 'epoch': 0.36}

{'loss': 0.9502, 'learning_rate': 1.4620684439403962e-05, 'epoch': 0.37}

{'loss': 0.9042, 'learning_rate': 1.438889649510956e-05, 'epoch': 0.38}

{'loss': 0.8312, 'learning_rate': 1.4154150130018867e-05, 'epoch': 0.38}

{'loss': 0.8944, 'learning_rate': 1.3916603579471705e-05, 'epoch': 0.39}

{'loss': 0.9355, 'learning_rate': 1.3676416966327201e-05, 'epoch': 0.4}

{'loss': 0.9346, 'learning_rate': 1.3433752193029888e-05, 'epoch': 0.41}

{'loss': 0.9118, 'learning_rate': 1.318877283247619e-05, 'epoch': 0.42}

{'loss': 0.8971, 'learning_rate': 1.2941644017754964e-05, 'epoch': 0.42}

{'loss': 0.903, 'learning_rate': 1.2692532330836346e-05, 'epoch': 0.43}

{'loss': 0.8992, 'learning_rate': 1.2441605690283915e-05, 'epoch': 0.44}

{'loss': 0.8818, 'learning_rate': 1.218903323806595e-05, 'epoch': 0.45}

{'loss': 0.9887, 'learning_rate': 1.1934985225541998e-05, 'epoch': 0.46}

{'loss': 0.8152, 'learning_rate': 1.1679632898701649e-05, 'epoch': 0.46}

{'loss': 0.9871, 'learning_rate': 1.1423148382732854e-05, 'epoch': 0.47}

{'loss': 0.9363, 'learning_rate': 1.1165704565997593e-05, 'epoch': 0.48}

{'loss': 0.8658, 'learning_rate': 1.0907474983493144e-05, 'epoch': 0.49}

{'loss': 0.9498, 'learning_rate': 1.064863369987743e-05, 'epoch': 0.5}

{'loss': 0.8337, 'learning_rate': 1.0389355192137379e-05, 'epoch': 0.5}

{'loss': 0.8719, 'learning_rate': 1.012981423197931e-05, 'epoch': 0.51}

{'loss': 0.9685, 'learning_rate': 9.870185768020694e-06, 'epoch': 0.52}

{'loss': 0.9252, 'learning_rate': 9.610644807862625e-06, 'epoch': 0.53}

{'loss': 0.9402, 'learning_rate': 9.351366300122569e-06, 'epoch': 0.54}

{'loss': 0.8848, 'learning_rate': 9.092525016506858e-06, 'epoch': 0.54}

{'loss': 0.9604, 'learning_rate': 8.83429543400241e-06, 'epoch': 0.55}

{'loss': 0.9537, 'learning_rate': 8.576851617267151e-06, 'epoch': 0.56}

{'loss': 0.941, 'learning_rate': 8.320367101298351e-06, 'epoch': 0.57}

{'loss': 0.8646, 'learning_rate': 8.065014774458004e-06, 'epoch': 0.58}

{'loss': 0.8058, 'learning_rate': 7.810966761934053e-06, 'epoch': 0.58}

{'loss': 0.7831, 'learning_rate': 7.558394309716088e-06, 'epoch': 0.59}

{'loss': 0.8748, 'learning_rate': 7.307467669163655e-06, 'epoch': 0.6}

{'loss': 0.8919, 'learning_rate': 7.058355982245038e-06, 'epoch': 0.61}

{'loss': 0.8885, 'learning_rate': 6.8112271675238154e-06, 'epoch': 0.62}

{'loss': 0.8759, 'learning_rate': 6.566247806970119e-06, 'epoch': 0.62}

{'loss': 0.8907, 'learning_rate': 6.323583033672799e-06, 'epoch': 0.63}

{'loss': 0.8693, 'learning_rate': 6.083396420528298e-06, 'epoch': 0.64}

{'loss': 0.9578, 'learning_rate': 5.845849869981137e-06, 'epoch': 0.65}

{'loss': 0.8748, 'learning_rate': 5.611103504890444e-06, 'epoch': 0.66}

{'loss': 0.8522, 'learning_rate': 5.379315560596038e-06, 'epoch': 0.66}

{'loss': 0.9064, 'learning_rate': 5.1506422782568345e-06, 'epoch': 0.67}

{'loss': 0.8862, 'learning_rate': 4.925237799533445e-06, 'epoch': 0.68}

{'loss': 0.931, 'learning_rate': 4.703254062686017e-06, 'epoch': 0.69}

{'loss': 0.8746, 'learning_rate': 4.4848407001572945e-06, 'epoch': 0.7}

{'loss': 0.9301, 'learning_rate': 4.270144937709981e-06, 'epoch': 0.7}

{'loss': 0.8918, 'learning_rate': 4.059311495186338e-06, 'epoch': 0.71}

{'loss': 0.9092, 'learning_rate': 3.852482488956992e-06, 'epoch': 0.72}

{'loss': 0.8492, 'learning_rate': 3.6497973361246153e-06, 'epoch': 0.73}

{'loss': 1.027, 'learning_rate': 3.4513926605471504e-06, 'epoch': 0.74}

{'loss': 0.839, 'learning_rate': 3.257402200743821e-06, 'epoch': 0.74}

{'loss': 0.9167, 'learning_rate': 3.0679567197461135e-06, 'epoch': 0.75}

{'loss': 0.9029, 'learning_rate': 2.8831839169543998e-06, 'epoch': 0.76}

{'loss': 0.8638, 'learning_rate': 2.7032083420597e-06, 'epoch': 0.77}

{'loss': 0.8811, 'learning_rate': 2.528151311088537e-06, 'epoch': 0.78}

{'loss': 0.8709, 'learning_rate': 2.3581308246275103e-06, 'epoch': 0.78}

{'loss': 0.9061, 'learning_rate': 2.1932614882827196e-06, 'epoch': 0.79}

{'loss': 0.9027, 'learning_rate': 2.03365443542764e-06, 'epoch': 0.8}

{'loss': 0.8721, 'learning_rate': 1.8794172522915022e-06, 'epoch': 0.81}

{'loss': 0.8557, 'learning_rate': 1.730653905438714e-06, 'epoch': 0.82}

{'loss': 0.9703, 'learning_rate': 1.587464671688187e-06, 'epoch': 0.82}

{'loss': 0.9325, 'learning_rate': 1.4499460705198e-06, 'epoch': 0.83}

{'loss': 0.8723, 'learning_rate': 1.3181907990135624e-06, 'epoch': 0.84}

{'loss': 0.8532, 'learning_rate': 1.1922876693653584e-06, 'epoch': 0.85}

{'loss': 0.9411, 'learning_rate': 1.0723215490213635e-06, 'epoch': 0.86}

{'loss': 0.9148, 'learning_rate': 9.583733034714982e-07, 'epoch': 0.86}

{'loss': 0.8744, 'learning_rate': 8.505197417404687e-07, 'epoch': 0.87}

{'loss': 0.8507, 'learning_rate': 7.488335646131628e-07, 'epoch': 0.88}

{'loss': 0.8349, 'learning_rate': 6.53383315629268e-07, 'epoch': 0.89}

{'loss': 0.7837, 'learning_rate': 5.64233334880181e-07, 'epoch': 0.9}

{'loss': 1.0089, 'learning_rate': 4.814437156393048e-07, 'epoch': 0.9}

{'loss': 1.325, 'learning_rate': 4.0507026385502747e-07, 'epoch': 0.91}

{'loss': 0.9054, 'learning_rate': 3.3516446053363015e-07, 'epoch': 0.92}

{'loss': 0.9379, 'learning_rate': 2.717734270375272e-07, 'epoch': 0.93}

{'loss': 0.8668, 'learning_rate': 2.1493989332218468e-07, 'epoch': 0.94}

{'loss': 0.8187, 'learning_rate': 1.6470216913317628e-07, 'epoch': 0.94}

{'loss': 0.8779, 'learning_rate': 1.2109411818274851e-07, 'epoch': 0.95}

{'loss': 0.8694, 'learning_rate': 8.41451353233369e-08, 'epoch': 0.96}

{'loss': 0.8711, 'learning_rate': 5.388012673338661e-08, 'epoch': 0.97}

{'loss': 0.9376, 'learning_rate': 3.03194931288664e-08, 'epoch': 0.98}

{'loss': 0.8231, 'learning_rate': 1.3479116011769766e-08, 'epoch': 0.98}

{'loss': 0.8703, 'learning_rate': 3.3703469648760367e-09, 'epoch': 0.99}

{'loss': 0.9083, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 331.3595, 'train_samples_per_second': 12.056, 'train_steps_per_second': 0.377, 'train_loss': 0.9053321862220765, 'epoch': 1.0}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
