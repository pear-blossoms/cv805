[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|                                                                                      | 0/19 [00:00<?, ?it/s]/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
                                                                                                                  

{'loss': 1.8608, 'learning_rate': 2e-05, 'epoch': 0.05}

{'loss': 1.528, 'learning_rate': 1.9848077530122083e-05, 'epoch': 0.11}

{'loss': 1.6758, 'learning_rate': 1.9396926207859085e-05, 'epoch': 0.16}

{'loss': 1.4712, 'learning_rate': 1.866025403784439e-05, 'epoch': 0.21}

{'loss': 1.1659, 'learning_rate': 1.766044443118978e-05, 'epoch': 0.26}

{'loss': 1.0759, 'learning_rate': 1.6427876096865394e-05, 'epoch': 0.32}

{'loss': 1.0844, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.37}

{'loss': 1.0783, 'learning_rate': 1.342020143325669e-05, 'epoch': 0.42}

{'loss': 1.312, 'learning_rate': 1.1736481776669307e-05, 'epoch': 0.47}

{'loss': 1.4499, 'learning_rate': 1e-05, 'epoch': 0.53}

{'loss': 1.1692, 'learning_rate': 8.263518223330698e-06, 'epoch': 0.58}

{'loss': 1.536, 'learning_rate': 6.579798566743314e-06, 'epoch': 0.63}

{'loss': 1.2603, 'learning_rate': 5.000000000000003e-06, 'epoch': 0.68}

{'loss': 1.029, 'learning_rate': 3.5721239031346067e-06, 'epoch': 0.74}

{'loss': 1.4854, 'learning_rate': 2.339555568810221e-06, 'epoch': 0.79}

{'loss': 1.1315, 'learning_rate': 1.339745962155613e-06, 'epoch': 0.84}

{'loss': 1.1755, 'learning_rate': 6.030737921409169e-07, 'epoch': 0.89}

{'loss': 1.431, 'learning_rate': 1.519224698779198e-07, 'epoch': 0.95}

{'loss': 1.3679, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 43.3003, 'train_samples_per_second': 13.533, 'train_steps_per_second': 0.439, 'train_loss': 1.3309428880089207, 'epoch': 1.0}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
