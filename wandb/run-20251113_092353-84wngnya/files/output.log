[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|                                                                                                                                                   | 0/138 [00:00<?, ?it/s]/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
                                                                                                                                                                                

{'loss': 0.9376, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}

{'loss': 0.9353, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}

{'loss': 0.8792, 'learning_rate': 1.2e-05, 'epoch': 0.02}

{'loss': 0.9303, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.03}

{'loss': 0.8809, 'learning_rate': 2e-05, 'epoch': 0.04}

{'loss': 0.9069, 'learning_rate': 1.9997210372120276e-05, 'epoch': 0.04}

{'loss': 0.9834, 'learning_rate': 1.998884304488584e-05, 'epoch': 0.05}

{'loss': 0.8866, 'learning_rate': 1.997490268664256e-05, 'epoch': 0.06}

{'loss': 0.8578, 'learning_rate': 1.995539707507284e-05, 'epoch': 0.07}

{'loss': 0.9291, 'learning_rate': 1.9930337092856243e-05, 'epoch': 0.07}

{'loss': 0.8881, 'learning_rate': 1.9899736721597787e-05, 'epoch': 0.08}

{'loss': 0.8807, 'learning_rate': 1.9863613034027224e-05, 'epoch': 0.09}

{'loss': 0.9674, 'learning_rate': 1.9821986184473757e-05, 'epoch': 0.09}

{'loss': 1.0307, 'learning_rate': 1.9774879397621387e-05, 'epoch': 0.1}

{'loss': 0.8916, 'learning_rate': 1.9722318955551307e-05, 'epoch': 0.11}

{'loss': 0.8887, 'learning_rate': 1.966433418307843e-05, 'epoch': 0.12}

{'loss': 0.9235, 'learning_rate': 1.960095743139033e-05, 'epoch': 0.12}

{'loss': 0.9011, 'learning_rate': 1.9532224059997693e-05, 'epoch': 0.13}

{'loss': 0.9242, 'learning_rate': 1.9458172417006347e-05, 'epoch': 0.14}

{'loss': 0.8541, 'learning_rate': 1.9378843817721856e-05, 'epoch': 0.14}

{'loss': 0.9297, 'learning_rate': 1.929428252159866e-05, 'epoch': 0.15}

{'loss': 0.951, 'learning_rate': 1.9204535707546602e-05, 'epoch': 0.16}

{'loss': 0.9012, 'learning_rate': 1.9109653447608607e-05, 'epoch': 0.17}

{'loss': 0.895, 'learning_rate': 1.900968867902419e-05, 'epoch': 0.17}

{'loss': 0.955, 'learning_rate': 1.8904697174694447e-05, 'epoch': 0.18}

{'loss': 0.8876, 'learning_rate': 1.879473751206489e-05, 'epoch': 0.19}

{'loss': 0.8812, 'learning_rate': 1.8679871040443632e-05, 'epoch': 0.2}

{'loss': 0.9262, 'learning_rate': 1.8560161846773002e-05, 'epoch': 0.2}

{'loss': 0.9923, 'learning_rate': 1.8435676719873828e-05, 'epoch': 0.21}

{'loss': 0.914, 'learning_rate': 1.830648511318223e-05, 'epoch': 0.22}

{'loss': 0.876, 'learning_rate': 1.817265910599978e-05, 'epoch': 0.22}

{'loss': 0.942, 'learning_rate': 1.8034273363278615e-05, 'epoch': 0.23}

{'loss': 0.7769, 'learning_rate': 1.789140509396394e-05, 'epoch': 0.24}

{'loss': 0.8541, 'learning_rate': 1.7744134007917195e-05, 'epoch': 0.25}

{'loss': 0.9695, 'learning_rate': 1.7592542271443888e-05, 'epoch': 0.25}

{'loss': 0.9745, 'learning_rate': 1.74367144614509e-05, 'epoch': 0.26}

{'loss': 0.9129, 'learning_rate': 1.7276737518258865e-05, 'epoch': 0.27}

{'loss': 0.8472, 'learning_rate': 1.7112700697095955e-05, 'epoch': 0.28}

{'loss': 0.9127, 'learning_rate': 1.6944695518300087e-05, 'epoch': 0.28}

{'loss': 0.8954, 'learning_rate': 1.6772815716257414e-05, 'epoch': 0.29}

{'loss': 0.9335, 'learning_rate': 1.6597157187105475e-05, 'epoch': 0.3}

{'loss': 0.8948, 'learning_rate': 1.6417817935230318e-05, 'epoch': 0.3}

{'loss': 0.8876, 'learning_rate': 1.6234898018587336e-05, 'epoch': 0.31}

{'loss': 0.8467, 'learning_rate': 1.6048499492876378e-05, 'epoch': 0.32}

{'loss': 0.9685, 'learning_rate': 1.5858726354602248e-05, 'epoch': 0.33}

{'loss': 0.8555, 'learning_rate': 1.5665684483052425e-05, 'epoch': 0.33}

{'loss': 0.9429, 'learning_rate': 1.5469481581224274e-05, 'epoch': 0.34}

{'loss': 0.8925, 'learning_rate': 1.527022711573479e-05, 'epoch': 0.35}

{'loss': 0.89, 'learning_rate': 1.50680322557464e-05, 'epoch': 0.36}

{'loss': 0.8979, 'learning_rate': 1.4863009810942814e-05, 'epoch': 0.36}

{'loss': 0.8384, 'learning_rate': 1.4655274168589635e-05, 'epoch': 0.37}

{'loss': 0.9603, 'learning_rate': 1.444494122971476e-05, 'epoch': 0.38}

{'loss': 0.9178, 'learning_rate': 1.4232128344444251e-05, 'epoch': 0.38}

{'loss': 0.8704, 'learning_rate': 1.4016954246529697e-05, 'epoch': 0.39}

{'loss': 0.855, 'learning_rate': 1.37995389871036e-05, 'epoch': 0.4}

{'loss': 0.8708, 'learning_rate': 1.3580003867699801e-05, 'epoch': 0.41}

{'loss': 0.9286, 'learning_rate': 1.3358471372576229e-05, 'epoch': 0.41}

{'loss': 0.8638, 'learning_rate': 1.3135065100377816e-05, 'epoch': 0.42}

{'loss': 0.8764, 'learning_rate': 1.2909909695177647e-05, 'epoch': 0.43}

{'loss': 0.883, 'learning_rate': 1.268313077693485e-05, 'epoch': 0.43}

{'loss': 0.8511, 'learning_rate': 1.2454854871407993e-05, 'epoch': 0.44}

{'loss': 0.939, 'learning_rate': 1.2225209339563144e-05, 'epoch': 0.45}

{'loss': 0.9391, 'learning_rate': 1.1994322306515926e-05, 'epoch': 0.46}

{'loss': 0.9527, 'learning_rate': 1.176232259004722e-05, 'epoch': 0.46}

{'loss': 0.919, 'learning_rate': 1.1529339628732462e-05, 'epoch': 0.47}

{'loss': 2.5372, 'learning_rate': 1.1295503409724526e-05, 'epoch': 0.48}

{'loss': 0.9221, 'learning_rate': 1.1060944396230583e-05, 'epoch': 0.49}

{'loss': 0.9434, 'learning_rate': 1.0825793454723325e-05, 'epoch': 0.49}

{'loss': 0.9283, 'learning_rate': 1.0590181781927229e-05, 'epoch': 0.5}

{'loss': 0.9134, 'learning_rate': 1.0354240831620542e-05, 'epoch': 0.51}

{'loss': 0.8693, 'learning_rate': 1.0118102241293848e-05, 'epoch': 0.51}

{'loss': 0.8707, 'learning_rate': 9.881897758706155e-06, 'epoch': 0.52}

{'loss': 0.8631, 'learning_rate': 9.645759168379463e-06, 'epoch': 0.53}

{'loss': 0.9276, 'learning_rate': 9.409818218072774e-06, 'epoch': 0.54}

{'loss': 0.9398, 'learning_rate': 9.174206545276678e-06, 'epoch': 0.54}

{'loss': 0.8742, 'learning_rate': 8.93905560376942e-06, 'epoch': 0.55}

{'loss': 0.8862, 'learning_rate': 8.704496590275479e-06, 'epoch': 0.56}

{'loss': 0.9452, 'learning_rate': 8.47066037126754e-06, 'epoch': 0.57}

{'loss': 0.8559, 'learning_rate': 8.237677409952784e-06, 'epoch': 0.57}

{'loss': 0.8349, 'learning_rate': 8.005677693484077e-06, 'epoch': 0.58}

{'loss': 0.9328, 'learning_rate': 7.774790660436857e-06, 'epoch': 0.59}

{'loss': 0.9303, 'learning_rate': 7.545145128592009e-06, 'epoch': 0.59}

{'loss': 0.8603, 'learning_rate': 7.316869223065156e-06, 'epoch': 0.6}

{'loss': 0.8871, 'learning_rate': 7.090090304822356e-06, 'epoch': 0.61}

{'loss': 0.9374, 'learning_rate': 6.864934899622191e-06, 'epoch': 0.62}

{'loss': 0.9391, 'learning_rate': 6.6415286274237744e-06, 'epoch': 0.62}

{'loss': 0.8867, 'learning_rate': 6.419996132300203e-06, 'epoch': 0.63}

{'loss': 0.9052, 'learning_rate': 6.200461012896401e-06, 'epoch': 0.64}

{'loss': 0.9565, 'learning_rate': 5.983045753470308e-06, 'epoch': 0.64}

{'loss': 0.8775, 'learning_rate': 5.7678716555557515e-06, 'epoch': 0.65}

{'loss': 0.9363, 'learning_rate': 5.5550587702852465e-06, 'epoch': 0.66}

{'loss': 0.8493, 'learning_rate': 5.344725831410369e-06, 'epoch': 0.67}

{'loss': 0.8641, 'learning_rate': 5.136990189057187e-06, 'epoch': 0.67}

{'loss': 0.941, 'learning_rate': 4.931967744253601e-06, 'epoch': 0.68}

{'loss': 0.8531, 'learning_rate': 4.729772884265212e-06, 'epoch': 0.69}

{'loss': 0.8851, 'learning_rate': 4.530518418775734e-06, 'epoch': 0.7}

{'loss': 0.9149, 'learning_rate': 4.33431551694758e-06, 'epoch': 0.7}

{'loss': 0.8433, 'learning_rate': 4.1412736453977545e-06, 'epoch': 0.71}

{'loss': 0.9105, 'learning_rate': 3.9515005071236274e-06, 'epoch': 0.72}

{'loss': 0.9233, 'learning_rate': 3.7651019814126656e-06, 'epoch': 0.72}

{'loss': 0.8021, 'learning_rate': 3.582182064769687e-06, 'epoch': 0.73}

{'loss': 0.8521, 'learning_rate': 3.402842812894529e-06, 'epoch': 0.74}

{'loss': 0.849, 'learning_rate': 3.2271842837425917e-06, 'epoch': 0.75}

{'loss': 0.8733, 'learning_rate': 3.0553044816999133e-06, 'epoch': 0.75}

{'loss': 0.8613, 'learning_rate': 2.8872993029040506e-06, 'epoch': 0.76}

{'loss': 0.8715, 'learning_rate': 2.723262481741138e-06, 'epoch': 0.77}

{'loss': 0.9189, 'learning_rate': 2.563285538549104e-06, 'epoch': 0.78}

{'loss': 0.8603, 'learning_rate': 2.407457728556115e-06, 'epoch': 0.78}

{'loss': 0.9895, 'learning_rate': 2.2558659920828095e-06, 'epoch': 0.79}

{'loss': 0.8798, 'learning_rate': 2.1085949060360654e-06, 'epoch': 0.8}

{'loss': 0.9306, 'learning_rate': 1.96572663672139e-06, 'epoch': 0.8}

{'loss': 0.8902, 'learning_rate': 1.8273408940002202e-06, 'epoch': 0.81}

{'loss': 0.8555, 'learning_rate': 1.693514886817772e-06, 'epoch': 0.82}

{'loss': 0.7665, 'learning_rate': 1.5643232801261731e-06, 'epoch': 0.83}

{'loss': 0.8648, 'learning_rate': 1.4398381532270001e-06, 'epoch': 0.83}

{'loss': 0.9216, 'learning_rate': 1.3201289595563693e-06, 'epoch': 0.84}

{'loss': 0.8506, 'learning_rate': 1.2052624879351105e-06, 'epoch': 0.85}

{'loss': 0.9544, 'learning_rate': 1.0953028253055541e-06, 'epoch': 0.86}

{'loss': 0.9033, 'learning_rate': 9.903113209758098e-07, 'epoch': 0.86}

{'loss': 0.9014, 'learning_rate': 8.903465523913957e-07, 'epoch': 0.87}

{'loss': 0.9006, 'learning_rate': 7.954642924533995e-07, 'epoch': 0.88}

{'loss': 0.7909, 'learning_rate': 7.057174784013432e-07, 'epoch': 0.88}

{'loss': 0.8837, 'learning_rate': 6.211561822781476e-07, 'epoch': 0.89}

{'loss': 0.9041, 'learning_rate': 5.418275829936537e-07, 'epoch': 0.9}

{'loss': 0.8112, 'learning_rate': 4.6777594000230855e-07, 'epoch': 0.91}

{'loss': 0.8348, 'learning_rate': 3.9904256860967436e-07, 'epoch': 0.91}

{'loss': 0.8782, 'learning_rate': 3.356658169215743e-07, 'epoch': 0.92}

{'loss': 0.9084, 'learning_rate': 2.776810444486944e-07, 'epoch': 0.93}

{'loss': 0.8364, 'learning_rate': 2.2512060237861455e-07, 'epoch': 0.93}

{'loss': 0.8592, 'learning_rate': 1.7801381552624565e-07, 'epoch': 0.94}

{'loss': 0.9462, 'learning_rate': 1.3638696597277678e-07, 'epoch': 0.95}

{'loss': 0.8142, 'learning_rate': 1.0026327840221728e-07, 'epoch': 0.96}

{'loss': 0.9137, 'learning_rate': 6.966290714375934e-08, 'epoch': 0.96}

{'loss': 0.9037, 'learning_rate': 4.460292492716512e-08, 'epoch': 0.97}

{'loss': 0.8556, 'learning_rate': 2.509731335744281e-08, 'epoch': 0.98}

{'loss': 0.8601, 'learning_rate': 1.1156955114162149e-08, 'epoch': 0.99}

{'loss': 0.9144, 'learning_rate': 2.7896278797256983e-09, 'epoch': 0.99}

{'loss': 0.8597, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 353.3426, 'train_samples_per_second': 12.498, 'train_steps_per_second': 0.391, 'train_loss': 0.9084273386692655, 'epoch': 1.0}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
