[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|                                                                                                                            | 0/138 [00:00<?, ?it/s]/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
                                                                                                                                                         

{'loss': 1.1278, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}

{'loss': 1.0793, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}

{'loss': 1.0233, 'learning_rate': 1.2e-05, 'epoch': 0.02}

{'loss': 1.0839, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.03}

{'loss': 1.0109, 'learning_rate': 2e-05, 'epoch': 0.04}

{'loss': 1.0485, 'learning_rate': 1.9997210372120276e-05, 'epoch': 0.04}

{'loss': 1.0824, 'learning_rate': 1.998884304488584e-05, 'epoch': 0.05}

{'loss': 1.0073, 'learning_rate': 1.997490268664256e-05, 'epoch': 0.06}

{'loss': 0.9672, 'learning_rate': 1.995539707507284e-05, 'epoch': 0.07}

{'loss': 1.0114, 'learning_rate': 1.9930337092856243e-05, 'epoch': 0.07}

{'loss': 0.9722, 'learning_rate': 1.9899736721597787e-05, 'epoch': 0.08}

{'loss': 0.9443, 'learning_rate': 1.9863613034027224e-05, 'epoch': 0.09}

{'loss': 1.0415, 'learning_rate': 1.9821986184473757e-05, 'epoch': 0.09}

{'loss': 1.1049, 'learning_rate': 1.9774879397621387e-05, 'epoch': 0.1}

{'loss': 0.9671, 'learning_rate': 1.9722318955551307e-05, 'epoch': 0.11}

{'loss': 0.9485, 'learning_rate': 1.966433418307843e-05, 'epoch': 0.12}

{'loss': 0.9882, 'learning_rate': 1.960095743139033e-05, 'epoch': 0.12}

{'loss': 0.977, 'learning_rate': 1.9532224059997693e-05, 'epoch': 0.13}

{'loss': 0.9815, 'learning_rate': 1.9458172417006347e-05, 'epoch': 0.14}

{'loss': 0.9084, 'learning_rate': 1.9378843817721856e-05, 'epoch': 0.14}

{'loss': 1.0023, 'learning_rate': 1.929428252159866e-05, 'epoch': 0.15}

{'loss': 1.0048, 'learning_rate': 1.9204535707546602e-05, 'epoch': 0.16}

{'loss': 0.969, 'learning_rate': 1.9109653447608607e-05, 'epoch': 0.17}

{'loss': 0.9508, 'learning_rate': 1.900968867902419e-05, 'epoch': 0.17}

{'loss': 1.0049, 'learning_rate': 1.8904697174694447e-05, 'epoch': 0.18}

{'loss': 0.9575, 'learning_rate': 1.879473751206489e-05, 'epoch': 0.19}

{'loss': 0.9339, 'learning_rate': 1.8679871040443632e-05, 'epoch': 0.2}

{'loss': 0.9862, 'learning_rate': 1.8560161846773002e-05, 'epoch': 0.2}

{'loss': 1.0518, 'learning_rate': 1.8435676719873828e-05, 'epoch': 0.21}

{'loss': 0.9547, 'learning_rate': 1.830648511318223e-05, 'epoch': 0.22}

{'loss': 0.9369, 'learning_rate': 1.817265910599978e-05, 'epoch': 0.22}

{'loss': 0.9978, 'learning_rate': 1.8034273363278615e-05, 'epoch': 0.23}

{'loss': 0.8227, 'learning_rate': 1.789140509396394e-05, 'epoch': 0.24}

{'loss': 0.9036, 'learning_rate': 1.7744134007917195e-05, 'epoch': 0.25}

{'loss': 1.0161, 'learning_rate': 1.7592542271443888e-05, 'epoch': 0.25}

{'loss': 1.0073, 'learning_rate': 1.74367144614509e-05, 'epoch': 0.26}

{'loss': 0.9474, 'learning_rate': 1.7276737518258865e-05, 'epoch': 0.27}

{'loss': 0.8899, 'learning_rate': 1.7112700697095955e-05, 'epoch': 0.28}

{'loss': 0.9605, 'learning_rate': 1.6944695518300087e-05, 'epoch': 0.28}

{'loss': 0.9375, 'learning_rate': 1.6772815716257414e-05, 'epoch': 0.29}

{'loss': 0.9626, 'learning_rate': 1.6597157187105475e-05, 'epoch': 0.3}

{'loss': 0.9356, 'learning_rate': 1.6417817935230318e-05, 'epoch': 0.3}

{'loss': 0.9267, 'learning_rate': 1.6234898018587336e-05, 'epoch': 0.31}

{'loss': 0.8797, 'learning_rate': 1.6048499492876378e-05, 'epoch': 0.32}

{'loss': 1.0028, 'learning_rate': 1.5858726354602248e-05, 'epoch': 0.33}

{'loss': 0.8888, 'learning_rate': 1.5665684483052425e-05, 'epoch': 0.33}

{'loss': 0.9709, 'learning_rate': 1.5469481581224274e-05, 'epoch': 0.34}

{'loss': 0.9147, 'learning_rate': 1.527022711573479e-05, 'epoch': 0.35}

{'loss': 0.9314, 'learning_rate': 1.50680322557464e-05, 'epoch': 0.36}

{'loss': 0.9406, 'learning_rate': 1.4863009810942814e-05, 'epoch': 0.36}

{'loss': 0.8774, 'learning_rate': 1.4655274168589635e-05, 'epoch': 0.37}

{'loss': 0.9771, 'learning_rate': 1.444494122971476e-05, 'epoch': 0.38}

{'loss': 0.9462, 'learning_rate': 1.4232128344444251e-05, 'epoch': 0.38}

{'loss': 0.8982, 'learning_rate': 1.4016954246529697e-05, 'epoch': 0.39}

{'loss': 0.8856, 'learning_rate': 1.37995389871036e-05, 'epoch': 0.4}

{'loss': 0.8868, 'learning_rate': 1.3580003867699801e-05, 'epoch': 0.41}

{'loss': 0.96, 'learning_rate': 1.3358471372576229e-05, 'epoch': 0.41}

{'loss': 0.8923, 'learning_rate': 1.3135065100377816e-05, 'epoch': 0.42}

{'loss': 0.9131, 'learning_rate': 1.2909909695177647e-05, 'epoch': 0.43}

{'loss': 0.9144, 'learning_rate': 1.268313077693485e-05, 'epoch': 0.43}

{'loss': 0.8805, 'learning_rate': 1.2454854871407993e-05, 'epoch': 0.44}

{'loss': 0.9637, 'learning_rate': 1.2225209339563144e-05, 'epoch': 0.45}

{'loss': 0.9706, 'learning_rate': 1.1994322306515926e-05, 'epoch': 0.46}

{'loss': 0.9782, 'learning_rate': 1.176232259004722e-05, 'epoch': 0.46}

{'loss': 0.998, 'learning_rate': 1.1529339628732462e-05, 'epoch': 0.47}

{'loss': 0.9263, 'learning_rate': 1.1295503409724526e-05, 'epoch': 0.48}

{'loss': 0.9456, 'learning_rate': 1.1060944396230583e-05, 'epoch': 0.49}

{'loss': 0.9622, 'learning_rate': 1.0825793454723325e-05, 'epoch': 0.49}

{'loss': 0.9488, 'learning_rate': 1.0590181781927229e-05, 'epoch': 0.5}

{'loss': 0.9476, 'learning_rate': 1.0354240831620542e-05, 'epoch': 0.51}

{'loss': 0.8936, 'learning_rate': 1.0118102241293848e-05, 'epoch': 0.51}

{'loss': 0.8999, 'learning_rate': 9.881897758706155e-06, 'epoch': 0.52}

{'loss': 0.8893, 'learning_rate': 9.645759168379463e-06, 'epoch': 0.53}

{'loss': 0.9427, 'learning_rate': 9.409818218072774e-06, 'epoch': 0.54}

{'loss': 0.9677, 'learning_rate': 9.174206545276678e-06, 'epoch': 0.54}

{'loss': 0.9061, 'learning_rate': 8.93905560376942e-06, 'epoch': 0.55}

{'loss': 0.9146, 'learning_rate': 8.704496590275479e-06, 'epoch': 0.56}

{'loss': 0.9625, 'learning_rate': 8.47066037126754e-06, 'epoch': 0.57}

{'loss': 0.8836, 'learning_rate': 8.237677409952784e-06, 'epoch': 0.57}

{'loss': 0.8546, 'learning_rate': 8.005677693484077e-06, 'epoch': 0.58}

{'loss': 0.9569, 'learning_rate': 7.774790660436857e-06, 'epoch': 0.59}

{'loss': 0.9495, 'learning_rate': 7.545145128592009e-06, 'epoch': 0.59}

{'loss': 0.8776, 'learning_rate': 7.316869223065156e-06, 'epoch': 0.6}

{'loss': 0.9064, 'learning_rate': 7.090090304822356e-06, 'epoch': 0.61}

{'loss': 0.9606, 'learning_rate': 6.864934899622191e-06, 'epoch': 0.62}

{'loss': 0.954, 'learning_rate': 6.6415286274237744e-06, 'epoch': 0.62}

{'loss': 0.9108, 'learning_rate': 6.419996132300203e-06, 'epoch': 0.63}

{'loss': 0.9293, 'learning_rate': 6.200461012896401e-06, 'epoch': 0.64}

{'loss': 0.9789, 'learning_rate': 5.983045753470308e-06, 'epoch': 0.64}

{'loss': 0.8991, 'learning_rate': 5.7678716555557515e-06, 'epoch': 0.65}

{'loss': 0.9677, 'learning_rate': 5.5550587702852465e-06, 'epoch': 0.66}

{'loss': 0.8715, 'learning_rate': 5.344725831410369e-06, 'epoch': 0.67}

{'loss': 0.892, 'learning_rate': 5.136990189057187e-06, 'epoch': 0.67}

{'loss': 0.9618, 'learning_rate': 4.931967744253601e-06, 'epoch': 0.68}

{'loss': 0.8771, 'learning_rate': 4.729772884265212e-06, 'epoch': 0.69}

{'loss': 0.9116, 'learning_rate': 4.530518418775734e-06, 'epoch': 0.7}

{'loss': 0.9303, 'learning_rate': 4.33431551694758e-06, 'epoch': 0.7}

{'loss': 0.862, 'learning_rate': 4.1412736453977545e-06, 'epoch': 0.71}

{'loss': 0.9315, 'learning_rate': 3.9515005071236274e-06, 'epoch': 0.72}

{'loss': 0.9479, 'learning_rate': 3.7651019814126656e-06, 'epoch': 0.72}

{'loss': 0.836, 'learning_rate': 3.582182064769687e-06, 'epoch': 0.73}

{'loss': 0.8735, 'learning_rate': 3.402842812894529e-06, 'epoch': 0.74}

{'loss': 0.8774, 'learning_rate': 3.2271842837425917e-06, 'epoch': 0.75}

{'loss': 0.8899, 'learning_rate': 3.0553044816999133e-06, 'epoch': 0.75}

{'loss': 0.8782, 'learning_rate': 2.8872993029040506e-06, 'epoch': 0.76}

{'loss': 0.8912, 'learning_rate': 2.723262481741138e-06, 'epoch': 0.77}

{'loss': 0.9377, 'learning_rate': 2.563285538549104e-06, 'epoch': 0.78}

{'loss': 0.8867, 'learning_rate': 2.407457728556115e-06, 'epoch': 0.78}

{'loss': 1.0047, 'learning_rate': 2.2558659920828095e-06, 'epoch': 0.79}

{'loss': 0.9047, 'learning_rate': 2.1085949060360654e-06, 'epoch': 0.8}

{'loss': 0.9513, 'learning_rate': 1.96572663672139e-06, 'epoch': 0.8}

{'loss': 0.91, 'learning_rate': 1.8273408940002202e-06, 'epoch': 0.81}

{'loss': 0.8827, 'learning_rate': 1.693514886817772e-06, 'epoch': 0.82}

{'loss': 0.7906, 'learning_rate': 1.5643232801261731e-06, 'epoch': 0.83}

{'loss': 0.8891, 'learning_rate': 1.4398381532270001e-06, 'epoch': 0.83}

{'loss': 0.9345, 'learning_rate': 1.3201289595563693e-06, 'epoch': 0.84}

{'loss': 0.8759, 'learning_rate': 1.2052624879351105e-06, 'epoch': 0.85}

{'loss': 0.9744, 'learning_rate': 1.0953028253055541e-06, 'epoch': 0.86}

{'loss': 0.9251, 'learning_rate': 9.903113209758098e-07, 'epoch': 0.86}

{'loss': 0.9153, 'learning_rate': 8.903465523913957e-07, 'epoch': 0.87}

{'loss': 0.921, 'learning_rate': 7.954642924533995e-07, 'epoch': 0.88}

{'loss': 0.8099, 'learning_rate': 7.057174784013432e-07, 'epoch': 0.88}

{'loss': 0.9002, 'learning_rate': 6.211561822781476e-07, 'epoch': 0.89}

{'loss': 0.9289, 'learning_rate': 5.418275829936537e-07, 'epoch': 0.9}

{'loss': 0.8269, 'learning_rate': 4.6777594000230855e-07, 'epoch': 0.91}

{'loss': 0.8533, 'learning_rate': 3.9904256860967436e-07, 'epoch': 0.91}

{'loss': 0.896, 'learning_rate': 3.356658169215743e-07, 'epoch': 0.92}

{'loss': 0.9291, 'learning_rate': 2.776810444486944e-07, 'epoch': 0.93}

{'loss': 0.858, 'learning_rate': 2.2512060237861455e-07, 'epoch': 0.93}

{'loss': 0.8787, 'learning_rate': 1.7801381552624565e-07, 'epoch': 0.94}

{'loss': 0.9633, 'learning_rate': 1.3638696597277678e-07, 'epoch': 0.95}

{'loss': 0.8303, 'learning_rate': 1.0026327840221728e-07, 'epoch': 0.96}

{'loss': 0.9298, 'learning_rate': 6.966290714375934e-08, 'epoch': 0.96}

{'loss': 0.9207, 'learning_rate': 4.460292492716512e-08, 'epoch': 0.97}

{'loss': 0.8875, 'learning_rate': 2.509731335744281e-08, 'epoch': 0.98}

{'loss': 0.8835, 'learning_rate': 1.1156955114162149e-08, 'epoch': 0.99}

{'loss': 0.9356, 'learning_rate': 2.7896278797256983e-09, 'epoch': 0.99}

{'loss': 0.883, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 354.2741, 'train_samples_per_second': 12.465, 'train_steps_per_second': 0.39, 'train_loss': 0.9361813495988431, 'epoch': 1.0}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
