[34m[1mwandb[39m[22m: Detected [openai] in use.
[34m[1mwandb[39m[22m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[39m[22m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|                                                                                       | 0/138 [00:00<?, ?it/s]/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
{'loss': 0.937, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
{'loss': 0.9323, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}
{'loss': 0.8793, 'learning_rate': 1.2e-05, 'epoch': 0.02}
{'loss': 0.928, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.03}
{'loss': 0.8804, 'learning_rate': 2e-05, 'epoch': 0.04}
{'loss': 0.9058, 'learning_rate': 1.9997210372120276e-05, 'epoch': 0.04}
{'loss': 0.9832, 'learning_rate': 1.998884304488584e-05, 'epoch': 0.05}
{'loss': 0.8844, 'learning_rate': 1.997490268664256e-05, 'epoch': 0.06}
{'loss': 0.8563, 'learning_rate': 1.995539707507284e-05, 'epoch': 0.07}
{'loss': 0.9285, 'learning_rate': 1.9930337092856243e-05, 'epoch': 0.07}
{'loss': 0.8884, 'learning_rate': 1.9899736721597787e-05, 'epoch': 0.08}
{'loss': 0.8811, 'learning_rate': 1.9863613034027224e-05, 'epoch': 0.09}
{'loss': 0.9659, 'learning_rate': 1.9821986184473757e-05, 'epoch': 0.09}
{'loss': 1.0284, 'learning_rate': 1.9774879397621387e-05, 'epoch': 0.1}
{'loss': 0.8906, 'learning_rate': 1.9722318955551307e-05, 'epoch': 0.11}
{'loss': 0.8883, 'learning_rate': 1.966433418307843e-05, 'epoch': 0.12}
{'loss': 0.9232, 'learning_rate': 1.960095743139033e-05, 'epoch': 0.12}
{'loss': 0.9004, 'learning_rate': 1.9532224059997693e-05, 'epoch': 0.13}
{'loss': 0.9219, 'learning_rate': 1.9458172417006347e-05, 'epoch': 0.14}
{'loss': 0.853, 'learning_rate': 1.9378843817721856e-05, 'epoch': 0.14}
{'loss': 0.9279, 'learning_rate': 1.929428252159866e-05, 'epoch': 0.15}
{'loss': 0.9494, 'learning_rate': 1.9204535707546602e-05, 'epoch': 0.16}
{'loss': 0.901, 'learning_rate': 1.9109653447608607e-05, 'epoch': 0.17}
{'loss': 0.8934, 'learning_rate': 1.900968867902419e-05, 'epoch': 0.17}
{'loss': 0.9531, 'learning_rate': 1.8904697174694447e-05, 'epoch': 0.18}
{'loss': 0.8861, 'learning_rate': 1.879473751206489e-05, 'epoch': 0.19}
{'loss': 0.8802, 'learning_rate': 1.8679871040443632e-05, 'epoch': 0.2}
{'loss': 0.9249, 'learning_rate': 1.8560161846773002e-05, 'epoch': 0.2}
{'loss': 0.9903, 'learning_rate': 1.8435676719873828e-05, 'epoch': 0.21}
{'loss': 0.9123, 'learning_rate': 1.830648511318223e-05, 'epoch': 0.22}
{'loss': 0.874, 'learning_rate': 1.817265910599978e-05, 'epoch': 0.22}
{'loss': 0.9399, 'learning_rate': 1.8034273363278615e-05, 'epoch': 0.23}
{'loss': 0.7742, 'learning_rate': 1.789140509396394e-05, 'epoch': 0.24}
{'loss': 0.8533, 'learning_rate': 1.7744134007917195e-05, 'epoch': 0.25}
{'loss': 0.9696, 'learning_rate': 1.7592542271443888e-05, 'epoch': 0.25}
{'loss': 0.9741, 'learning_rate': 1.74367144614509e-05, 'epoch': 0.26}
{'loss': 0.9119, 'learning_rate': 1.7276737518258865e-05, 'epoch': 0.27}
{'loss': 0.8466, 'learning_rate': 1.7112700697095955e-05, 'epoch': 0.28}
{'loss': 0.9136, 'learning_rate': 1.6944695518300087e-05, 'epoch': 0.28}
{'loss': 0.8934, 'learning_rate': 1.6772815716257414e-05, 'epoch': 0.29}
{'loss': 0.9323, 'learning_rate': 1.6597157187105475e-05, 'epoch': 0.3}
{'loss': 0.8935, 'learning_rate': 1.6417817935230318e-05, 'epoch': 0.3}
{'loss': 0.8841, 'learning_rate': 1.6234898018587336e-05, 'epoch': 0.31}
{'loss': 0.8463, 'learning_rate': 1.6048499492876378e-05, 'epoch': 0.32}
{'loss': 0.9665, 'learning_rate': 1.5858726354602248e-05, 'epoch': 0.33}
{'loss': 0.8545, 'learning_rate': 1.5665684483052425e-05, 'epoch': 0.33}
{'loss': 0.9407, 'learning_rate': 1.5469481581224274e-05, 'epoch': 0.34}
{'loss': 0.8936, 'learning_rate': 1.527022711573479e-05, 'epoch': 0.35}
{'loss': 0.8893, 'learning_rate': 1.50680322557464e-05, 'epoch': 0.36}
{'loss': 0.8949, 'learning_rate': 1.4863009810942814e-05, 'epoch': 0.36}
{'loss': 0.8385, 'learning_rate': 1.4655274168589635e-05, 'epoch': 0.37}
{'loss': 0.9583, 'learning_rate': 1.444494122971476e-05, 'epoch': 0.38}
{'loss': 0.9152, 'learning_rate': 1.4232128344444251e-05, 'epoch': 0.38}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 138/138 [05:50<00:00,  2.64s/it]
{'loss': 0.8685, 'learning_rate': 1.4016954246529697e-05, 'epoch': 0.39}
{'loss': 0.8536, 'learning_rate': 1.37995389871036e-05, 'epoch': 0.4}
{'loss': 0.8568, 'learning_rate': 1.3580003867699801e-05, 'epoch': 0.41}
{'loss': 0.9273, 'learning_rate': 1.3358471372576229e-05, 'epoch': 0.41}
{'loss': 0.8613, 'learning_rate': 1.3135065100377816e-05, 'epoch': 0.42}
{'loss': 0.8748, 'learning_rate': 1.2909909695177647e-05, 'epoch': 0.43}
{'loss': 0.8807, 'learning_rate': 1.268313077693485e-05, 'epoch': 0.43}
{'loss': 0.8491, 'learning_rate': 1.2454854871407993e-05, 'epoch': 0.44}
{'loss': 0.9379, 'learning_rate': 1.2225209339563144e-05, 'epoch': 0.45}
{'loss': 0.9365, 'learning_rate': 1.1994322306515926e-05, 'epoch': 0.46}
{'loss': 0.9496, 'learning_rate': 1.176232259004722e-05, 'epoch': 0.46}
{'loss': 0.916, 'learning_rate': 1.1529339628732462e-05, 'epoch': 0.47}
{'loss': 0.9069, 'learning_rate': 1.1295503409724526e-05, 'epoch': 0.48}
{'loss': 0.9216, 'learning_rate': 1.1060944396230583e-05, 'epoch': 0.49}
{'loss': 0.9424, 'learning_rate': 1.0825793454723325e-05, 'epoch': 0.49}
{'loss': 0.9263, 'learning_rate': 1.0590181781927229e-05, 'epoch': 0.5}
{'loss': 0.9128, 'learning_rate': 1.0354240831620542e-05, 'epoch': 0.51}
{'loss': 0.9863, 'learning_rate': 1.0118102241293848e-05, 'epoch': 0.51}
{'loss': 0.8671, 'learning_rate': 9.881897758706155e-06, 'epoch': 0.52}
{'loss': 0.8615, 'learning_rate': 9.645759168379463e-06, 'epoch': 0.53}
{'loss': 0.9249, 'learning_rate': 9.409818218072774e-06, 'epoch': 0.54}
{'loss': 0.9412, 'learning_rate': 9.174206545276678e-06, 'epoch': 0.54}
{'loss': 0.8723, 'learning_rate': 8.93905560376942e-06, 'epoch': 0.55}
{'loss': 0.8828, 'learning_rate': 8.704496590275479e-06, 'epoch': 0.56}
{'loss': 0.9414, 'learning_rate': 8.47066037126754e-06, 'epoch': 0.57}
{'loss': 0.853, 'learning_rate': 8.237677409952784e-06, 'epoch': 0.57}
{'loss': 0.8324, 'learning_rate': 8.005677693484077e-06, 'epoch': 0.58}
{'loss': 0.9296, 'learning_rate': 7.774790660436857e-06, 'epoch': 0.59}
{'loss': 0.9268, 'learning_rate': 7.545145128592009e-06, 'epoch': 0.59}
{'loss': 0.857, 'learning_rate': 7.316869223065156e-06, 'epoch': 0.6}
{'loss': 0.8867, 'learning_rate': 7.090090304822356e-06, 'epoch': 0.61}
{'loss': 0.9294, 'learning_rate': 6.864934899622191e-06, 'epoch': 0.62}
{'loss': 0.9356, 'learning_rate': 6.6415286274237744e-06, 'epoch': 0.62}
{'loss': 0.8862, 'learning_rate': 6.419996132300203e-06, 'epoch': 0.63}
{'loss': 0.9031, 'learning_rate': 6.200461012896401e-06, 'epoch': 0.64}
{'loss': 0.954, 'learning_rate': 5.983045753470308e-06, 'epoch': 0.64}
{'loss': 0.8744, 'learning_rate': 5.7678716555557515e-06, 'epoch': 0.65}
{'loss': 0.9316, 'learning_rate': 5.5550587702852465e-06, 'epoch': 0.66}
{'loss': 0.8502, 'learning_rate': 5.344725831410369e-06, 'epoch': 0.67}
{'loss': 0.8589, 'learning_rate': 5.136990189057187e-06, 'epoch': 0.67}
{'loss': 0.9395, 'learning_rate': 4.931967744253601e-06, 'epoch': 0.68}
{'loss': 0.8521, 'learning_rate': 4.729772884265212e-06, 'epoch': 0.69}
{'loss': 0.8805, 'learning_rate': 4.530518418775734e-06, 'epoch': 0.7}
{'loss': 0.913, 'learning_rate': 4.33431551694758e-06, 'epoch': 0.7}
{'loss': 0.8411, 'learning_rate': 4.1412736453977545e-06, 'epoch': 0.71}
{'loss': 0.907, 'learning_rate': 3.9515005071236274e-06, 'epoch': 0.72}
{'loss': 0.92, 'learning_rate': 3.7651019814126656e-06, 'epoch': 0.72}
{'loss': 0.7991, 'learning_rate': 3.582182064769687e-06, 'epoch': 0.73}
{'loss': 0.848, 'learning_rate': 3.402842812894529e-06, 'epoch': 0.74}
{'loss': 0.8454, 'learning_rate': 3.2271842837425917e-06, 'epoch': 0.75}
{'loss': 0.8696, 'learning_rate': 3.0553044816999133e-06, 'epoch': 0.75}
{'loss': 0.8511, 'learning_rate': 2.8872993029040506e-06, 'epoch': 0.76}
{'loss': 0.8681, 'learning_rate': 2.723262481741138e-06, 'epoch': 0.77}
{'loss': 0.9137, 'learning_rate': 2.563285538549104e-06, 'epoch': 0.78}
{'loss': 0.8575, 'learning_rate': 2.407457728556115e-06, 'epoch': 0.78}
{'loss': 0.9868, 'learning_rate': 2.2558659920828095e-06, 'epoch': 0.79}
{'loss': 0.8739, 'learning_rate': 2.1085949060360654e-06, 'epoch': 0.8}
{'loss': 0.9286, 'learning_rate': 1.96572663672139e-06, 'epoch': 0.8}
{'loss': 0.8862, 'learning_rate': 1.8273408940002202e-06, 'epoch': 0.81}
{'loss': 0.8529, 'learning_rate': 1.693514886817772e-06, 'epoch': 0.82}
{'loss': 0.7635, 'learning_rate': 1.5643232801261731e-06, 'epoch': 0.83}
{'loss': 0.8621, 'learning_rate': 1.4398381532270001e-06, 'epoch': 0.83}
{'loss': 0.9181, 'learning_rate': 1.3201289595563693e-06, 'epoch': 0.84}
{'loss': 0.961, 'learning_rate': 1.2052624879351105e-06, 'epoch': 0.85}
{'loss': 0.9527, 'learning_rate': 1.0953028253055541e-06, 'epoch': 0.86}
{'loss': 0.8995, 'learning_rate': 9.903113209758098e-07, 'epoch': 0.86}
{'loss': 0.8984, 'learning_rate': 8.903465523913957e-07, 'epoch': 0.87}
{'loss': 0.8951, 'learning_rate': 7.954642924533995e-07, 'epoch': 0.88}
{'loss': 0.7897, 'learning_rate': 7.057174784013432e-07, 'epoch': 0.88}
{'loss': 0.8803, 'learning_rate': 6.211561822781476e-07, 'epoch': 0.89}
{'loss': 0.8998, 'learning_rate': 5.418275829936537e-07, 'epoch': 0.9}
{'loss': 0.8117, 'learning_rate': 4.6777594000230855e-07, 'epoch': 0.91}
{'loss': 0.833, 'learning_rate': 3.9904256860967436e-07, 'epoch': 0.91}
{'loss': 0.874, 'learning_rate': 3.356658169215743e-07, 'epoch': 0.92}
{'loss': 0.9011, 'learning_rate': 2.776810444486944e-07, 'epoch': 0.93}
{'loss': 0.8321, 'learning_rate': 2.2512060237861455e-07, 'epoch': 0.93}
{'loss': 0.858, 'learning_rate': 1.7801381552624565e-07, 'epoch': 0.94}
{'loss': 0.9434, 'learning_rate': 1.3638696597277678e-07, 'epoch': 0.95}
{'loss': 0.8127, 'learning_rate': 1.0026327840221728e-07, 'epoch': 0.96}
{'loss': 0.9108, 'learning_rate': 6.966290714375934e-08, 'epoch': 0.96}
{'loss': 0.9002, 'learning_rate': 4.460292492716512e-08, 'epoch': 0.97}
{'loss': 0.8519, 'learning_rate': 2.509731335744281e-08, 'epoch': 0.98}
{'loss': 0.8577, 'learning_rate': 1.1156955114162149e-08, 'epoch': 0.99}
{'loss': 0.9139, 'learning_rate': 2.7896278797256983e-09, 'epoch': 0.99}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 138/138 [05:50<00:00,  2.54s/it]
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
{'train_runtime': 354.109, 'train_samples_per_second': 12.471, 'train_steps_per_second': 0.39, 'train_loss': 0.8960090698539347, 'epoch': 1.0}