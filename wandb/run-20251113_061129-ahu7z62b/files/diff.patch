diff --git a/llava.egg-info/PKG-INFO b/llava.egg-info/PKG-INFO
index c9b9b80..c5c7e50 100644
--- a/llava.egg-info/PKG-INFO
+++ b/llava.egg-info/PKG-INFO
@@ -39,92 +39,3 @@ Provides-Extra: build
 Requires-Dist: build; extra == "build"
 Requires-Dist: twine; extra == "build"
 Dynamic: license-file
-
-
-This repo contains the core code for CRAG. Download the required pretrained weights including 'clip-vit-large-patch14-336', 'llava-v1.5-7b', 'projector_weight/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5' and put them to checkpoints folder
-
-The training code for the model part is ready;
-
-The evaluation code is ready.
-
-# Install
-
-**1. Clone the repository and enter the project directory**
-
-```bash
-git clone xxxxxx
-cd CRAG
-```
-
-**2. Create a Python 3.10 conda environment and install the package**
-```bash
-conda create -n crag python=3.10 -y
-conda activate crag
-pip install --upgrade pip            # ensure modern pip (PEP 660 support)
-pip install -e .
-```
-
-**3. Install extra dependencies needed for training**
-```bash
-pip install -e ".[train]"
-pip install flash-attn --no-build-isolation
-```
-
-If you get errors when installing flash-attn, try downloading .whl from flash-attn official website first.
-
-# Dataset download
-
-## Dataset Links
-
-Please download the following datasets used for training:
-
-| Domain         | Dataset                                                                 | Link                                                                                  |
-|----------------|-------------------------------------------------------------------------|----------------------------------------------------------------------------------------|
-| Meteorology        | ClimateIQA                                                               | https://huggingface.co/datasets/GPS-Lab/ClimateIQA                                    |
-| Chemistry      | ChemQA                                                                   | https://huggingface.co/datasets/shangzhu/ChemQA                                       |
-| Remote Sensing | GeoChat Instruct                                          | https://huggingface.co/datasets/MBZUAI/GeoChat_Instruct                               |
-| Math           | AMATH-SFT                                                                | https://huggingface.co/datasets/Quinn777/AMATH-SFT                                    |
-| Art            | SemArt                                                                   | https://researchdata.aston.ac.uk/380/1/SemArt.zip                                     |
-| Medical        | LLaVA-Med                                                                 | https://github.com/microsoft/LLaVA-Med                                                |
-| Astronomy      | AstroLLaVA conversations                                                  | https://huggingface.co/datasets/UniverseTBD/AstroLLaVA_convos                         |
-| Agriculture    | Agricultural pests & diseases instruction tuning data                   | https://huggingface.co/datasets/Agri-LLaVA-Anonymous/Agricultural_pests_and_diseases_instruction_tuning_data |
-
-
-Then download the training files from this link:  
-https://huggingface.co/datasets/leo20000306/CRAG_trainjson
-
-## Next Steps: Training
-
-After downloading the datasets, please follow these steps to start fine-tuning on KANT dataset:
-
-1. Open the script file located at `scripts/v1_5/finetune_newdomain.sh`.
-
-2. Modify the `image_folder` parameter inside that script to point to the local path where your images are stored.
-
-3. Run the training script:
-
-   ```bash
-   bash scripts/v1_5/finetune_newdomain.sh
-   ```
-## Evaluation
-
-To run evaluation, please follow the steps below:
-
-1. Download the test file from this link:  
-   https://huggingface.co/datasets/leo20000306/crag_evaldata
-
-2. In the scripts under `scripts/v1_5/eval/`, modify the relevant files to set:
-
-   - `--question-file` → path to the downloaded test file  
-   - `--image-folder` → path to the folder with the images for evaluation
-
-3. Configure the evaluation script `llava/eval/eval_video_qa.py`:
-
-   - Set your **OpenAI key**  
-   - Set the **base URL** for the OpenAI API (used for scoring model predictions vs labels)
-
-4. Once everything is set up, run the evaluation shell script:
-
-   ```bash
-   bash scripts/v1_5/eval/xxx.sh
-
diff --git a/llava.egg-info/SOURCES.txt b/llava.egg-info/SOURCES.txt
index 1e02a0c..09dd7d7 100644
--- a/llava.egg-info/SOURCES.txt
+++ b/llava.egg-info/SOURCES.txt
@@ -1,6 +1,6 @@
 LICENSE
-README.md
 pyproject.toml
+eval/llava_med_eval_qa50/viz_result.py
 llava/__init__.py
 llava/constants.py
 llava/conversation.py
@@ -20,6 +20,7 @@ llava/eval/eval_science_qa.py
 llava/eval/eval_science_qa_gpt4.py
 llava/eval/eval_science_qa_gpt4_requery.py
 llava/eval/eval_textvqa.py
+llava/eval/eval_video_qa-old.py
 llava/eval/eval_video_qa.py
 llava/eval/generate_webpage_data_from_table.py
 llava/eval/geochat_refering.py
@@ -59,6 +60,7 @@ llava/serve/test_message.py
 llava/train/llama_flash_attn_monkey_patch.py
 llava/train/llama_xformers_attn_monkey_patch.py
 llava/train/llava_trainer.py
+llava/train/merge_llava_med_key.py
 llava/train/merge_prompt_key.py
 llava/train/train.py
 llava/train/train_mem.py
diff --git a/llava.egg-info/top_level.txt b/llava.egg-info/top_level.txt
index 7b24c5b..bfd0a08 100644
--- a/llava.egg-info/top_level.txt
+++ b/llava.egg-info/top_level.txt
@@ -1,2 +1,6 @@
+eval
+evaluation_plots_by_task
 llava
 llava_original
+output
+wandb
diff --git a/llava/model/language_model/__pycache__/llava_llama.cpython-310.pyc b/llava/model/language_model/__pycache__/llava_llama.cpython-310.pyc
index e31e56e..7d1bcda 100644
Binary files a/llava/model/language_model/__pycache__/llava_llama.cpython-310.pyc and b/llava/model/language_model/__pycache__/llava_llama.cpython-310.pyc differ
diff --git a/llava/model/language_model/llava_llama.py b/llava/model/language_model/llava_llama.py
index f4c1c52..e93f1db 100644
--- a/llava/model/language_model/llava_llama.py
+++ b/llava/model/language_model/llava_llama.py
@@ -75,7 +75,12 @@ class LlavaConfig(LlamaConfig):
                 'CT':[0, 8],
                 "CXR":[8, 16],
                 "Histopathology":[16, 24],
-                "MRI":[24, 32]}
+                "MRI":[24, 32]},
+            'vqa_rad': {
+                'abd': [0, 8],    
+                'chest': [8, 16],  
+                'head': [16, 24] 
+            }
 
         }
         self.task_pool_index_range = self.dataset_type_map[self.dataset_type]
@@ -106,9 +111,21 @@ class LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaMetaForCausalLM):
         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
 
         self.retriever = Retriever(config)
+        # if config.retriever_state_dict:
+        #     self.retriever.keys = nn.parameter.Parameter(torch.load(config.retriever_state_dict), requires_grad=False)
+        #     print('retriever shape: ', self.retriever.keys.shape)
+        
         if config.retriever_state_dict:
-            self.retriever.keys = nn.parameter.Parameter(torch.load(config.retriever_state_dict), requires_grad=False)
-            print('retriever shape: ', self.retriever.keys.shape)
+            print(f"loading parameters from {config.retriever_state_dict}...")
+            merged_params = torch.load(config.retriever_state_dict)
+            
+            # 1. 加载 keys
+            self.retriever.keys = nn.parameter.Parameter(merged_params['keys'], requires_grad=False)
+            print('Retriever keys: ', self.retriever.keys.shape)
+            
+            # 2. 加载 weight_offset_components
+            self.retriever.weight_offset_components = nn.parameter.Parameter(merged_params['weight_offset_components'], requires_grad=False)
+            print('Retriever weights: ', self.retriever.weight_offset_components.shape)
 
         # Initialize weights and apply final processing
         self.post_init()
diff --git a/llava/train/merge_prompt_key.py b/llava/train/merge_prompt_key.py
index da458f2..11f4e96 100644
--- a/llava/train/merge_prompt_key.py
+++ b/llava/train/merge_prompt_key.py
@@ -47,14 +47,32 @@ print('keys loaded')
 
 prompt_key = torch.cat(
     (
-        CT_prompt_key[:, :1, :, :], 
-        CXR_prompt_key[:, 1:2, :, :], 
-        Histopathology_prompt_key[:, 2:3, :, :], 
-        MRI_prompt_key[:, 3:4, :, :], 
+        CT_prompt_key[:, :8, :, :], 
+        CXR_prompt_key[:, 8:16, :, :], 
+        Histopathology_prompt_key[:, 16:24, :, :], 
+        MRI_prompt_key[:, 24:32, :, :], 
     ), 
     dim=1
 )
 
 print("prompt_key.shape: {}".format(prompt_key.shape))
 
-torch.save(prompt_key, '/vast/users/xiaodan/haokunlin/Continual_LLaVA/llava/output/prompt-key/merged_prompt_key.pth')
\ No newline at end of file
+torch.save(prompt_key, '/vast/users/xiaodan/haokunlin/Continual_LLaVA/llava/output/prompt-key/merged_prompt_key.pth')
+
+# CT_prompt_key = torch.load('/vast/users/xiaodan/haokunlin/Continual_LLaVA/llava/output/prompt-key/CT.pth')['keys'] # [1, 4, 32, 192]
+# CXR_prompt_key = torch.load('/vast/users/xiaodan/haokunlin/Continual_LLaVA/llava/output/prompt-key/CXR.pth')['keys']
+# Histopathology_prompt_key = torch.load('/vast/users/xiaodan/haokunlin/Continual_LLaVA/llava/output/prompt-key/Histopathology.pth')['keys']
+# print('keys loaded')
+
+# prompt_key = torch.cat(
+#     (
+#         CT_prompt_key[:, :1, :, :], 
+#         CXR_prompt_key[:, 1:2, :, :], 
+#         Histopathology_prompt_key[:, 2:3, :, :], 
+#     ), 
+#     dim=1
+# )
+
+# print("prompt_key.shape: {}".format(prompt_key.shape))
+
+# torch.save(prompt_key, '/vast/users/xiaodan/haokunlin/Continual_LLaVA/llava/output/prompt-key/merged_prompt_key.pth')
\ No newline at end of file
diff --git a/llava/train/train_prompt_key.py b/llava/train/train_prompt_key.py
index ead0637..205590c 100644
--- a/llava/train/train_prompt_key.py
+++ b/llava/train/train_prompt_key.py
@@ -128,11 +128,11 @@ if __name__  == "__main__":
     parser.add_argument('--lr', default=1e-3, type=float)
     parser.add_argument('--batch_size', default=128, type=int)
     parser.add_argument('--num_workers', default=4, type=int)
-    parser.add_argument('--save_path', default="/vast/users/xiaodan/haokunlin/Continual_LLaVA/llava/output/prompt-key/MRI.pth", type=str)
+    parser.add_argument('--save_path', default="/vast/users/xiaodan/haokunlin/Continual_LLaVA/llava/output/prompt-key/abd.pth", type=str)
     parser.add_argument('--checkpoint', default=None, type=str)
-    parser.add_argument('--task', default='MRI', type=str)
+    parser.add_argument('--task', default='abd', type=str)
     parser.add_argument('--config_path', default=None, type=str)
-    parser.add_argument('--data_paths', default="/vast/users/xiaodan/haokunlin/data/llava_med_for_cv805/upload_hf/training/MRI_data_4416.json", type=str)
+    parser.add_argument('--data_paths', default="/vast/users/xiaodan/haokunlin/data/vqa-rad/train_abd.jsonl", type=str)
     parser.add_argument('--master_port', default="8889", type=str)
     parser.add_argument('--local-rank', default=-1, type=int)
 
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index abc05f9..f9718f6 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20251019_063451-7pc46z7m/logs/debug-internal.log
\ No newline at end of file
+run-20251113_061129-ahu7z62b/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 6d9c27b..e9b56cc 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20251019_063451-7pc46z7m/logs/debug.log
\ No newline at end of file
+run-20251113_061129-ahu7z62b/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 85187b2..43d0e9d 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20251019_063451-7pc46z7m
\ No newline at end of file
+run-20251113_061129-ahu7z62b
\ No newline at end of file
