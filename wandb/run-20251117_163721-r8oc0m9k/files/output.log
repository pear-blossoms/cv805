[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|                                                                                                      | 0/151 [00:00<?, ?it/s]/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
                                                                                                                                   

{'loss': 3.3592, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}

{'loss': 3.0646, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}

{'loss': 3.8454, 'learning_rate': 1.2e-05, 'epoch': 0.02}

{'loss': 3.0428, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.03}

{'loss': 3.2522, 'learning_rate': 2e-05, 'epoch': 0.03}

{'loss': 3.3193, 'learning_rate': 1.9997685019798913e-05, 'epoch': 0.04}

{'loss': 4.2321, 'learning_rate': 1.9990741151022302e-05, 'epoch': 0.05}

{'loss': 2.316, 'learning_rate': 1.9979171608653926e-05, 'epoch': 0.05}

{'loss': 1.7033, 'learning_rate': 1.996298174934608e-05, 'epoch': 0.06}

{'loss': 1.7522, 'learning_rate': 1.994217906893952e-05, 'epoch': 0.07}

{'loss': 1.3741, 'learning_rate': 1.99167731989929e-05, 'epoch': 0.07}

{'loss': 1.3549, 'learning_rate': 1.9886775902323405e-05, 'epoch': 0.08}

{'loss': 1.1403, 'learning_rate': 1.9852201067560607e-05, 'epoch': 0.09}

{'loss': 0.9209, 'learning_rate': 1.9813064702716094e-05, 'epoch': 0.09}

{'loss': 1.2553, 'learning_rate': 1.976938492777182e-05, 'epoch': 0.1}

{'loss': 1.1533, 'learning_rate': 1.9721181966290614e-05, 'epoch': 0.11}

{'loss': 1.1276, 'learning_rate': 1.9668478136052776e-05, 'epoch': 0.11}

{'loss': 0.9766, 'learning_rate': 1.961129783872301e-05, 'epoch': 0.12}

{'loss': 0.6793, 'learning_rate': 1.9549667548552557e-05, 'epoch': 0.13}

{'loss': 0.924, 'learning_rate': 1.9483615800121717e-05, 'epoch': 0.13}

{'loss': 0.8758, 'learning_rate': 1.9413173175128472e-05, 'epoch': 0.14}

{'loss': 1.0795, 'learning_rate': 1.9338372288229253e-05, 'epoch': 0.15}

{'loss': 0.8765, 'learning_rate': 1.92592477719385e-05, 'epoch': 0.15}

{'loss': 1.0489, 'learning_rate': 1.9175836260593937e-05, 'epoch': 0.16}

{'loss': 0.6337, 'learning_rate': 1.908817637339503e-05, 'epoch': 0.17}

{'loss': 0.9015, 'learning_rate': 1.8996308696522435e-05, 'epoch': 0.17}

{'loss': 1.0377, 'learning_rate': 1.890027576434677e-05, 'epoch': 0.18}

{'loss': 0.6741, 'learning_rate': 1.880012203973536e-05, 'epoch': 0.19}

{'loss': 0.6893, 'learning_rate': 1.869589389346611e-05, 'epoch': 0.19}

{'loss': 0.9421, 'learning_rate': 1.8587639582758032e-05, 'epoch': 0.2}

{'loss': 1.0357, 'learning_rate': 1.8475409228928314e-05, 'epoch': 0.21}

{'loss': 0.6854, 'learning_rate': 1.8359254794186368e-05, 'epoch': 0.21}

{'loss': 1.0827, 'learning_rate': 1.8239230057575542e-05, 'epoch': 0.22}

{'loss': 0.5408, 'learning_rate': 1.8115390590073612e-05, 'epoch': 0.23}

{'loss': 0.4602, 'learning_rate': 1.798779372886365e-05, 'epoch': 0.23}

{'loss': 0.646, 'learning_rate': 1.7856498550787144e-05, 'epoch': 0.24}

{'loss': 0.4101, 'learning_rate': 1.7721565844991643e-05, 'epoch': 0.25}

{'loss': 0.6243, 'learning_rate': 1.7583058084785626e-05, 'epoch': 0.25}

{'loss': 0.696, 'learning_rate': 1.744103939871361e-05, 'epoch': 0.26}

{'loss': 0.3334, 'learning_rate': 1.7295575540864878e-05, 'epoch': 0.26}

{'loss': 0.6468, 'learning_rate': 1.7146733860429614e-05, 'epoch': 0.27}

{'loss': 0.473, 'learning_rate': 1.699458327051647e-05, 'epoch': 0.28}

{'loss': 0.5577, 'learning_rate': 1.683919421624611e-05, 'epoch': 0.28}

{'loss': 0.4395, 'learning_rate': 1.6680638642135337e-05, 'epoch': 0.29}

{'loss': 0.4553, 'learning_rate': 1.6518989958787126e-05, 'epoch': 0.3}

{'loss': 0.5959, 'learning_rate': 1.6354323008901774e-05, 'epoch': 0.3}

{'loss': 0.7098, 'learning_rate': 1.6186714032625036e-05, 'epoch': 0.31}

{'loss': 0.5911, 'learning_rate': 1.6016240632249224e-05, 'epoch': 0.32}

{'loss': 0.5235, 'learning_rate': 1.5842981736283686e-05, 'epoch': 0.32}

{'loss': 0.6161, 'learning_rate': 1.566701756291118e-05, 'epoch': 0.33}

{'loss': 0.4061, 'learning_rate': 1.5488429582847194e-05, 'epoch': 0.34}

{'loss': 0.7567, 'learning_rate': 1.5307300481619334e-05, 'epoch': 0.34}

{'loss': 0.6531, 'learning_rate': 1.512371412128424e-05, 'epoch': 0.35}

{'loss': 0.7444, 'learning_rate': 1.4937755501599774e-05, 'epoch': 0.36}

{'loss': 0.4649, 'learning_rate': 1.4749510720670506e-05, 'epoch': 0.36}

{'loss': 0.5694, 'learning_rate': 1.455906693508459e-05, 'epoch': 0.37}

{'loss': 0.7458, 'learning_rate': 1.4366512319560642e-05, 'epoch': 0.38}

{'loss': 0.621, 'learning_rate': 1.417193602612317e-05, 'epoch': 0.38}

{'loss': 0.6339, 'learning_rate': 1.3975428142825562e-05, 'epoch': 0.39}

{'loss': 0.7471, 'learning_rate': 1.3777079652039649e-05, 'epoch': 0.4}

{'loss': 0.5994, 'learning_rate': 1.3576982388331258e-05, 'epoch': 0.4}

{'loss': 0.483, 'learning_rate': 1.3375228995941135e-05, 'epoch': 0.41}

{'loss': 0.7158, 'learning_rate': 1.3171912885891063e-05, 'epoch': 0.42}

{'loss': 0.5059, 'learning_rate': 1.2967128192734903e-05, 'epoch': 0.42}

{'loss': 0.705, 'learning_rate': 1.2760969730974692e-05, 'epoch': 0.43}

{'loss': 0.7024, 'learning_rate': 1.255353295116187e-05, 'epoch': 0.44}

{'loss': 0.3517, 'learning_rate': 1.2344913895704099e-05, 'epoch': 0.44}

{'loss': 0.5585, 'learning_rate': 1.2135209154397962e-05, 'epoch': 0.45}

{'loss': 0.5329, 'learning_rate': 1.19245158197083e-05, 'epoch': 0.46}

{'loss': 0.6556, 'learning_rate': 1.1712931441814776e-05, 'epoch': 0.46}

{'loss': 0.4422, 'learning_rate': 1.1500553983446527e-05, 'epoch': 0.47}

{'loss': 0.698, 'learning_rate': 1.128748177452581e-05, 'epoch': 0.48}

{'loss': 0.6263, 'learning_rate': 1.1073813466641633e-05, 'epoch': 0.48}

{'loss': 0.5792, 'learning_rate': 1.0859647987374467e-05, 'epoch': 0.49}

{'loss': 0.3739, 'learning_rate': 1.0645084494493166e-05, 'epoch': 0.5}

{'loss': 1.4739, 'learning_rate': 1.0430222330045306e-05, 'epoch': 0.5}

{'loss': 0.4058, 'learning_rate': 1.0215160974362224e-05, 'epoch': 0.51}

{'loss': 0.7648, 'learning_rate': 1e-05, 'epoch': 0.52}

{'loss': 0.3392, 'learning_rate': 9.78483902563778e-06, 'epoch': 0.52}

{'loss': 0.5753, 'learning_rate': 9.569777669954694e-06, 'epoch': 0.53}

{'loss': 0.2771, 'learning_rate': 9.354915505506839e-06, 'epoch': 0.54}

{'loss': 0.2812, 'learning_rate': 9.140352012625538e-06, 'epoch': 0.54}

{'loss': 0.5309, 'learning_rate': 8.92618653335837e-06, 'epoch': 0.55}

{'loss': 0.5416, 'learning_rate': 8.712518225474191e-06, 'epoch': 0.56}

{'loss': 0.7149, 'learning_rate': 8.499446016553475e-06, 'epoch': 0.56}

{'loss': 0.5835, 'learning_rate': 8.287068558185225e-06, 'epoch': 0.57}

{'loss': 0.4056, 'learning_rate': 8.075484180291702e-06, 'epoch': 0.58}

{'loss': 0.4847, 'learning_rate': 7.86479084560204e-06, 'epoch': 0.58}

{'loss': 4.3675, 'learning_rate': 7.655086104295904e-06, 'epoch': 0.59}

{'loss': 0.6255, 'learning_rate': 7.446467048838131e-06, 'epoch': 0.6}

{'loss': 0.6167, 'learning_rate': 7.239030269025311e-06, 'epoch': 0.6}

{'loss': 0.3518, 'learning_rate': 7.032871807265097e-06, 'epoch': 0.61}

{'loss': 0.5387, 'learning_rate': 6.8280871141089415e-06, 'epoch': 0.62}

{'loss': 0.9391, 'learning_rate': 6.624771004058869e-06, 'epoch': 0.62}

{'loss': 0.3804, 'learning_rate': 6.423017611668745e-06, 'epoch': 0.63}

{'loss': 0.3101, 'learning_rate': 6.22292034796035e-06, 'epoch': 0.64}

{'loss': 0.2686, 'learning_rate': 6.024571857174443e-06, 'epoch': 0.64}

{'loss': 0.3276, 'learning_rate': 5.828063973876834e-06, 'epoch': 0.65}

{'loss': 0.5329, 'learning_rate': 5.633487680439362e-06, 'epoch': 0.66}

{'loss': 0.5048, 'learning_rate': 5.440933064915414e-06, 'epoch': 0.66}

{'loss': 0.3131, 'learning_rate': 5.250489279329501e-06, 'epoch': 0.67}

{'loss': 0.6166, 'learning_rate': 5.062244498400228e-06, 'epoch': 0.68}

{'loss': 0.3566, 'learning_rate': 4.876285878715764e-06, 'epoch': 0.68}

{'loss': 0.4968, 'learning_rate': 4.692699518380664e-06, 'epoch': 0.69}

{'loss': 0.4593, 'learning_rate': 4.5115704171528105e-06, 'epoch': 0.7}

{'loss': 0.6866, 'learning_rate': 4.332982437088825e-06, 'epoch': 0.7}

{'loss': 0.3811, 'learning_rate': 4.1570182637163155e-06, 'epoch': 0.71}

{'loss': 0.3938, 'learning_rate': 3.983759367750772e-06, 'epoch': 0.72}

{'loss': 0.2509, 'learning_rate': 3.8132859673749688e-06, 'epoch': 0.72}

{'loss': 0.3668, 'learning_rate': 3.645676991098227e-06, 'epoch': 0.73}

{'loss': 0.3544, 'learning_rate': 3.4810100412128743e-06, 'epoch': 0.74}

{'loss': 0.6788, 'learning_rate': 3.3193613578646633e-06, 'epoch': 0.74}

{'loss': 0.5129, 'learning_rate': 3.1608057837538976e-06, 'epoch': 0.75}

{'loss': 0.429, 'learning_rate': 3.0054167294835314e-06, 'epoch': 0.75}

{'loss': 0.5533, 'learning_rate': 2.853266139570391e-06, 'epoch': 0.76}

{'loss': 0.4432, 'learning_rate': 2.704424459135123e-06, 'epoch': 0.77}

{'loss': 0.4189, 'learning_rate': 2.5589606012863968e-06, 'epoch': 0.77}

{'loss': 0.3883, 'learning_rate': 2.416941915214377e-06, 'epoch': 0.78}

{'loss': 0.6127, 'learning_rate': 2.2784341550083577e-06, 'epoch': 0.79}

{'loss': 0.3862, 'learning_rate': 2.1435014492128547e-06, 'epoch': 0.79}

{'loss': 0.3617, 'learning_rate': 2.012206271136353e-06, 'epoch': 0.8}

{'loss': 0.468, 'learning_rate': 1.8846094099263911e-06, 'epoch': 0.81}

{'loss': 0.4133, 'learning_rate': 1.7607699424244583e-06, 'epoch': 0.81}

{'loss': 0.3987, 'learning_rate': 1.6407452058136298e-06, 'epoch': 0.82}

{'loss': 0.4171, 'learning_rate': 1.5245907710716912e-06, 'epoch': 0.83}

{'loss': 0.3917, 'learning_rate': 1.4123604172419714e-06, 'epoch': 0.83}

{'loss': 0.6871, 'learning_rate': 1.30410610653389e-06, 'epoch': 0.84}

{'loss': 0.4922, 'learning_rate': 1.1998779602646438e-06, 'epoch': 0.85}

{'loss': 0.3168, 'learning_rate': 1.0997242356532335e-06, 'epoch': 0.85}

{'loss': 1.8383, 'learning_rate': 1.0036913034775675e-06, 'epoch': 0.86}

{'loss': 0.4292, 'learning_rate': 9.118236266049707e-07, 'epoch': 0.87}

{'loss': 0.6151, 'learning_rate': 8.241637394060619e-07, 'epoch': 0.87}

{'loss': 0.3085, 'learning_rate': 7.40752228061502e-07, 'epoch': 0.88}

{'loss': 0.4849, 'learning_rate': 6.616277117707493e-07, 'epoch': 0.89}

{'loss': 0.3988, 'learning_rate': 5.868268248715292e-07, 'epoch': 0.89}

{'loss': 0.3565, 'learning_rate': 5.163841998782837e-07, 'epoch': 0.9}

{'loss': 0.6429, 'learning_rate': 4.503324514474483e-07, 'epoch': 0.91}

{'loss': 0.441, 'learning_rate': 3.887021612769937e-07, 'epoch': 0.91}

{'loss': 0.3587, 'learning_rate': 3.3152186394722506e-07, 'epoch': 0.92}

{'loss': 0.2748, 'learning_rate': 2.78818033709386e-07, 'epoch': 0.93}

{'loss': 0.3133, 'learning_rate': 2.3061507222818303e-07, 'epoch': 0.93}

{'loss': 0.6144, 'learning_rate': 1.869352972839067e-07, 'epoch': 0.94}

{'loss': 0.4746, 'learning_rate': 1.4779893243939358e-07, 'epoch': 0.95}

{'loss': 0.4967, 'learning_rate': 1.1322409767659526e-07, 'epoch': 0.95}

{'loss': 0.4947, 'learning_rate': 8.322680100710023e-08, 'epoch': 0.96}

{'loss': 0.3493, 'learning_rate': 5.782093106048159e-08, 'epoch': 0.97}

{'loss': 0.5496, 'learning_rate': 3.701825065392184e-08, 'epoch': 0.97}

{'loss': 0.4654, 'learning_rate': 2.082839134607828e-08, 'epoch': 0.98}

{'loss': 0.6219, 'learning_rate': 9.25884897770013e-09, 'epoch': 0.99}

{'loss': 0.6255, 'learning_rate': 2.3149802010913323e-09, 'epoch': 0.99}

{'loss': 0.581, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 287.7647, 'train_samples_per_second': 16.746, 'train_steps_per_second': 0.525, 'train_loss': 0.784579336643219, 'epoch': 1.0}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
