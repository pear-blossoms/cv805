[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|                                                                                      | 0/20 [00:00<?, ?it/s]/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
                                                                                                                  

{'loss': 4.2095, 'learning_rate': 2e-05, 'epoch': 0.05}

{'loss': 3.7666, 'learning_rate': 1.9863613034027224e-05, 'epoch': 0.1}

{'loss': 3.3148, 'learning_rate': 1.9458172417006347e-05, 'epoch': 0.15}

{'loss': 3.0223, 'learning_rate': 1.879473751206489e-05, 'epoch': 0.2}

{'loss': 2.173, 'learning_rate': 1.789140509396394e-05, 'epoch': 0.25}

{'loss': 1.8769, 'learning_rate': 1.6772815716257414e-05, 'epoch': 0.3}

{'loss': 2.0276, 'learning_rate': 1.5469481581224274e-05, 'epoch': 0.35}

{'loss': 1.5161, 'learning_rate': 1.4016954246529697e-05, 'epoch': 0.4}

{'loss': 1.4257, 'learning_rate': 1.2454854871407993e-05, 'epoch': 0.45}

{'loss': 1.658, 'learning_rate': 1.0825793454723325e-05, 'epoch': 0.5}

{'loss': 1.328, 'learning_rate': 9.174206545276678e-06, 'epoch': 0.55}

{'loss': 1.1397, 'learning_rate': 7.545145128592009e-06, 'epoch': 0.6}

{'loss': 1.4369, 'learning_rate': 5.983045753470308e-06, 'epoch': 0.65}

{'loss': 1.2729, 'learning_rate': 4.530518418775734e-06, 'epoch': 0.7}

{'loss': 0.9933, 'learning_rate': 3.2271842837425917e-06, 'epoch': 0.75}

{'loss': 1.4281, 'learning_rate': 2.1085949060360654e-06, 'epoch': 0.8}

{'loss': 1.1074, 'learning_rate': 1.2052624879351105e-06, 'epoch': 0.85}

{'loss': 1.1543, 'learning_rate': 5.418275829936537e-07, 'epoch': 0.9}

{'loss': 1.4593, 'learning_rate': 1.3638696597277678e-07, 'epoch': 0.95}

{'loss': 1.7911, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 43.9116, 'train_samples_per_second': 14.074, 'train_steps_per_second': 0.455, 'train_loss': 1.9050757199525834, 'epoch': 1.0}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
