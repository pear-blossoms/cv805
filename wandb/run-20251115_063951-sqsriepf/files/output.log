[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|                                                                                      | 0/19 [00:00<?, ?it/s]/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
                                                                                                                  

{'loss': 6.2175, 'learning_rate': 2e-05, 'epoch': 0.05}

{'loss': 7.1243, 'learning_rate': 1.9848077530122083e-05, 'epoch': 0.11}

{'loss': 6.5763, 'learning_rate': 1.9396926207859085e-05, 'epoch': 0.16}

{'loss': 5.7549, 'learning_rate': 1.866025403784439e-05, 'epoch': 0.21}

{'loss': 6.3062, 'learning_rate': 1.766044443118978e-05, 'epoch': 0.26}

{'loss': 4.6678, 'learning_rate': 1.6427876096865394e-05, 'epoch': 0.32}

{'loss': 3.9956, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.37}

{'loss': 3.4342, 'learning_rate': 1.342020143325669e-05, 'epoch': 0.42}

{'loss': 3.6304, 'learning_rate': 1.1736481776669307e-05, 'epoch': 0.47}

{'loss': 3.1234, 'learning_rate': 1e-05, 'epoch': 0.53}

{'loss': 2.5201, 'learning_rate': 8.263518223330698e-06, 'epoch': 0.58}

{'loss': 2.9152, 'learning_rate': 6.579798566743314e-06, 'epoch': 0.63}

{'loss': 2.2206, 'learning_rate': 5.000000000000003e-06, 'epoch': 0.68}

{'loss': 2.3529, 'learning_rate': 3.5721239031346067e-06, 'epoch': 0.74}

{'loss': 1.9785, 'learning_rate': 2.339555568810221e-06, 'epoch': 0.79}

{'loss': 1.9248, 'learning_rate': 1.339745962155613e-06, 'epoch': 0.84}

{'loss': 2.0803, 'learning_rate': 6.030737921409169e-07, 'epoch': 0.89}

{'loss': 2.0002, 'learning_rate': 1.519224698779198e-07, 'epoch': 0.95}

{'loss': 2.0624, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 58.0347, 'train_samples_per_second': 10.149, 'train_steps_per_second': 0.327, 'train_loss': 3.7308202793723657, 'epoch': 1.0}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
