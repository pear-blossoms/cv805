[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|                                                                                                                                                   | 0/107 [00:00<?, ?it/s]/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
                                                                                                                                                                                

{'loss': 0.8997, 'learning_rate': 5e-06, 'epoch': 0.01}

{'loss': 0.9979, 'learning_rate': 1e-05, 'epoch': 0.02}

{'loss': 0.8997, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.03}

{'loss': 0.9367, 'learning_rate': 2e-05, 'epoch': 0.04}

{'loss': 0.837, 'learning_rate': 1.9995348836233517e-05, 'epoch': 0.05}

{'loss': 0.958, 'learning_rate': 1.998139967159894e-05, 'epoch': 0.06}

{'loss': 0.9555, 'learning_rate': 1.9958165482066094e-05, 'epoch': 0.07}

{'loss': 0.9164, 'learning_rate': 1.992566788083908e-05, 'epoch': 0.07}

{'loss': 0.8932, 'learning_rate': 1.9883937098250962e-05, 'epoch': 0.08}

{'loss': 1.021, 'learning_rate': 1.9833011953642525e-05, 'epoch': 0.09}

{'loss': 0.9051, 'learning_rate': 1.9772939819251247e-05, 'epoch': 0.1}

{'loss': 0.9219, 'learning_rate': 1.9703776576144106e-05, 'epoch': 0.11}

{'loss': 0.8793, 'learning_rate': 1.962558656223516e-05, 'epoch': 0.12}

{'loss': 0.8425, 'learning_rate': 1.953844251243633e-05, 'epoch': 0.13}

{'loss': 0.904, 'learning_rate': 1.9442425490996987e-05, 'epoch': 0.14}

{'loss': 0.902, 'learning_rate': 1.933762481609536e-05, 'epoch': 0.15}

{'loss': 0.8715, 'learning_rate': 1.9224137976751797e-05, 'epoch': 0.16}

{'loss': 0.9281, 'learning_rate': 1.910207054214133e-05, 'epoch': 0.17}

{'loss': 0.9006, 'learning_rate': 1.8971536063389745e-05, 'epoch': 0.18}

{'loss': 0.8762, 'learning_rate': 1.8832655967944607e-05, 'epoch': 0.19}

{'loss': 0.93, 'learning_rate': 1.868555944661949e-05, 'epoch': 0.2}

{'loss': 0.9032, 'learning_rate': 1.853038333341642e-05, 'epoch': 0.21}

{'loss': 0.8923, 'learning_rate': 1.8367271978238422e-05, 'epoch': 0.21}

{'loss': 0.8729, 'learning_rate': 1.8196377112610524e-05, 'epoch': 0.22}

{'loss': 0.8613, 'learning_rate': 1.8017857708534107e-05, 'epoch': 0.23}

{'loss': 0.9229, 'learning_rate': 1.783187983060594e-05, 'epoch': 0.24}

{'loss': 0.8433, 'learning_rate': 1.763861648153945e-05, 'epoch': 0.25}

{'loss': 0.9462, 'learning_rate': 1.743824744123196e-05, 'epoch': 0.26}

{'loss': 0.8677, 'learning_rate': 1.7230959099527512e-05, 'epoch': 0.27}

{'loss': 0.9714, 'learning_rate': 1.7016944282830935e-05, 'epoch': 0.28}

{'loss': 0.8477, 'learning_rate': 1.6796402074734404e-05, 'epoch': 0.29}

{'loss': 0.8638, 'learning_rate': 1.6569537630823385e-05, 'epoch': 0.3}

{'loss': 0.9285, 'learning_rate': 1.6336561987834155e-05, 'epoch': 0.31}

{'loss': 0.9887, 'learning_rate': 1.6097691867340547e-05, 'epoch': 0.32}

{'loss': 0.8315, 'learning_rate': 1.585314947415242e-05, 'epoch': 0.33}

{'loss': 0.8701, 'learning_rate': 1.5603162289613503e-05, 'epoch': 0.34}

{'loss': 0.8525, 'learning_rate': 1.5347962859990744e-05, 'epoch': 0.35}

{'loss': 0.8638, 'learning_rate': 1.5087788580152207e-05, 'epoch': 0.36}

{'loss': 0.8001, 'learning_rate': 1.4822881472734563e-05, 'epoch': 0.36}

{'loss': 0.9692, 'learning_rate': 1.4553487963005712e-05, 'epoch': 0.37}

{'loss': 0.8611, 'learning_rate': 1.427985864963193e-05, 'epoch': 0.38}

{'loss': 0.8853, 'learning_rate': 1.400224807156278e-05, 'epoch': 0.39}

{'loss': 0.8675, 'learning_rate': 1.3720914471250644e-05, 'epoch': 0.4}

{'loss': 0.832, 'learning_rate': 1.3436119554425133e-05, 'epoch': 0.41}

{'loss': 0.8737, 'learning_rate': 1.314812824664585e-05, 'epoch': 0.42}

{'loss': 0.9138, 'learning_rate': 1.285720844685996e-05, 'epoch': 0.43}

{'loss': 0.8794, 'learning_rate': 1.2563630778193805e-05, 'epoch': 0.44}

{'loss': 0.9247, 'learning_rate': 1.2267668336210411e-05, 'epoch': 0.45}

{'loss': 0.9092, 'learning_rate': 1.1969596434867063e-05, 'epoch': 0.46}

{'loss': 0.9187, 'learning_rate': 1.1669692350409223e-05, 'epoch': 0.47}

{'loss': 0.8989, 'learning_rate': 1.1368235063439103e-05, 'epoch': 0.48}

{'loss': 0.9652, 'learning_rate': 1.1065504999398762e-05, 'epoch': 0.49}

{'loss': 0.8444, 'learning_rate': 1.0761783767709182e-05, 'epoch': 0.5}

{'loss': 0.9279, 'learning_rate': 1.0457353899807947e-05, 'epoch': 0.5}

{'loss': 0.8389, 'learning_rate': 1.015249858632926e-05, 'epoch': 0.51}

{'loss': 0.8383, 'learning_rate': 9.847501413670742e-06, 'epoch': 0.52}

{'loss': 0.8919, 'learning_rate': 9.542646100192056e-06, 'epoch': 0.53}

{'loss': 0.8526, 'learning_rate': 9.238216232290821e-06, 'epoch': 0.54}

{'loss': 0.9179, 'learning_rate': 8.934495000601241e-06, 'epoch': 0.55}

{'loss': 0.9244, 'learning_rate': 8.6317649365609e-06, 'epoch': 0.56}

{'loss': 0.8653, 'learning_rate': 8.330307649590782e-06, 'epoch': 0.57}

{'loss': 0.8583, 'learning_rate': 8.030403565132942e-06, 'epoch': 0.58}

{'loss': 0.9029, 'learning_rate': 7.732331663789592e-06, 'epoch': 0.59}

{'loss': 0.8953, 'learning_rate': 7.436369221806201e-06, 'epoch': 0.6}

{'loss': 0.8855, 'learning_rate': 7.142791553140045e-06, 'epoch': 0.61}

{'loss': 0.9252, 'learning_rate': 6.851871753354154e-06, 'epoch': 0.62}

{'loss': 0.8954, 'learning_rate': 6.563880445574873e-06, 'epoch': 0.63}

{'loss': 0.7855, 'learning_rate': 6.2790855287493605e-06, 'epoch': 0.64}

{'loss': 0.9022, 'learning_rate': 5.99775192843722e-06, 'epoch': 0.64}

{'loss': 0.8818, 'learning_rate': 5.720141350368072e-06, 'epoch': 0.65}

{'loss': 0.881, 'learning_rate': 5.446512036994287e-06, 'epoch': 0.66}

{'loss': 0.8373, 'learning_rate': 5.177118527265438e-06, 'epoch': 0.67}

{'loss': 0.8018, 'learning_rate': 4.912211419847795e-06, 'epoch': 0.68}

{'loss': 0.9043, 'learning_rate': 4.652037140009259e-06, 'epoch': 0.69}

{'loss': 1.0244, 'learning_rate': 4.396837710386503e-06, 'epoch': 0.7}

{'loss': 0.9674, 'learning_rate': 4.1468505258475785e-06, 'epoch': 0.71}

{'loss': 0.9433, 'learning_rate': 3.902308132659457e-06, 'epoch': 0.72}

{'loss': 0.9072, 'learning_rate': 3.6634380121658484e-06, 'epoch': 0.73}

{'loss': 0.8847, 'learning_rate': 3.4304623691766193e-06, 'epoch': 0.74}

{'loss': 0.8755, 'learning_rate': 3.203597925265598e-06, 'epoch': 0.75}

{'loss': 0.8681, 'learning_rate': 2.98305571716907e-06, 'epoch': 0.76}

{'loss': 0.8955, 'learning_rate': 2.7690409004724883e-06, 'epoch': 0.77}

{'loss': 0.8048, 'learning_rate': 2.56175255876804e-06, 'epoch': 0.78}

{'loss': 0.8535, 'learning_rate': 2.3613835184605527e-06, 'epoch': 0.79}

{'loss': 0.8292, 'learning_rate': 2.1681201693940667e-06, 'epoch': 0.79}

{'loss': 0.9094, 'learning_rate': 1.982142291465896e-06, 'epoch': 0.8}

{'loss': 0.8732, 'learning_rate': 1.8036228873894745e-06, 'epoch': 0.81}

{'loss': 0.8934, 'learning_rate': 1.6327280217615793e-06, 'epoch': 0.82}

{'loss': 0.8412, 'learning_rate': 1.4696166665835853e-06, 'epoch': 0.83}

{'loss': 0.861, 'learning_rate': 1.3144405533805138e-06, 'epoch': 0.84}

{'loss': 0.9601, 'learning_rate': 1.1673440320553941e-06, 'epoch': 0.85}

{'loss': 0.9161, 'learning_rate': 1.02846393661026e-06, 'epoch': 0.86}

{'loss': 0.8983, 'learning_rate': 8.979294578586739e-07, 'epoch': 0.87}

{'loss': 0.9353, 'learning_rate': 7.758620232482083e-07, 'epoch': 0.88}

{'loss': 0.8889, 'learning_rate': 6.623751839046455e-07, 'epoch': 0.89}

{'loss': 0.831, 'learning_rate': 5.575745090030138e-07, 'epoch': 0.9}

{'loss': 0.8135, 'learning_rate': 4.61557487563673e-07, 'epoch': 0.91}

{'loss': 0.9181, 'learning_rate': 3.7441343776484116e-07, 'epoch': 0.92}

{'loss': 0.8437, 'learning_rate': 2.9622342385589256e-07, 'epoch': 0.93}

{'loss': 0.9367, 'learning_rate': 2.2706018074875046e-07, 'epoch': 0.93}

{'loss': 0.8554, 'learning_rate': 1.669880463574758e-07, 'epoch': 0.94}

{'loss': 0.916, 'learning_rate': 1.160629017490389e-07, 'epoch': 0.95}

{'loss': 0.8799, 'learning_rate': 7.433211916092143e-08, 'epoch': 0.96}

{'loss': 0.8825, 'learning_rate': 4.183451793390747e-08, 'epoch': 0.97}

{'loss': 0.879, 'learning_rate': 1.860032840106163e-08, 'epoch': 0.98}

{'loss': 0.8609, 'learning_rate': 4.651163766484779e-09, 'epoch': 0.99}

{'loss': 0.8883, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 273.2128, 'train_samples_per_second': 12.459, 'train_steps_per_second': 0.392, 'train_loss': 0.8915524805817648, 'epoch': 1.0}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
