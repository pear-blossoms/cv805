[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|                                                                                                                                                 | 0/128 [00:00<?, ?it/s]/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
                                                                                                                                                                              

{'loss': 0.9504, 'learning_rate': 5e-06, 'epoch': 0.01}

{'loss': 0.901, 'learning_rate': 1e-05, 'epoch': 0.02}

{'loss': 0.8894, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.02}

{'loss': 0.8939, 'learning_rate': 2e-05, 'epoch': 0.03}

{'loss': 0.9298, 'learning_rate': 1.9996790752964305e-05, 'epoch': 0.04}

{'loss': 0.8245, 'learning_rate': 1.998716507171053e-05, 'epoch': 0.05}

{'loss': 0.8807, 'learning_rate': 1.9971129134476474e-05, 'epoch': 0.05}

{'loss': 0.8107, 'learning_rate': 1.994869323391895e-05, 'epoch': 0.06}

{'loss': 0.8134, 'learning_rate': 1.991987177050743e-05, 'epoch': 0.07}

{'loss': 0.8272, 'learning_rate': 1.9884683243281117e-05, 'epoch': 0.08}

{'loss': 0.7222, 'learning_rate': 1.9843150237975343e-05, 'epoch': 0.09}

{'loss': 0.7668, 'learning_rate': 1.9795299412524948e-05, 'epoch': 0.09}

{'loss': 0.747, 'learning_rate': 1.9741161479953872e-05, 'epoch': 0.1}

{'loss': 0.8075, 'learning_rate': 1.9680771188662044e-05, 'epoch': 0.11}

{'loss': 0.7159, 'learning_rate': 1.9614167300122126e-05, 'epoch': 0.12}

{'loss': 0.7014, 'learning_rate': 1.954139256400049e-05, 'epoch': 0.12}

{'loss': 0.7482, 'learning_rate': 1.9462493690718373e-05, 'epoch': 0.13}

{'loss': 0.7391, 'learning_rate': 1.9377521321470806e-05, 'epoch': 0.14}

{'loss': 0.702, 'learning_rate': 1.9286529995722624e-05, 'epoch': 0.15}

{'loss': 0.7193, 'learning_rate': 1.918957811620231e-05, 'epoch': 0.16}

{'loss': 0.7501, 'learning_rate': 1.908672791141625e-05, 'epoch': 0.16}

{'loss': 0.7829, 'learning_rate': 1.897804539570742e-05, 'epoch': 0.17}

{'loss': 0.7634, 'learning_rate': 1.8863600326884085e-05, 'epoch': 0.18}

{'loss': 0.6298, 'learning_rate': 1.8743466161445823e-05, 'epoch': 0.19}

{'loss': 0.7202, 'learning_rate': 1.8617720007435497e-05, 'epoch': 0.2}

{'loss': 0.746, 'learning_rate': 1.848644257494751e-05, 'epoch': 0.2}

{'loss': 0.7194, 'learning_rate': 1.8349718124324075e-05, 'epoch': 0.21}

{'loss': 0.6925, 'learning_rate': 1.8207634412072765e-05, 'epoch': 0.22}

{'loss': 0.6784, 'learning_rate': 1.8060282634540053e-05, 'epoch': 0.23}

{'loss': 0.7024, 'learning_rate': 1.7907757369376984e-05, 'epoch': 0.23}

{'loss': 0.7288, 'learning_rate': 1.775015651483459e-05, 'epoch': 0.24}

{'loss': 0.7873, 'learning_rate': 1.758758122692791e-05, 'epoch': 0.25}

{'loss': 0.751, 'learning_rate': 1.742013585450911e-05, 'epoch': 0.26}

{'loss': 0.7204, 'learning_rate': 1.72479278722912e-05, 'epoch': 0.27}

{'loss': 0.6662, 'learning_rate': 1.7071067811865477e-05, 'epoch': 0.27}

{'loss': 0.7034, 'learning_rate': 1.688966919075687e-05, 'epoch': 0.28}

{'loss': 0.6651, 'learning_rate': 1.6703848439562787e-05, 'epoch': 0.29}

{'loss': 0.6978, 'learning_rate': 1.6513724827222225e-05, 'epoch': 0.3}

{'loss': 0.7615, 'learning_rate': 1.631942038446304e-05, 'epoch': 0.3}

{'loss': 0.7033, 'learning_rate': 1.612105982547663e-05, 'epoch': 0.31}

{'loss': 0.7304, 'learning_rate': 1.5918770467870174e-05, 'epoch': 0.32}

{'loss': 0.7498, 'learning_rate': 1.5712682150947926e-05, 'epoch': 0.33}

{'loss': 0.6561, 'learning_rate': 1.5502927152373913e-05, 'epoch': 0.34}

{'loss': 0.784, 'learning_rate': 1.5289640103269626e-05, 'epoch': 0.34}

{'loss': 0.6864, 'learning_rate': 1.5072957901801075e-05, 'epoch': 0.35}

{'loss': 1.1751, 'learning_rate': 1.4853019625310813e-05, 'epoch': 0.36}

{'loss': 0.6872, 'learning_rate': 1.4629966441051208e-05, 'epoch': 0.37}

{'loss': 0.7219, 'learning_rate': 1.4403941515576344e-05, 'epoch': 0.38}

{'loss': 0.6418, 'learning_rate': 1.4175089922850633e-05, 'epoch': 0.38}

{'loss': 0.6416, 'learning_rate': 1.3943558551133186e-05, 'epoch': 0.39}

{'loss': 0.6888, 'learning_rate': 1.370949600869768e-05, 'epoch': 0.4}

{'loss': 0.6134, 'learning_rate': 1.3473052528448203e-05, 'epoch': 0.41}

{'loss': 0.7243, 'learning_rate': 1.3234379871492381e-05, 'epoch': 0.41}

{'loss': 0.6634, 'learning_rate': 1.2993631229733584e-05, 'epoch': 0.42}

{'loss': 0.7114, 'learning_rate': 1.2750961127544782e-05, 'epoch': 0.43}

{'loss': 0.6871, 'learning_rate': 1.2506525322587207e-05, 'epoch': 0.44}

{'loss': 0.7039, 'learning_rate': 1.226048070583735e-05, 'epoch': 0.45}

{'loss': 0.6425, 'learning_rate': 1.2012985200886602e-05, 'epoch': 0.45}

{'loss': 0.686, 'learning_rate': 1.1764197662578087e-05, 'epoch': 0.46}

{'loss': 0.6533, 'learning_rate': 1.1514277775045768e-05, 'epoch': 0.47}

{'loss': 0.697, 'learning_rate': 1.1263385949221294e-05, 'epoch': 0.48}

{'loss': 0.6421, 'learning_rate': 1.1011683219874324e-05, 'epoch': 0.48}

{'loss': 0.7011, 'learning_rate': 1.0759331142252463e-05, 'epoch': 0.49}

{'loss': 0.6633, 'learning_rate': 1.0506491688387128e-05, 'epoch': 0.5}

{'loss': 0.6852, 'learning_rate': 1.025332714313188e-05, 'epoch': 0.51}

{'loss': 0.6724, 'learning_rate': 1e-05, 'epoch': 0.52}

{'loss': 0.7049, 'learning_rate': 9.746672856868124e-06, 'epoch': 0.52}

{'loss': 0.7206, 'learning_rate': 9.493508311612874e-06, 'epoch': 0.53}

{'loss': 0.6615, 'learning_rate': 9.24066885774754e-06, 'epoch': 0.54}

{'loss': 0.6883, 'learning_rate': 8.98831678012568e-06, 'epoch': 0.55}

{'loss': 0.7298, 'learning_rate': 8.73661405077871e-06, 'epoch': 0.55}

{'loss': 0.6583, 'learning_rate': 8.485722224954237e-06, 'epoch': 0.56}

{'loss': 0.6369, 'learning_rate': 8.23580233742192e-06, 'epoch': 0.57}

{'loss': 0.6768, 'learning_rate': 7.987014799113398e-06, 'epoch': 0.58}

{'loss': 0.6625, 'learning_rate': 7.739519294162652e-06, 'epoch': 0.59}

{'loss': 0.7019, 'learning_rate': 7.493474677412795e-06, 'epoch': 0.59}

{'loss': 0.6979, 'learning_rate': 7.24903887245522e-06, 'epoch': 0.6}

{'loss': 0.6874, 'learning_rate': 7.006368770266421e-06, 'epoch': 0.61}

{'loss': 0.6211, 'learning_rate': 6.7656201285076195e-06, 'epoch': 0.62}

{'loss': 0.7969, 'learning_rate': 6.526947471551799e-06, 'epoch': 0.62}

{'loss': 0.7538, 'learning_rate': 6.290503991302324e-06, 'epoch': 0.63}

{'loss': 0.7017, 'learning_rate': 6.056441448866817e-06, 'epoch': 0.64}

{'loss': 0.7121, 'learning_rate': 5.824910077149372e-06, 'epoch': 0.65}

{'loss': 0.6701, 'learning_rate': 5.5960584844236565e-06, 'epoch': 0.66}

{'loss': 0.6833, 'learning_rate': 5.370033558948793e-06, 'epoch': 0.66}

{'loss': 0.6375, 'learning_rate': 5.146980374689192e-06, 'epoch': 0.67}

{'loss': 0.6849, 'learning_rate': 4.9270420981989295e-06, 'epoch': 0.68}

{'loss': 0.7771, 'learning_rate': 4.710359896730379e-06, 'epoch': 0.69}

{'loss': 0.6203, 'learning_rate': 4.497072847626087e-06, 'epoch': 0.7}

{'loss': 0.7052, 'learning_rate': 4.287317849052075e-06, 'epoch': 0.7}

{'loss': 0.6392, 'learning_rate': 4.081229532129826e-06, 'epoch': 0.71}

{'loss': 0.6809, 'learning_rate': 3.878940174523371e-06, 'epoch': 0.72}

{'loss': 0.6437, 'learning_rate': 3.680579615536961e-06, 'epoch': 0.73}

{'loss': 0.7013, 'learning_rate': 3.48627517277778e-06, 'epoch': 0.73}

{'loss': 0.6797, 'learning_rate': 3.296151560437214e-06, 'epoch': 0.74}

{'loss': 0.689, 'learning_rate': 3.110330809243134e-06, 'epoch': 0.75}

{'loss': 0.6507, 'learning_rate': 2.9289321881345257e-06, 'epoch': 0.76}

{'loss': 0.6748, 'learning_rate': 2.7520721277088023e-06, 'epoch': 0.77}

{'loss': 0.6576, 'learning_rate': 2.5798641454908945e-06, 'epoch': 0.77}

{'loss': 0.6297, 'learning_rate': 2.4124187730720916e-06, 'epoch': 0.78}

{'loss': 0.6978, 'learning_rate': 2.2498434851654125e-06, 'epoch': 0.79}

{'loss': 0.7022, 'learning_rate': 2.092242630623016e-06, 'epoch': 0.8}

{'loss': 0.6608, 'learning_rate': 1.939717365459952e-06, 'epoch': 0.8}

{'loss': 0.696, 'learning_rate': 1.7923655879272395e-06, 'epoch': 0.81}

{'loss': 0.6893, 'learning_rate': 1.6502818756759275e-06, 'epoch': 0.82}

{'loss': 0.6774, 'learning_rate': 1.5135574250524898e-06, 'epoch': 0.83}

{'loss': 0.6792, 'learning_rate': 1.3822799925645036e-06, 'epoch': 0.84}

{'loss': 0.5731, 'learning_rate': 1.2565338385541792e-06, 'epoch': 0.84}

{'loss': 0.6196, 'learning_rate': 1.1363996731159188e-06, 'epoch': 0.85}

{'loss': 0.6084, 'learning_rate': 1.0219546042925842e-06, 'epoch': 0.86}

{'loss': 0.6653, 'learning_rate': 9.132720885837509e-07, 'epoch': 0.87}

{'loss': 0.7012, 'learning_rate': 8.10421883797694e-07, 'epoch': 0.88}

{'loss': 0.6656, 'learning_rate': 7.13470004277379e-07, 'epoch': 0.88}

{'loss': 0.6339, 'learning_rate': 6.22478678529197e-07, 'epoch': 0.89}

{'loss': 0.7229, 'learning_rate': 5.375063092816313e-07, 'epoch': 0.9}

{'loss': 0.6742, 'learning_rate': 4.5860743599951186e-07, 'epoch': 0.91}

{'loss': 0.6352, 'learning_rate': 3.8583269987787607e-07, 'epoch': 0.91}

{'loss': 0.6567, 'learning_rate': 3.1922881133795827e-07, 'epoch': 0.92}

{'loss': 0.5834, 'learning_rate': 2.588385200461307e-07, 'epoch': 0.93}

{'loss': 0.6408, 'learning_rate': 2.0470058747505516e-07, 'epoch': 0.94}

{'loss': 0.6006, 'learning_rate': 1.5684976202465786e-07, 'epoch': 0.95}

{'loss': 0.6926, 'learning_rate': 1.1531675671888621e-07, 'epoch': 0.95}

{'loss': 0.7173, 'learning_rate': 8.012822949256981e-08, 'epoch': 0.96}

{'loss': 0.6589, 'learning_rate': 5.1306766081048456e-08, 'epoch': 0.97}

{'loss': 0.5675, 'learning_rate': 2.8870865523525916e-08, 'epoch': 0.98}

{'loss': 0.6608, 'learning_rate': 1.2834928289472415e-08, 'epoch': 0.98}

{'loss': 0.6623, 'learning_rate': 3.209247035694807e-09, 'epoch': 0.99}

{'loss': 0.7287, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 321.6441, 'train_samples_per_second': 12.71, 'train_steps_per_second': 0.398, 'train_loss': 0.706861827056855, 'epoch': 1.0}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
