[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|                                                                                                                                                 | 0/138 [00:00<?, ?it/s]/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
                                                                                                                                                                              

{'loss': 0.9432, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}

{'loss': 0.9333, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}

{'loss': 0.8814, 'learning_rate': 1.2e-05, 'epoch': 0.02}

{'loss': 0.929, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.03}

{'loss': 0.874, 'learning_rate': 2e-05, 'epoch': 0.04}

{'loss': 0.9142, 'learning_rate': 1.9997210372120276e-05, 'epoch': 0.04}

{'loss': 0.9812, 'learning_rate': 1.998884304488584e-05, 'epoch': 0.05}

{'loss': 0.8875, 'learning_rate': 1.997490268664256e-05, 'epoch': 0.06}

{'loss': 0.8559, 'learning_rate': 1.995539707507284e-05, 'epoch': 0.07}

{'loss': 0.9315, 'learning_rate': 1.9930337092856243e-05, 'epoch': 0.07}

{'loss': 0.8911, 'learning_rate': 1.9899736721597787e-05, 'epoch': 0.08}

{'loss': 0.8813, 'learning_rate': 1.9863613034027224e-05, 'epoch': 0.09}

{'loss': 0.9663, 'learning_rate': 1.9821986184473757e-05, 'epoch': 0.09}

{'loss': 1.0286, 'learning_rate': 1.9774879397621387e-05, 'epoch': 0.1}

{'loss': 0.8952, 'learning_rate': 1.9722318955551307e-05, 'epoch': 0.11}

{'loss': 0.8889, 'learning_rate': 1.966433418307843e-05, 'epoch': 0.12}

{'loss': 0.9271, 'learning_rate': 1.960095743139033e-05, 'epoch': 0.12}

{'loss': 0.9424, 'learning_rate': 1.9532224059997693e-05, 'epoch': 0.13}

{'loss': 0.9267, 'learning_rate': 1.9458172417006347e-05, 'epoch': 0.14}

{'loss': 0.8504, 'learning_rate': 1.9378843817721856e-05, 'epoch': 0.14}

{'loss': 0.9294, 'learning_rate': 1.929428252159866e-05, 'epoch': 0.15}

{'loss': 0.9555, 'learning_rate': 1.9204535707546602e-05, 'epoch': 0.16}

{'loss': 0.9022, 'learning_rate': 1.9109653447608607e-05, 'epoch': 0.17}

{'loss': 0.8953, 'learning_rate': 1.900968867902419e-05, 'epoch': 0.17}

{'loss': 0.957, 'learning_rate': 1.8904697174694447e-05, 'epoch': 0.18}

{'loss': 0.8959, 'learning_rate': 1.879473751206489e-05, 'epoch': 0.19}

{'loss': 0.9751, 'learning_rate': 1.8679871040443632e-05, 'epoch': 0.2}

{'loss': 0.93, 'learning_rate': 1.8560161846773002e-05, 'epoch': 0.2}

{'loss': 0.9964, 'learning_rate': 1.8435676719873828e-05, 'epoch': 0.21}

{'loss': 0.9159, 'learning_rate': 1.830648511318223e-05, 'epoch': 0.22}

{'loss': 0.8838, 'learning_rate': 1.817265910599978e-05, 'epoch': 0.22}

{'loss': 0.9472, 'learning_rate': 1.8034273363278615e-05, 'epoch': 0.23}

{'loss': 0.7777, 'learning_rate': 1.789140509396394e-05, 'epoch': 0.24}

{'loss': 0.8572, 'learning_rate': 1.7744134007917195e-05, 'epoch': 0.25}

{'loss': 0.971, 'learning_rate': 1.7592542271443888e-05, 'epoch': 0.25}

{'loss': 0.9752, 'learning_rate': 1.74367144614509e-05, 'epoch': 0.26}

{'loss': 0.9145, 'learning_rate': 1.7276737518258865e-05, 'epoch': 0.27}

{'loss': 0.8499, 'learning_rate': 1.7112700697095955e-05, 'epoch': 0.28}

{'loss': 0.9184, 'learning_rate': 1.6944695518300087e-05, 'epoch': 0.28}

{'loss': 0.899, 'learning_rate': 1.6772815716257414e-05, 'epoch': 0.29}

{'loss': 0.9338, 'learning_rate': 1.6597157187105475e-05, 'epoch': 0.3}

{'loss': 0.8974, 'learning_rate': 1.6417817935230318e-05, 'epoch': 0.3}

{'loss': 0.8901, 'learning_rate': 1.6234898018587336e-05, 'epoch': 0.31}

{'loss': 0.85, 'learning_rate': 1.6048499492876378e-05, 'epoch': 0.32}

{'loss': 0.969, 'learning_rate': 1.5858726354602248e-05, 'epoch': 0.33}

{'loss': 0.8577, 'learning_rate': 1.5665684483052425e-05, 'epoch': 0.33}

{'loss': 0.944, 'learning_rate': 1.5469481581224274e-05, 'epoch': 0.34}

{'loss': 0.8928, 'learning_rate': 1.527022711573479e-05, 'epoch': 0.35}

{'loss': 0.8968, 'learning_rate': 1.50680322557464e-05, 'epoch': 0.36}

{'loss': 0.9065, 'learning_rate': 1.4863009810942814e-05, 'epoch': 0.36}

{'loss': 0.8421, 'learning_rate': 1.4655274168589635e-05, 'epoch': 0.37}

{'loss': 0.96, 'learning_rate': 1.444494122971476e-05, 'epoch': 0.38}

{'loss': 1.0209, 'learning_rate': 1.4232128344444251e-05, 'epoch': 0.38}

{'loss': 0.8705, 'learning_rate': 1.4016954246529697e-05, 'epoch': 0.39}

{'loss': 0.8604, 'learning_rate': 1.37995389871036e-05, 'epoch': 0.4}

{'loss': 0.8612, 'learning_rate': 1.3580003867699801e-05, 'epoch': 0.41}

{'loss': 0.9329, 'learning_rate': 1.3358471372576229e-05, 'epoch': 0.41}

{'loss': 0.8665, 'learning_rate': 1.3135065100377816e-05, 'epoch': 0.42}

{'loss': 0.8834, 'learning_rate': 1.2909909695177647e-05, 'epoch': 0.43}

{'loss': 0.893, 'learning_rate': 1.268313077693485e-05, 'epoch': 0.43}

{'loss': 0.8563, 'learning_rate': 1.2454854871407993e-05, 'epoch': 0.44}

{'loss': 0.9408, 'learning_rate': 1.2225209339563144e-05, 'epoch': 0.45}

{'loss': 0.9443, 'learning_rate': 1.1994322306515926e-05, 'epoch': 0.46}

{'loss': 0.9538, 'learning_rate': 1.176232259004722e-05, 'epoch': 0.46}

{'loss': 0.9234, 'learning_rate': 1.1529339628732462e-05, 'epoch': 0.47}

{'loss': 0.9123, 'learning_rate': 1.1295503409724526e-05, 'epoch': 0.48}

{'loss': 0.9256, 'learning_rate': 1.1060944396230583e-05, 'epoch': 0.49}

{'loss': 0.9506, 'learning_rate': 1.0825793454723325e-05, 'epoch': 0.49}

{'loss': 0.9295, 'learning_rate': 1.0590181781927229e-05, 'epoch': 0.5}

{'loss': 0.9191, 'learning_rate': 1.0354240831620542e-05, 'epoch': 0.51}

{'loss': 0.8717, 'learning_rate': 1.0118102241293848e-05, 'epoch': 0.51}

{'loss': 0.8741, 'learning_rate': 9.881897758706155e-06, 'epoch': 0.52}

{'loss': 0.8654, 'learning_rate': 9.645759168379463e-06, 'epoch': 0.53}

{'loss': 0.9259, 'learning_rate': 9.409818218072774e-06, 'epoch': 0.54}

{'loss': 0.9446, 'learning_rate': 9.174206545276678e-06, 'epoch': 0.54}

{'loss': 0.8791, 'learning_rate': 8.93905560376942e-06, 'epoch': 0.55}

{'loss': 0.8913, 'learning_rate': 8.704496590275479e-06, 'epoch': 0.56}

{'loss': 0.944, 'learning_rate': 8.47066037126754e-06, 'epoch': 0.57}

{'loss': 0.858, 'learning_rate': 8.237677409952784e-06, 'epoch': 0.57}

{'loss': 0.8355, 'learning_rate': 8.005677693484077e-06, 'epoch': 0.58}

{'loss': 0.9328, 'learning_rate': 7.774790660436857e-06, 'epoch': 0.59}

{'loss': 0.9314, 'learning_rate': 7.545145128592009e-06, 'epoch': 0.59}

{'loss': 0.8625, 'learning_rate': 7.316869223065156e-06, 'epoch': 0.6}

{'loss': 0.8863, 'learning_rate': 7.090090304822356e-06, 'epoch': 0.61}

{'loss': 0.9349, 'learning_rate': 6.864934899622191e-06, 'epoch': 0.62}

{'loss': 0.9359, 'learning_rate': 6.6415286274237744e-06, 'epoch': 0.62}

{'loss': 0.8917, 'learning_rate': 6.419996132300203e-06, 'epoch': 0.63}

{'loss': 0.9082, 'learning_rate': 6.200461012896401e-06, 'epoch': 0.64}

{'loss': 0.9564, 'learning_rate': 5.983045753470308e-06, 'epoch': 0.64}

{'loss': 0.8767, 'learning_rate': 5.7678716555557515e-06, 'epoch': 0.65}

{'loss': 0.9372, 'learning_rate': 5.5550587702852465e-06, 'epoch': 0.66}

{'loss': 0.8483, 'learning_rate': 5.344725831410369e-06, 'epoch': 0.67}

{'loss': 0.8658, 'learning_rate': 5.136990189057187e-06, 'epoch': 0.67}

{'loss': 0.9425, 'learning_rate': 4.931967744253601e-06, 'epoch': 0.68}

{'loss': 0.8533, 'learning_rate': 4.729772884265212e-06, 'epoch': 0.69}

{'loss': 0.8888, 'learning_rate': 4.530518418775734e-06, 'epoch': 0.7}

{'loss': 0.9141, 'learning_rate': 4.33431551694758e-06, 'epoch': 0.7}

{'loss': 0.8448, 'learning_rate': 4.1412736453977545e-06, 'epoch': 0.71}

{'loss': 0.91, 'learning_rate': 3.9515005071236274e-06, 'epoch': 0.72}

{'loss': 0.9261, 'learning_rate': 3.7651019814126656e-06, 'epoch': 0.72}

{'loss': 0.8033, 'learning_rate': 3.582182064769687e-06, 'epoch': 0.73}

{'loss': 0.8526, 'learning_rate': 3.402842812894529e-06, 'epoch': 0.74}

{'loss': 0.8498, 'learning_rate': 3.2271842837425917e-06, 'epoch': 0.75}

{'loss': 0.8719, 'learning_rate': 3.0553044816999133e-06, 'epoch': 0.75}

{'loss': 0.8575, 'learning_rate': 2.8872993029040506e-06, 'epoch': 0.76}

{'loss': 0.8741, 'learning_rate': 2.723262481741138e-06, 'epoch': 0.77}

{'loss': 0.9175, 'learning_rate': 2.563285538549104e-06, 'epoch': 0.78}

{'loss': 0.8617, 'learning_rate': 2.407457728556115e-06, 'epoch': 0.78}

{'loss': 0.9898, 'learning_rate': 2.2558659920828095e-06, 'epoch': 0.79}

{'loss': 0.8793, 'learning_rate': 2.1085949060360654e-06, 'epoch': 0.8}

{'loss': 0.9337, 'learning_rate': 1.96572663672139e-06, 'epoch': 0.8}

{'loss': 0.8916, 'learning_rate': 1.8273408940002202e-06, 'epoch': 0.81}

{'loss': 0.8578, 'learning_rate': 1.693514886817772e-06, 'epoch': 0.82}

{'loss': 0.7691, 'learning_rate': 1.5643232801261731e-06, 'epoch': 0.83}

{'loss': 0.8684, 'learning_rate': 1.4398381532270001e-06, 'epoch': 0.83}

{'loss': 0.9191, 'learning_rate': 1.3201289595563693e-06, 'epoch': 0.84}

{'loss': 0.8547, 'learning_rate': 1.2052624879351105e-06, 'epoch': 0.85}

{'loss': 0.9574, 'learning_rate': 1.0953028253055541e-06, 'epoch': 0.86}

{'loss': 0.9026, 'learning_rate': 9.903113209758098e-07, 'epoch': 0.86}

{'loss': 1.2167, 'learning_rate': 8.903465523913957e-07, 'epoch': 0.87}

{'loss': 0.9032, 'learning_rate': 7.954642924533995e-07, 'epoch': 0.88}

{'loss': 0.794, 'learning_rate': 7.057174784013432e-07, 'epoch': 0.88}

{'loss': 0.8833, 'learning_rate': 6.211561822781476e-07, 'epoch': 0.89}

{'loss': 0.9077, 'learning_rate': 5.418275829936537e-07, 'epoch': 0.9}

{'loss': 0.8111, 'learning_rate': 4.6777594000230855e-07, 'epoch': 0.91}

{'loss': 0.8381, 'learning_rate': 3.9904256860967436e-07, 'epoch': 0.91}

{'loss': 0.88, 'learning_rate': 3.356658169215743e-07, 'epoch': 0.92}

{'loss': 0.9086, 'learning_rate': 2.776810444486944e-07, 'epoch': 0.93}

{'loss': 0.8354, 'learning_rate': 2.2512060237861455e-07, 'epoch': 0.93}

{'loss': 0.8614, 'learning_rate': 1.7801381552624565e-07, 'epoch': 0.94}

{'loss': 0.9486, 'learning_rate': 1.3638696597277678e-07, 'epoch': 0.95}

{'loss': 0.816, 'learning_rate': 1.0026327840221728e-07, 'epoch': 0.96}

{'loss': 0.9163, 'learning_rate': 6.966290714375934e-08, 'epoch': 0.96}

{'loss': 0.9046, 'learning_rate': 4.460292492716512e-08, 'epoch': 0.97}

{'loss': 0.8589, 'learning_rate': 2.509731335744281e-08, 'epoch': 0.98}

{'loss': 0.8662, 'learning_rate': 1.1156955114162149e-08, 'epoch': 0.99}

{'loss': 0.9136, 'learning_rate': 2.7896278797256983e-09, 'epoch': 0.99}

{'loss': 0.8567, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 359.6182, 'train_samples_per_second': 12.28, 'train_steps_per_second': 0.384, 'train_loss': 0.9023913209852965, 'epoch': 1.0}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
