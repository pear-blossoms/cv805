[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|                                                                                                       | 0/69 [00:00<?, ?it/s]/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
                                                                                                                                   

{'loss': 1.3238, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.01}

{'loss': 1.5624, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.03}

{'loss': 1.4559, 'learning_rate': 2e-05, 'epoch': 0.04}

{'loss': 1.1596, 'learning_rate': 1.9988673391830082e-05, 'epoch': 0.06}

{'loss': 0.925, 'learning_rate': 1.9954719225730847e-05, 'epoch': 0.07}

{'loss': 0.7148, 'learning_rate': 1.989821441880933e-05, 'epoch': 0.09}

{'loss': 0.6446, 'learning_rate': 1.9819286972627066e-05, 'epoch': 0.1}

{'loss': 0.9821, 'learning_rate': 1.9718115683235418e-05, 'epoch': 0.12}

{'loss': 0.8479, 'learning_rate': 1.9594929736144978e-05, 'epoch': 0.13}

{'loss': 0.9239, 'learning_rate': 1.9450008187146685e-05, 'epoch': 0.14}

{'loss': 0.6228, 'learning_rate': 1.9283679330160726e-05, 'epoch': 0.16}

{'loss': 0.6025, 'learning_rate': 1.9096319953545186e-05, 'epoch': 0.17}

{'loss': 0.6887, 'learning_rate': 1.8888354486549238e-05, 'epoch': 0.19}

{'loss': 0.7338, 'learning_rate': 1.866025403784439e-05, 'epoch': 0.2}

{'loss': 0.7979, 'learning_rate': 1.8412535328311813e-05, 'epoch': 0.22}

{'loss': 0.8106, 'learning_rate': 1.814575952050336e-05, 'epoch': 0.23}

{'loss': 0.6692, 'learning_rate': 1.7860530947427878e-05, 'epoch': 0.25}

{'loss': 0.7384, 'learning_rate': 1.7557495743542586e-05, 'epoch': 0.26}

{'loss': 1.2385, 'learning_rate': 1.72373403810507e-05, 'epoch': 0.28}

{'loss': 0.8223, 'learning_rate': 1.6900790114821122e-05, 'epoch': 0.29}

{'loss': 0.9413, 'learning_rate': 1.6548607339452853e-05, 'epoch': 0.3}

{'loss': 0.58, 'learning_rate': 1.6181589862206053e-05, 'epoch': 0.32}

{'loss': 0.4439, 'learning_rate': 1.5800569095711983e-05, 'epoch': 0.33}

{'loss': 0.7603, 'learning_rate': 1.5406408174555978e-05, 'epoch': 0.35}

{'loss': 0.7394, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.36}

{'loss': 0.5644, 'learning_rate': 1.4582265217274105e-05, 'epoch': 0.38}

{'loss': 0.6599, 'learning_rate': 1.4154150130018867e-05, 'epoch': 0.39}

{'loss': 0.7342, 'learning_rate': 1.3716624556603275e-05, 'epoch': 0.41}

{'loss': 0.5712, 'learning_rate': 1.3270679633174219e-05, 'epoch': 0.42}

{'loss': 0.5184, 'learning_rate': 1.2817325568414299e-05, 'epoch': 0.43}

{'loss': 0.4489, 'learning_rate': 1.2357589355094275e-05, 'epoch': 0.45}

{'loss': 0.6405, 'learning_rate': 1.1892512443604103e-05, 'epoch': 0.46}

{'loss': 0.6521, 'learning_rate': 1.1423148382732854e-05, 'epoch': 0.48}

{'loss': 0.605, 'learning_rate': 1.0950560433041825e-05, 'epoch': 0.49}

{'loss': 0.9008, 'learning_rate': 1.0475819158237426e-05, 'epoch': 0.51}

{'loss': 0.5139, 'learning_rate': 1e-05, 'epoch': 0.52}

{'loss': 0.3993, 'learning_rate': 9.524180841762577e-06, 'epoch': 0.54}

{'loss': 0.5052, 'learning_rate': 9.049439566958176e-06, 'epoch': 0.55}

{'loss': 0.5124, 'learning_rate': 8.576851617267151e-06, 'epoch': 0.57}

{'loss': 0.5113, 'learning_rate': 8.107487556395902e-06, 'epoch': 0.58}

{'loss': 0.6625, 'learning_rate': 7.642410644905726e-06, 'epoch': 0.59}

{'loss': 0.5105, 'learning_rate': 7.182674431585703e-06, 'epoch': 0.61}

{'loss': 0.4918, 'learning_rate': 6.729320366825785e-06, 'epoch': 0.62}

{'loss': 0.545, 'learning_rate': 6.283375443396726e-06, 'epoch': 0.64}

{'loss': 0.4987, 'learning_rate': 5.845849869981137e-06, 'epoch': 0.65}

{'loss': 0.4379, 'learning_rate': 5.417734782725896e-06, 'epoch': 0.67}

{'loss': 1.0807, 'learning_rate': 5.000000000000003e-06, 'epoch': 0.68}

{'loss': 0.5286, 'learning_rate': 4.593591825444028e-06, 'epoch': 0.7}

{'loss': 0.4412, 'learning_rate': 4.19943090428802e-06, 'epoch': 0.71}

{'loss': 0.4498, 'learning_rate': 3.818410137793947e-06, 'epoch': 0.72}

{'loss': 0.5628, 'learning_rate': 3.4513926605471504e-06, 'epoch': 0.74}

{'loss': 0.7288, 'learning_rate': 3.099209885178882e-06, 'epoch': 0.75}

{'loss': 0.3687, 'learning_rate': 2.7626596189492983e-06, 'epoch': 0.77}

{'loss': 0.5083, 'learning_rate': 2.4425042564574186e-06, 'epoch': 0.78}

{'loss': 0.4814, 'learning_rate': 2.1394690525721275e-06, 'epoch': 0.8}

{'loss': 0.2657, 'learning_rate': 1.854240479496643e-06, 'epoch': 0.81}

{'loss': 0.6091, 'learning_rate': 1.587464671688187e-06, 'epoch': 0.83}

{'loss': 0.4775, 'learning_rate': 1.339745962155613e-06, 'epoch': 0.84}

{'loss': 0.4788, 'learning_rate': 1.1116455134507665e-06, 'epoch': 0.86}

{'loss': 0.4893, 'learning_rate': 9.036800464548157e-07, 'epoch': 0.87}

{'loss': 0.5964, 'learning_rate': 7.163206698392744e-07, 'epoch': 0.88}

{'loss': 0.4402, 'learning_rate': 5.499918128533155e-07, 'epoch': 0.9}

{'loss': 0.3448, 'learning_rate': 4.0507026385502747e-07, 'epoch': 0.91}

{'loss': 0.5205, 'learning_rate': 2.818843167645835e-07, 'epoch': 0.93}

{'loss': 0.3329, 'learning_rate': 1.8071302737293294e-07, 'epoch': 0.94}

{'loss': 0.3449, 'learning_rate': 1.0178558119067316e-07, 'epoch': 0.96}

{'loss': 0.4466, 'learning_rate': 4.528077426915412e-08, 'epoch': 0.97}

{'loss': 0.3691, 'learning_rate': 1.1326608169920373e-08, 'epoch': 0.99}

{'loss': 0.6561, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 137.7584, 'train_samples_per_second': 16.028, 'train_steps_per_second': 0.501, 'train_loss': 0.6541394552458888, 'epoch': 1.0}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
