[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|                                                                                                       | 0/88 [00:00<?, ?it/s]/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/vast/users/xiaodan/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
                                                                                                                                   

{'loss': 1.0402, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.01}

{'loss': 0.8635, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.02}

{'loss': 0.9078, 'learning_rate': 2e-05, 'epoch': 0.03}

{'loss': 1.3009, 'learning_rate': 1.9993170601430233e-05, 'epoch': 0.05}

{'loss': 1.0125, 'learning_rate': 1.997269173385788e-05, 'epoch': 0.06}

{'loss': 0.5849, 'learning_rate': 1.993859136895274e-05, 'epoch': 0.07}

{'loss': 0.7173, 'learning_rate': 1.9890916083711463e-05, 'epoch': 0.08}

{'loss': 0.7708, 'learning_rate': 1.982973099683902e-05, 'epoch': 0.09}

{'loss': 0.9879, 'learning_rate': 1.975511967980437e-05, 'epoch': 0.1}

{'loss': 0.6533, 'learning_rate': 1.9667184042691877e-05, 'epoch': 0.11}

{'loss': 0.589, 'learning_rate': 1.956604419500441e-05, 'epoch': 0.12}

{'loss': 0.4291, 'learning_rate': 1.94518382816082e-05, 'epoch': 0.14}

{'loss': 0.7528, 'learning_rate': 1.932472229404356e-05, 'epoch': 0.15}

{'loss': 0.865, 'learning_rate': 1.9184869857459233e-05, 'epoch': 0.16}

{'loss': 0.8488, 'learning_rate': 1.903247199346129e-05, 'epoch': 0.17}

{'loss': 0.6742, 'learning_rate': 1.886773685920062e-05, 'epoch': 0.18}

{'loss': 0.5902, 'learning_rate': 1.8690889463055285e-05, 'epoch': 0.19}

{'loss': 0.6858, 'learning_rate': 1.8502171357296144e-05, 'epoch': 0.2}

{'loss': 0.7635, 'learning_rate': 1.8301840308155507e-05, 'epoch': 0.22}

{'loss': 0.5937, 'learning_rate': 1.8090169943749477e-05, 'epoch': 0.23}

{'loss': 0.5339, 'learning_rate': 1.7867449380334834e-05, 'epoch': 0.24}

{'loss': 0.569, 'learning_rate': 1.763398282741103e-05, 'epoch': 0.25}

{'loss': 0.4521, 'learning_rate': 1.7390089172206594e-05, 'epoch': 0.26}

{'loss': 0.4868, 'learning_rate': 1.7136101544117526e-05, 'epoch': 0.27}

{'loss': 0.6655, 'learning_rate': 1.687236685969263e-05, 'epoch': 0.28}

{'loss': 0.4778, 'learning_rate': 1.659924534878723e-05, 'epoch': 0.3}

{'loss': 0.5613, 'learning_rate': 1.631711006253251e-05, 'epoch': 0.31}

{'loss': 0.4664, 'learning_rate': 1.6026346363792565e-05, 'epoch': 0.32}

{'loss': 0.5947, 'learning_rate': 1.5727351400805054e-05, 'epoch': 0.33}

{'loss': 0.4766, 'learning_rate': 1.5420533564724495e-05, 'epoch': 0.34}

{'loss': 0.6052, 'learning_rate': 1.510631193180907e-05, 'epoch': 0.35}

{'loss': 0.5705, 'learning_rate': 1.4785115691012866e-05, 'epoch': 0.36}

{'loss': 0.4773, 'learning_rate': 1.4457383557765385e-05, 'epoch': 0.38}

{'loss': 0.2729, 'learning_rate': 1.4123563174739036e-05, 'epoch': 0.39}

{'loss': 0.5062, 'learning_rate': 1.3784110500423104e-05, 'epoch': 0.4}

{'loss': 0.7328, 'learning_rate': 1.3439489186339283e-05, 'epoch': 0.41}

{'loss': 0.4803, 'learning_rate': 1.3090169943749475e-05, 'epoch': 0.42}

{'loss': 0.4613, 'learning_rate': 1.2736629900720832e-05, 'epoch': 0.43}

{'loss': 0.4544, 'learning_rate': 1.2379351950426188e-05, 'epoch': 0.44}

{'loss': 0.4211, 'learning_rate': 1.2018824091570103e-05, 'epoch': 0.45}

{'loss': 0.4319, 'learning_rate': 1.16555387618413e-05, 'epoch': 0.47}

{'loss': 0.3986, 'learning_rate': 1.1289992165302036e-05, 'epoch': 0.48}

{'loss': 0.3965, 'learning_rate': 1.092268359463302e-05, 'epoch': 0.49}

{'loss': 0.6189, 'learning_rate': 1.05541147491597e-05, 'epoch': 0.5}

{'loss': 0.5553, 'learning_rate': 1.01847890495913e-05, 'epoch': 0.51}

{'loss': 0.3239, 'learning_rate': 9.815210950408703e-06, 'epoch': 0.52}

{'loss': 0.3268, 'learning_rate': 9.445885250840301e-06, 'epoch': 0.53}

{'loss': 0.6023, 'learning_rate': 9.07731640536698e-06, 'epoch': 0.55}

{'loss': 0.6034, 'learning_rate': 8.71000783469797e-06, 'epoch': 0.56}

{'loss': 0.2388, 'learning_rate': 8.3444612381587e-06, 'epoch': 0.57}

{'loss': 0.4383, 'learning_rate': 7.9811759084299e-06, 'epoch': 0.58}

{'loss': 0.3571, 'learning_rate': 7.620648049573815e-06, 'epoch': 0.59}

{'loss': 0.4143, 'learning_rate': 7.263370099279173e-06, 'epoch': 0.6}

{'loss': 0.4177, 'learning_rate': 6.909830056250527e-06, 'epoch': 0.61}

{'loss': 0.6401, 'learning_rate': 6.560510813660719e-06, 'epoch': 0.62}

{'loss': 0.4079, 'learning_rate': 6.215889499576898e-06, 'epoch': 0.64}

{'loss': 0.2224, 'learning_rate': 5.876436825260967e-06, 'epoch': 0.65}

{'loss': 0.4382, 'learning_rate': 5.542616442234618e-06, 'epoch': 0.66}

{'loss': 0.4149, 'learning_rate': 5.214884308987136e-06, 'epoch': 0.67}

{'loss': 0.2936, 'learning_rate': 4.893688068190933e-06, 'epoch': 0.68}

{'loss': 0.6914, 'learning_rate': 4.579466435275506e-06, 'epoch': 0.69}

{'loss': 0.3377, 'learning_rate': 4.272648599194948e-06, 'epoch': 0.7}

{'loss': 0.6205, 'learning_rate': 3.973653636207437e-06, 'epoch': 0.72}

{'loss': 0.298, 'learning_rate': 3.6828899374674933e-06, 'epoch': 0.73}

{'loss': 0.2501, 'learning_rate': 3.4007546512127764e-06, 'epoch': 0.74}

{'loss': 0.2282, 'learning_rate': 3.1276331403073733e-06, 'epoch': 0.75}

{'loss': 0.5147, 'learning_rate': 2.8638984558824777e-06, 'epoch': 0.76}

{'loss': 0.3255, 'learning_rate': 2.6099108277934105e-06, 'epoch': 0.77}

{'loss': 0.4588, 'learning_rate': 2.3660171725889703e-06, 'epoch': 0.78}

{'loss': 0.4987, 'learning_rate': 2.132550619665168e-06, 'epoch': 0.8}

{'loss': 0.3752, 'learning_rate': 1.9098300562505266e-06, 'epoch': 0.81}

{'loss': 0.3505, 'learning_rate': 1.6981596918444953e-06, 'epoch': 0.82}

{'loss': 0.4258, 'learning_rate': 1.4978286427038602e-06, 'epoch': 0.83}

{'loss': 0.4416, 'learning_rate': 1.3091105369447166e-06, 'epoch': 0.84}

{'loss': 0.5231, 'learning_rate': 1.132263140799381e-06, 'epoch': 0.85}

{'loss': 0.4317, 'learning_rate': 9.675280065387117e-07, 'epoch': 0.86}

{'loss': 0.3405, 'learning_rate': 8.151301425407699e-07, 'epoch': 0.88}

{'loss': 0.4685, 'learning_rate': 6.752777059564431e-07, 'epoch': 0.89}

{'loss': 0.1508, 'learning_rate': 5.481617183918053e-07, 'epoch': 0.9}

{'loss': 0.4517, 'learning_rate': 4.3395580499559276e-07, 'epoch': 0.91}

{'loss': 0.4324, 'learning_rate': 3.328159573081258e-07, 'epoch': 0.92}

{'loss': 0.5265, 'learning_rate': 2.44880320195634e-07, 'epoch': 0.93}

{'loss': 0.2712, 'learning_rate': 1.7026900316098217e-07, 'epoch': 0.94}

{'loss': 0.2879, 'learning_rate': 1.0908391628854042e-07, 'epoch': 0.95}

{'loss': 0.3148, 'learning_rate': 6.140863104726391e-08, 'epoch': 0.97}

{'loss': 0.5155, 'learning_rate': 2.7308266142119788e-08, 'epoch': 0.98}

{'loss': 0.4453, 'learning_rate': 6.82939856977094e-09, 'epoch': 0.99}

{'loss': 0.352, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 172.6602, 'train_samples_per_second': 16.263, 'train_steps_per_second': 0.51, 'train_loss': 0.5200784919275478, 'epoch': 1.0}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
